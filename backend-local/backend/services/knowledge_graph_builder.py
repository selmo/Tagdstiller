"""
Knowledge Graph Builder Service

ë¬¸ì„œ ì „ì²´ë¥¼ Knowledge Graphë¡œ ë³€í™˜í•˜ëŠ” ì „ìš© ì„œë¹„ìŠ¤
LLMì„ ì‚¬ìš©í•˜ì—¬ ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ê³  ê·¸ë˜í”„ êµ¬ì¡°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from sqlalchemy.orm import Session

from prompts.templates import KnowledgeGraphPrompts, PromptTemplate
from services.local_file_analyzer import LocalFileAnalyzer

logger = logging.getLogger(__name__)


class KnowledgeGraphBuilder:
    """ë¬¸ì„œë¥¼ Knowledge Graphë¡œ ë³€í™˜í•˜ëŠ” ë¹Œë” í´ë˜ìŠ¤"""

    def __init__(self, db: Session):
        self.db = db
        self.analyzer = LocalFileAnalyzer(db)
        self.logger = logging.getLogger(__name__)

    def build_knowledge_graph(
        self,
        text: str,
        file_path: str,
        domain: str = "general",
        structure_info: Optional[Dict[str, Any]] = None,
        llm_config: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œ í…ìŠ¤íŠ¸ì—ì„œ Knowledge Graph ìƒì„±

        Args:
            text: ë¬¸ì„œ í…ìŠ¤íŠ¸
            file_path: íŒŒì¼ ê²½ë¡œ
            domain: ë¬¸ì„œ ë„ë©”ì¸ (general, technical, academic, business, legal)
            structure_info: ë¬¸ì„œ êµ¬ì¡° ì •ë³´ (ì„ íƒ)
            llm_config: LLM ì„¤ì • (ì„ íƒ)

        Returns:
            Knowledge Graph JSON êµ¬ì¡°
        """
        try:
            self.logger.info(f"ğŸ” Knowledge Graph ìƒì„± ì‹œì‘: {Path(file_path).name} (ë„ë©”ì¸: {domain})")

            # 1. ë„ë©”ì¸ë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
            prompt_template = self._get_kg_prompt_template(domain)

            # 2. êµ¬ì¡° ì •ë³´ ê°„ëµí™”
            structure_summary = self._summarize_structure(structure_info) if structure_info else "êµ¬ì¡° ì •ë³´ ì—†ìŒ"

            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = prompt_template.format(
                text=text[:100000],  # ìµœëŒ€ 100K ë¬¸ì (ì•½ 50K í† í°)
                domain=domain,
                structure_info=structure_summary
            )

            # 4. LLM í˜¸ì¶œ (LocalFileAnalyzerì˜ LLM ê¸°ëŠ¥ í™œìš©)
            llm_response = self._call_llm_for_kg(prompt, llm_config or {})

            if not llm_response.get("success"):
                return self._create_error_result(llm_response.get("error", "LLM í˜¸ì¶œ ì‹¤íŒ¨"))

            # 5. LLM ì‘ë‹µ íŒŒì‹±
            raw_response = llm_response.get("response", "")
            self.logger.debug(f"ğŸ” LLM ì›ì‹œ ì‘ë‹µ (ì²˜ìŒ 500ì): {raw_response[:500]}")
            kg_data = self._parse_kg_response(raw_response)

            # 6. ë©”íƒ€ë°ì´í„° ì¶”ê°€
            kg_result = self._enrich_kg_with_metadata(
                kg_data,
                file_path,
                domain,
                structure_info
            )

            self.logger.info(
                f"âœ… Knowledge Graph ìƒì„± ì™„ë£Œ: "
                f"{kg_result['stats']['entity_count']}ê°œ ì—”í‹°í‹°, "
                f"{kg_result['stats']['relationship_count']}ê°œ ê´€ê³„"
            )

            return kg_result

        except Exception as e:
            self.logger.error(f"âŒ Knowledge Graph ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return self._create_error_result(str(e))

    def _get_kg_prompt_template(self, domain: str) -> PromptTemplate:
        """ë„ë©”ì¸ë³„ KG ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ì„ íƒ"""
        domain_prompts = {
            "general": KnowledgeGraphPrompts.GENERAL_KG_EXTRAION,
            "technical": KnowledgeGraphPrompts.TECHNICAL_KG_EXTRACTION,
            "academic": KnowledgeGraphPrompts.ACADEMIC_KG_EXTRACTION,
            "business": KnowledgeGraphPrompts.BUSINESS_KG_EXTRACTION,
            "legal": KnowledgeGraphPrompts.LEGAL_KG_EXTRACTION,
        }
        return domain_prompts.get(domain, KnowledgeGraphPrompts.GENERAL_KG_EXTRAION)

    def _summarize_structure(self, structure_info: Dict[str, Any]) -> str:
        """ë¬¸ì„œ êµ¬ì¡° ì •ë³´ë¥¼ ê°„ëµí•œ í…ìŠ¤íŠ¸ë¡œ ìš”ì•½"""
        if not structure_info:
            return "êµ¬ì¡° ì •ë³´ ì—†ìŒ"

        try:
            summary_parts = []

            # ë¬¸ì„œ ê¸°ë³¸ ì •ë³´
            doc_info = structure_info.get("documentInfo", {})
            if doc_info:
                title = doc_info.get("title", "ì œëª© ì—†ìŒ")
                doc_type = doc_info.get("documentType", "ë¯¸ë¶„ë¥˜")
                summary_parts.append(f"ë¬¸ì„œ: {title} ({doc_type})")

            # êµ¬ì¡° ë¶„ì„ ìš”ì•½
            structure_analysis = structure_info.get("structureAnalysis", [])
            if structure_analysis:
                section_count = len(structure_analysis)
                summary_parts.append(f"{section_count}ê°œ ì£¼ìš” ì„¹ì…˜")

            # í•µì‹¬ ë‚´ìš© ìš”ì•½
            core_content = structure_info.get("coreContent", {})
            if core_content:
                main_topic = core_content.get("mainTopic", "")
                if main_topic:
                    summary_parts.append(f"ì£¼ì œ: {main_topic}")

            return " | ".join(summary_parts) if summary_parts else "êµ¬ì¡° ì •ë³´ ì—†ìŒ"

        except Exception as e:
            self.logger.warning(f"êµ¬ì¡° ì •ë³´ ìš”ì•½ ì‹¤íŒ¨: {e}")
            return "êµ¬ì¡° ì •ë³´ ì²˜ë¦¬ ì˜¤ë¥˜"

    def _call_llm_for_kg(self, prompt: str, llm_config: Dict[str, Any]) -> Dict[str, Any]:
        """LLM í˜¸ì¶œí•˜ì—¬ Knowledge Graph ì¶”ì¶œ"""
        try:
            # LLM ì„¤ì • ì¶”ì¶œ
            provider = llm_config.get("provider", "gemini")

            if provider == "gemini":
                return self._call_gemini_for_kg(prompt, llm_config)
            elif provider == "openai":
                return self._call_openai_for_kg(prompt, llm_config)
            elif provider == "ollama":
                return self._call_ollama_for_kg(prompt, llm_config)
            else:
                return {"success": False, "error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM í”„ë¡œë°”ì´ë”: {provider}"}

        except Exception as e:
            self.logger.error(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    def _call_gemini_for_kg(self, prompt: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """Gemini API í˜¸ì¶œ (ìë™ ì¬ì‹œë„ í¬í•¨)"""
        import requests
        import time

        api_key = config.get("api_key")
        model = config.get("model", "models/gemini-2.0-flash")
        base_url = config.get("base_url", "https://generativelanguage.googleapis.com")
        timeout = config.get("timeout", 600)
        max_retries = config.get("max_retries", 3)

        if not api_key:
            return {"success": False, "error": "Gemini API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤"}

        # Gemini API ì—”ë“œí¬ì¸íŠ¸
        url = f"{base_url}/v1beta/{model}:generateContent?key={api_key}"

        # ìš”ì²­ ë³¸ë¬¸
        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }],
            "generationConfig": {
                "temperature": config.get("temperature", 0.1),
                "maxOutputTokens": config.get("max_tokens", 8192),
            }
        }

        # ì¬ì‹œë„ ë¡œì§ (exponential backoff)
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    wait_time = (2 ** attempt) * 2  # 2, 4, 8ì´ˆ
                    self.logger.warning(f"â³ Rate limit ëŒ€ê¸° ì¤‘... {wait_time}ì´ˆ ({attempt + 1}/{max_retries})")
                    time.sleep(wait_time)

                self.logger.info(f"ğŸ“¡ Gemini API í˜¸ì¶œ ì‹œì‘... (ëª¨ë¸: {model}, ì‹œë„: {attempt + 1}/{max_retries})")

                response = requests.post(url, json=payload, timeout=timeout)
                response.raise_for_status()

                result = response.json()

                # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                candidates = result.get("candidates", [])
                if not candidates:
                    return {"success": False, "error": "Gemini ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}

                content = candidates[0].get("content", {})
                parts = content.get("parts", [])

                if not parts:
                    return {"success": False, "error": "Gemini ì‘ë‹µì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"}

                response_text = parts[0].get("text", "")

                self.logger.info(f"âœ… Gemini ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ: {len(response_text):,}ì")

                return {"success": True, "response": response_text}

            except requests.exceptions.Timeout:
                return {"success": False, "error": f"Gemini API íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)"}
            except requests.exceptions.HTTPError as e:
                # 429 Too Many Requests - ì¬ì‹œë„
                if e.response.status_code == 429 and attempt < max_retries - 1:
                    self.logger.warning(f"âš ï¸ Rate limit ì´ˆê³¼ (429), ì¬ì‹œë„ {attempt + 1}/{max_retries}")
                    continue
                # ë‹¤ë¥¸ HTTP ì—ëŸ¬ ë˜ëŠ” ë§ˆì§€ë§‰ ì¬ì‹œë„ - ì‹¤íŒ¨
                return {"success": False, "error": f"Gemini API ì˜¤ë¥˜: {str(e)}"}
            except requests.exceptions.RequestException as e:
                return {"success": False, "error": f"Gemini API ì˜¤ë¥˜: {str(e)}"}
            except Exception as e:
                self.logger.error(f"Gemini í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
                return {"success": False, "error": str(e)}

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        return {"success": False, "error": f"Gemini API rate limit ì´ˆê³¼ - {max_retries}íšŒ ì¬ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨"}

    def _call_openai_for_kg(self, prompt: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """OpenAI API í˜¸ì¶œ"""
        try:
            import requests

            api_key = config.get("api_key")
            model = config.get("model", "gpt-4")
            base_url = config.get("base_url", "https://api.openai.com/v1")
            timeout = config.get("timeout", 600)

            if not api_key:
                return {"success": False, "error": "OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤"}

            url = f"{base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }

            payload = {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": config.get("temperature", 0.1),
                "max_tokens": config.get("max_tokens", 8192),
            }

            self.logger.info(f"ğŸ“¡ OpenAI API í˜¸ì¶œ ì‹œì‘... (ëª¨ë¸: {model})")

            response = requests.post(url, headers=headers, json=payload, timeout=timeout)
            response.raise_for_status()

            result = response.json()
            response_text = result["choices"][0]["message"]["content"]

            self.logger.info(f"âœ… OpenAI ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ: {len(response_text):,}ì")

            return {"success": True, "response": response_text}

        except Exception as e:
            self.logger.error(f"OpenAI í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    def _call_ollama_for_kg(self, prompt: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """Ollama API í˜¸ì¶œ"""
        try:
            import requests

            base_url = config.get("base_url", "http://localhost:11434")
            model = config.get("model", "llama3.2")
            timeout = config.get("timeout", 600)

            url = f"{base_url}/api/generate"
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": config.get("temperature", 0.1),
                }
            }

            self.logger.info(f"ğŸ“¡ Ollama API í˜¸ì¶œ ì‹œì‘... (ëª¨ë¸: {model})")

            response = requests.post(url, json=payload, timeout=timeout)
            response.raise_for_status()

            result = response.json()
            response_text = result.get("response", "")

            self.logger.info(f"âœ… Ollama ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ: {len(response_text):,}ì")

            return {"success": True, "response": response_text}

        except Exception as e:
            self.logger.error(f"Ollama í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    def _parse_kg_response(self, response: str) -> Dict[str, Any]:
        """LLM ì‘ë‹µì„ Knowledge Graph êµ¬ì¡°ë¡œ íŒŒì‹±"""
        try:
            # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
            kg_data = json.loads(response)

            # ê·¸ë˜í”„ êµ¬ì¡° ê²€ì¦
            if "graph" in kg_data:
                # Neo4j/Memgraph ìŠ¤íƒ€ì¼ (nodes, edges)
                return self._normalize_graph_structure(kg_data["graph"])
            elif "entities" in kg_data and "relationships" in kg_data:
                # ê¸°ì¡´ ìŠ¤íƒ€ì¼ (entities, relationships)
                return {
                    "nodes": kg_data.get("entities", []),
                    "edges": kg_data.get("relationships", [])
                }
            else:
                # ìµœìƒìœ„ ë ˆë²¨ì´ ì§ì ‘ ê·¸ë˜í”„ì¸ ê²½ìš°
                return kg_data

        except json.JSONDecodeError as e:
            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ì§„ ê²½ìš° ì˜ˆìƒë˜ëŠ” ìƒí™©ì´ë¯€ë¡œ DEBUG ë ˆë²¨ë¡œ ê¸°ë¡
            self.logger.debug(f"JSON ì§ì ‘ íŒŒì‹± ì‹¤íŒ¨ (ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ ì‹œë„): {e}")
            # ë°±ì—…: ì‘ë‹µì—ì„œ JSON ë¸”ë¡ ì¶”ì¶œ ì‹œë„
            extracted = self._extract_json_from_text(response)
            # ì¶”ì¶œëœ ë°ì´í„°ë„ êµ¬ì¡° ì •ê·œí™” í•„ìš”
            if "graph" in extracted:
                return self._normalize_graph_structure(extracted["graph"])
            elif "entities" in extracted and "relationships" in extracted:
                return {
                    "nodes": extracted.get("entities", []),
                    "edges": extracted.get("relationships", [])
                }
            else:
                return extracted
        except Exception as e:
            self.logger.error(f"KG ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}", exc_info=True)
            return {"nodes": [], "edges": []}

    def _normalize_graph_structure(self, graph: Dict[str, Any]) -> Dict[str, Any]:
        """ê·¸ë˜í”„ êµ¬ì¡° ì •ê·œí™” (nodes, edges í˜•ì‹ìœ¼ë¡œ í†µì¼)"""
        return {
            "nodes": graph.get("nodes", graph.get("entities", [])),
            "edges": graph.get("edges", graph.get("relationships", []))
        }

    def _extract_json_from_text(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ JSON ë¸”ë¡ ì¶”ì¶œ (ë¶ˆì™„ì „í•œ JSON ë³µêµ¬ í¬í•¨)"""
        import re

        # JSON ì½”ë“œ ë¸”ë¡ íŒ¨í„´ (```json ... ```) - greedy ë§¤ì¹­ìœ¼ë¡œ ì „ì²´ JSON ì¶”ì¶œ
        json_block_pattern = r'```(?:json)?\s*(\{.*\})\s*```'
        match = re.search(json_block_pattern, text, re.DOTALL)

        json_str = None
        if match:
            json_str = match.group(1)
            self.logger.info(f"âœ… ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ({len(json_str)}ì)")
        else:
            # ì§ì ‘ { } ë¸”ë¡ ì°¾ê¸° (greedy ë§¤ì¹­)
            brace_pattern = r'\{.*\}'
            match = re.search(brace_pattern, text, re.DOTALL)
            if match:
                json_str = match.group(0)
                self.logger.info(f"âœ… ì¤‘ê´„í˜¸ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ({len(json_str)}ì)")

        if json_str:
            # ë¨¼ì € ì •ìƒ íŒŒì‹± ì‹œë„
            try:
                return json.loads(json_str)
            except json.JSONDecodeError as e:
                self.logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, ë¶ˆì™„ì „í•œ JSON ë³µêµ¬ ì‹œë„: {e}")
                # ë¶ˆì™„ì „í•œ JSON ë³µêµ¬ ì‹œë„
                repaired = self._repair_incomplete_json(json_str)
                if repaired:
                    try:
                        result = json.loads(repaired)
                        self.logger.info(f"âœ… ë¶ˆì™„ì „í•œ JSON ë³µêµ¬ ì„±ê³µ: {len(result.get('entities', []))}ê°œ ì—”í‹°í‹°")
                        return result
                    except json.JSONDecodeError as e2:
                        self.logger.error(f"âŒ JSON ë³µêµ¬ ì‹¤íŒ¨: {e2}")

        self.logger.warning("JSON ì¶”ì¶œ ì‹¤íŒ¨, ë¹ˆ ê·¸ë˜í”„ ë°˜í™˜")
        return {"nodes": [], "edges": []}

    def _repair_incomplete_json(self, json_str: str) -> Optional[str]:
        """ë¶ˆì™„ì „í•œ JSONì„ ìˆ˜ì • (LLM ì‘ë‹µì´ ì˜ë ¸ì„ ë•Œ)"""
        try:
            # ì˜ë¦° JSONì˜ ì¼ë°˜ì ì¸ íŒ¨í„´ ìˆ˜ì •
            # 1. ë§ˆì§€ë§‰ ê°ì²´ê°€ ë¶ˆì™„ì „í•œ ê²½ìš° ì œê±°
            # 2. ë°°ì—´ê³¼ ê°ì²´ ë‹«ê¸°

            # ë§ˆì§€ë§‰ ì‰¼í‘œ ë’¤ì— ë¶ˆì™„ì „í•œ í•­ëª©ì´ ìˆëŠ”ì§€ í™•ì¸
            last_comma_pos = json_str.rfind(',')
            if last_comma_pos > 0:
                # ë§ˆì§€ë§‰ ì‰¼í‘œ ì´í›„ ë‚´ìš© í™•ì¸
                after_comma = json_str[last_comma_pos+1:].strip()
                # ì™„ì „í•œ ê°ì²´ì¸ì§€ í™•ì¸ (ë‹«ëŠ” ì¤‘ê´„í˜¸ê°€ ìˆëŠ”ì§€)
                if after_comma and not after_comma.endswith('}'):
                    # ë¶ˆì™„ì „í•œ ê°ì²´ ì œê±°
                    json_str = json_str[:last_comma_pos]
                    self.logger.info(f"ğŸ”§ ë¶ˆì™„ì „í•œ ë§ˆì§€ë§‰ ê°ì²´ ì œê±°")

            # í•„ìš”í•œ ë‹«ëŠ” ê´„í˜¸ ì¶”ê°€
            open_braces = json_str.count('{') - json_str.count('}')
            open_brackets = json_str.count('[') - json_str.count(']')

            if open_brackets > 0:
                json_str += '\n  ]' * open_brackets
                self.logger.info(f"ğŸ”§ ë‹«ëŠ” ë°°ì—´ ê´„í˜¸ {open_brackets}ê°œ ì¶”ê°€")

            if open_braces > 0:
                json_str += '\n}' * open_braces
                self.logger.info(f"ğŸ”§ ë‹«ëŠ” ê°ì²´ ê´„í˜¸ {open_braces}ê°œ ì¶”ê°€")

            return json_str
        except Exception as e:
            self.logger.error(f"JSON ë³µêµ¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def _enrich_kg_with_metadata(
        self,
        kg_data: Dict[str, Any],
        file_path: str,
        domain: str,
        structure_info: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Knowledge Graphì— ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        nodes = kg_data.get("nodes", [])
        edges = kg_data.get("edges", [])

        # í†µê³„ ê³„ì‚°
        entity_types = {}
        relationship_types = {}

        for node in nodes:
            node_type = node.get("type", "Unknown")
            entity_types[node_type] = entity_types.get(node_type, 0) + 1

        for edge in edges:
            edge_type = edge.get("type", "UNKNOWN")
            relationship_types[edge_type] = relationship_types.get(edge_type, 0) + 1

        # ê²°ê³¼ êµ¬ì¡° ìƒì„±
        result = {
            "success": True,
            "file_path": file_path,
            "domain": domain,
            "extraction_date": datetime.now().isoformat(),
            "graph": {
                "nodes": nodes,
                "edges": edges
            },
            "stats": {
                "entity_count": len(nodes),
                "relationship_count": len(edges),
                "entity_types": entity_types,
                "relationship_types": relationship_types,
                "density": self._calculate_graph_density(len(nodes), len(edges))
            },
            "metadata": {
                "source_document": Path(file_path).name,
                "domain": domain,
                "has_structure_info": structure_info is not None,
                "version": "1.0"
            }
        }

        return result

    def _calculate_graph_density(self, node_count: int, edge_count: int) -> float:
        """ê·¸ë˜í”„ ë°€ë„ ê³„ì‚° (0~1 ë²”ìœ„)"""
        if node_count <= 1:
            return 0.0
        max_possible_edges = node_count * (node_count - 1)  # ë°©í–¥ ê·¸ë˜í”„ ê¸°ì¤€
        return round(edge_count / max_possible_edges, 4) if max_possible_edges > 0 else 0.0

    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ê²°ê³¼ ìƒì„±"""
        return {
            "success": False,
            "error": error_message,
            "graph": {
                "nodes": [],
                "edges": []
            },
            "stats": {
                "entity_count": 0,
                "relationship_count": 0,
                "entity_types": {},
                "relationship_types": {},
                "density": 0.0
            }
        }

    def save_knowledge_graph(
        self,
        kg_result: Dict[str, Any],
        output_dir: Path,
        format: str = "json"
    ) -> Dict[str, str]:
        """
        Knowledge Graphë¥¼ íŒŒì¼ë¡œ ì €ì¥

        Args:
            kg_result: Knowledge Graph ê²°ê³¼
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            format: ì €ì¥ í˜•ì‹ (json, cypher, graphml)

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        saved_files = {}

        try:
            if format == "json" or format == "all":
                # JSON í˜•ì‹ ì €ì¥
                json_path = output_dir / "knowledge_graph.json"
                with json_path.open('w', encoding='utf-8') as f:
                    json.dump(kg_result, f, ensure_ascii=False, indent=2)
                saved_files["json"] = str(json_path)
                self.logger.info(f"ğŸ“ Knowledge Graph JSON ì €ì¥: {json_path}")

            if format == "cypher" or format == "all":
                # Cypher ì¿¼ë¦¬ í˜•ì‹ ì €ì¥
                cypher_path = output_dir / "knowledge_graph.cypher"
                cypher_content = self._generate_cypher_queries(kg_result)
                with cypher_path.open('w', encoding='utf-8') as f:
                    f.write(cypher_content)
                saved_files["cypher"] = str(cypher_path)
                self.logger.info(f"ğŸ“ Knowledge Graph Cypher ì €ì¥: {cypher_path}")

            if format == "graphml" or format == "all":
                # GraphML í˜•ì‹ ì €ì¥
                graphml_path = output_dir / "knowledge_graph.graphml"
                graphml_content = self._generate_graphml(kg_result)
                with graphml_path.open('w', encoding='utf-8') as f:
                    f.write(graphml_content)
                saved_files["graphml"] = str(graphml_path)
                self.logger.info(f"ğŸ“ Knowledge Graph GraphML ì €ì¥: {graphml_path}")

            return saved_files

        except Exception as e:
            self.logger.error(f"âŒ Knowledge Graph ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)
            return saved_files

    def _generate_cypher_queries(self, kg_result: Dict[str, Any]) -> str:
        """Cypher CREATE ì¿¼ë¦¬ ìƒì„± (Neo4j/Memgraph í˜¸í™˜)"""
        queries = []
        queries.append("// Knowledge Graph Cypher Queries")
        queries.append(f"// Generated: {datetime.now().isoformat()}\n")

        # ë…¸ë“œ ìƒì„± ì¿¼ë¦¬
        queries.append("// Create Nodes")
        for node in kg_result.get("graph", {}).get("nodes", []):
            node_id = node.get("id", "")
            node_type = node.get("type", "Node")
            properties = node.get("properties", {})

            # í”„ë¡œí¼í‹° ë¬¸ìì—´ ìƒì„±
            props_str = ", ".join([
                f"{k}: {self._cypher_value(v)}"
                for k, v in properties.items()
            ])

            query = f"CREATE (n:{node_type} {{id: '{node_id}', {props_str}}});"
            queries.append(query)

        queries.append("\n// Create Relationships")
        for edge in kg_result.get("graph", {}).get("edges", []):
            source = edge.get("source", "")
            target = edge.get("target", "")
            edge_type = edge.get("type", "RELATED_TO")
            properties = edge.get("properties", {})

            # í”„ë¡œí¼í‹° ë¬¸ìì—´ ìƒì„±
            props_str = ", ".join([
                f"{k}: {self._cypher_value(v)}"
                for k, v in properties.items()
            ])

            query = (
                f"MATCH (a {{id: '{source}'}}), (b {{id: '{target}'}}) "
                f"CREATE (a)-[r:{edge_type} {{{props_str}}}]->(b);"
            )
            queries.append(query)

        return "\n".join(queries)

    def _cypher_value(self, value: Any) -> str:
        """Python ê°’ì„ Cypher ê°’ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        if isinstance(value, str):
            # ë¬¸ìì—´ ì´ìŠ¤ì¼€ì´í”„
            escaped = value.replace("'", "\\'").replace('"', '\\"')
            return f"'{escaped}'"
        elif isinstance(value, bool):
            return "true" if value else "false"
        elif isinstance(value, (int, float)):
            return str(value)
        elif isinstance(value, list):
            items = [self._cypher_value(v) for v in value]
            return f"[{', '.join(items)}]"
        elif value is None:
            return "null"
        else:
            return f"'{str(value)}'"

    def _generate_graphml(self, kg_result: Dict[str, Any]) -> str:
        """GraphML XML í˜•ì‹ ìƒì„±"""
        lines = []
        lines.append('<?xml version="1.0" encoding="UTF-8"?>')
        lines.append('<graphml xmlns="http://graphml.graphdrawing.org/xmlns">')
        lines.append('  <graph id="KnowledgeGraph" edgedefault="directed">')

        # ë…¸ë“œ ì¶”ê°€
        for node in kg_result.get("graph", {}).get("nodes", []):
            node_id = node.get("id", "")
            node_type = node.get("type", "Node")
            lines.append(f'    <node id="{node_id}">')
            lines.append(f'      <data key="type">{node_type}</data>')

            # í”„ë¡œí¼í‹° ì¶”ê°€
            for key, value in node.get("properties", {}).items():
                lines.append(f'      <data key="{key}">{self._xml_escape(str(value))}</data>')

            lines.append('    </node>')

        # ì—£ì§€ ì¶”ê°€
        for idx, edge in enumerate(kg_result.get("graph", {}).get("edges", [])):
            edge_id = edge.get("id", f"e{idx}")
            source = edge.get("source", "")
            target = edge.get("target", "")
            edge_type = edge.get("type", "RELATED_TO")

            lines.append(f'    <edge id="{edge_id}" source="{source}" target="{target}">')
            lines.append(f'      <data key="type">{edge_type}</data>')

            # í”„ë¡œí¼í‹° ì¶”ê°€
            for key, value in edge.get("properties", {}).items():
                lines.append(f'      <data key="{key}">{self._xml_escape(str(value))}</data>')

            lines.append('    </edge>')

        lines.append('  </graph>')
        lines.append('</graphml>')

        return "\n".join(lines)

    def _xml_escape(self, text: str) -> str:
        """XML íŠ¹ìˆ˜ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„"""
        return (
            text.replace("&", "&amp;")
            .replace("<", "&lt;")
            .replace(">", "&gt;")
            .replace('"', "&quot;")
            .replace("'", "&apos;")
        )

    # ========== ì²­í‚¹ ê¸°ë°˜ Full KG ì¶”ì¶œ (ì‹ ê·œ) ==========

    def build_full_knowledge_graph_with_chunking(
        self,
        text: str,
        file_path: str,
        domain: str = "general",
        structure_info: Optional[Dict[str, Any]] = None,
        llm_config: Optional[Dict[str, Any]] = None,
        max_chunk_tokens: int = 8000,
        output_dir: Optional[Path] = None,
        extraction_level: str = "standard"
    ) -> Dict[str, Any]:
        """
        êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹ì„ ì‚¬ìš©í•œ ì™„ì „í•œ Knowledge Graph ìƒì„±

        Args:
            text: ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸
            extraction_level: ì¶”ì¶œ ìˆ˜ì¤€ ("brief", "standard", "deep")
            file_path: íŒŒì¼ ê²½ë¡œ
            domain: ë¬¸ì„œ ë„ë©”ì¸
            structure_info: ë¬¸ì„œ êµ¬ì¡° ì •ë³´
            llm_config: LLM ì„¤ì •
            max_chunk_tokens: ì²­í¬ë‹¹ ìµœëŒ€ í† í° ìˆ˜

        Returns:
            ë³‘í•©ëœ ì™„ì „í•œ Knowledge Graph
        """
        try:
            # íŒŒì¼ëª…ì—ì„œ ë¬¸ì„œ ì œëª© ì¶”ì¶œ
            document_title = Path(file_path).stem  # í™•ì¥ì ì œì™¸í•œ íŒŒì¼ëª…
            self.logger.info(f"ğŸ” ì²­í‚¹ ê¸°ë°˜ Full KG ìƒì„± ì‹œì‘: {document_title}")

            # 1. ë¬¸ì„œ ì²­í‚¹
            from .document_chunker import StructuralChunker
            chunker = StructuralChunker()

            # ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ë° ì²­í‚¹
            document_tree = chunker.analyzer.analyze_structure(text)
            chunker_level = chunker.determine_chunking_level(len(text), document_tree)
            chunks = chunker.create_chunks(document_tree, chunk_level=chunker_level)

            self.logger.info(f"ğŸ“„ ë¬¸ì„œë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

            # 2. ê° ì²­í¬ì—ì„œ KG ì¶”ì¶œ
            chunk_graphs = []

            # ì²­í¬ ë””ë²„ê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
            if output_dir:
                chunk_debug_dir = Path(output_dir) / "chunk_kg_debug"
                chunk_debug_dir.mkdir(parents=True, exist_ok=True)
            else:
                chunk_debug_dir = None

            for idx, chunk_group in enumerate(chunks):
                chunk_id = f"chunk_{idx+1:03d}"
                chunk_text = chunk_group.get_total_content()
                parent_context = chunk_group.parent_context or "ë¬¸ì„œ ë£¨íŠ¸"

                self.logger.info(f"ğŸ” ì²­í¬ {idx+1}/{len(chunks)} KG ì¶”ì¶œ ì¤‘... ({len(chunk_text):,}ì)")

                # 2-Phase ì¶”ì¶œ ì‚¬ìš© (ì—”í‹°í‹° ë¨¼ì €, ê´€ê³„ ë‚˜ì¤‘)
                kg_data = self._extract_kg_from_chunk_2phase(
                    chunk_text=chunk_text,
                    chunk_id=chunk_id,
                    parent_context=parent_context,
                    structure_info=structure_info,
                    llm_config=llm_config or {},
                    debug_dir=chunk_debug_dir,
                    extraction_level=extraction_level,
                    document_title=document_title
                )

                # kg_dataê°€ Noneì´ë©´ ì´ë¯¸ ì˜ˆì™¸ê°€ ë°œìƒí–ˆì„ ê²ƒì´ë¯€ë¡œ ì—¬ê¸°ê¹Œì§€ ì˜¤ì§€ ì•ŠìŒ
                chunk_graphs.append({
                    "chunk_id": chunk_id,
                    "graph": kg_data,
                    "level": chunk_group.level,
                    "nodes_in_chunk": chunk_group.nodes
                })

            # 3. ì²­í¬ë³„ KG ë³‘í•©
            self.logger.info(f"ğŸ”— {len(chunk_graphs)}ê°œ ì²­í¬ KG ë³‘í•© ì¤‘...")
            merged_kg = self._merge_chunk_graphs(chunk_graphs)

            # 4. ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result = self._enrich_kg_with_metadata(
                merged_kg,
                file_path,
                domain,
                structure_info
            )

            # ì²­í‚¹ ì •ë³´ ì¶”ê°€
            result["chunking_stats"] = {
                "total_chunks": len(chunks),
                "successful_extractions": len(chunk_graphs),
                "max_chunk_tokens": max_chunk_tokens
            }

            self.logger.info(
                f"âœ… Full KG ìƒì„± ì™„ë£Œ: "
                f"{result['stats']['entity_count']}ê°œ ì—”í‹°í‹°, "
                f"{result['stats']['relationship_count']}ê°œ ê´€ê³„ "
                f"(from {len(chunks)} chunks)"
            )

            return result

        except Exception as e:
            self.logger.error(f"âŒ Full KG ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return self._create_error_result(str(e))

    def _extract_kg_from_chunk_2phase(
        self,
        chunk_text: str,
        chunk_id: str,
        parent_context: str,
        structure_info: Optional[Dict[str, Any]],
        llm_config: Dict[str, Any],
        debug_dir: Optional[Path] = None,
        extraction_level: str = "standard",
        document_title: str = "Untitled Document"
    ) -> Optional[Dict[str, Any]]:
        """2-Phase ì¶”ì¶œ: 1ë‹¨ê³„ ì—”í‹°í‹°, 2ë‹¨ê³„ ê´€ê³„

        Args:
            extraction_level: ì¶”ì¶œ ìˆ˜ì¤€ ("brief", "standard", "deep")
            document_title: ë¬¸ì„œ ì œëª© (íŒŒì¼ëª… ë˜ëŠ” íƒ€ì´í‹€)
        """
        try:
            from prompts.templates import KnowledgeGraphPrompts
            import json

            # === Phase 1: ì—”í‹°í‹°ë§Œ ì¶”ì¶œ ===
            # ì¶”ì¶œ ë ˆë²¨ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ì„ íƒ
            level_prompts = {
                "brief": KnowledgeGraphPrompts.PHASE1_ENTITY_BRIEF,
                "standard": KnowledgeGraphPrompts.PHASE1_ENTITY_STANDARD,
                "deep": KnowledgeGraphPrompts.PHASE1_ENTITY_DEEP
            }

            entity_template = level_prompts.get(extraction_level.lower(), level_prompts["standard"])

            self.logger.info(f"ğŸ” {chunk_id} Phase 1: ì—”í‹°í‹° ì¶”ì¶œ ì¤‘... (ë ˆë²¨: {extraction_level}, ë¬¸ì„œ: {document_title})")

            entity_prompt = entity_template.format(text=chunk_text, document_title=document_title)

            # ë””ë²„ê·¸: Phase 1 í”„ë¡¬í”„íŠ¸ ì €ì¥
            if debug_dir:
                (debug_dir / f"{chunk_id}_phase1_prompt.txt").write_text(entity_prompt, encoding='utf-8')

            # Phase 1 LLM í˜¸ì¶œ
            phase1_response = self._call_llm_for_kg(entity_prompt, llm_config)

            if not phase1_response.get("success"):
                error_msg = f"{chunk_id} Phase 1 LLM í˜¸ì¶œ ì‹¤íŒ¨: {phase1_response.get('error')}"
                self.logger.error(f"âŒ {error_msg}")
                if debug_dir:
                    (debug_dir / f"{chunk_id}_phase1_error.txt").write_text(phase1_response.get('error', ''), encoding='utf-8')
                raise ValueError(error_msg)

            phase1_raw = phase1_response.get("response", "")

            # ë””ë²„ê·¸: Phase 1 ì‘ë‹µ ì €ì¥
            if debug_dir:
                (debug_dir / f"{chunk_id}_phase1_response.txt").write_text(phase1_raw, encoding='utf-8')

            # Phase 1 íŒŒì‹±
            entities_data = self._parse_kg_response(phase1_raw)
            entities = entities_data.get('entities', entities_data.get('nodes', []))

            if not entities:
                error_msg = f"{chunk_id} Phase 1 ì‹¤íŒ¨: ì—”í‹°í‹°ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
                self.logger.error(f"âŒ {error_msg}")
                if debug_dir:
                    (debug_dir / f"{chunk_id}_phase1_parse_error.txt").write_text(
                        f"{error_msg}\n\nResponse: {phase1_raw[:1000]}", encoding='utf-8'
                    )
                raise ValueError(error_msg)

            self.logger.info(f"âœ… {chunk_id} Phase 1 ì™„ë£Œ: {len(entities)}ê°œ ì—”í‹°í‹° ì¶”ì¶œ")

            # === Phase 2: ê´€ê³„ë§Œ ì¶”ì¶œ ===
            self.logger.info(f"ğŸ”— {chunk_id} Phase 2: ê´€ê³„ ì¶”ì¶œ ì¤‘...")

            # ì—”í‹°í‹° ëª©ë¡ì„ JSONìœ¼ë¡œ ë³€í™˜ (ê°„ê²°í•˜ê²Œ)
            entities_json = json.dumps([
                {"id": e.get("id"), "type": e.get("type"), "name": e.get("properties", {}).get("name", "Unknown")}
                for e in entities
            ], ensure_ascii=False, indent=2)

            relation_prompt = KnowledgeGraphPrompts.PHASE2_RELATION_ONLY.format(
                entities_json=entities_json,
                text=chunk_text[:5000]  # í…ìŠ¤íŠ¸ëŠ” ì•ë¶€ë¶„ë§Œ (í† í° ì ˆì•½)
            )

            # ë””ë²„ê·¸: Phase 2 í”„ë¡¬í”„íŠ¸ ì €ì¥
            if debug_dir:
                (debug_dir / f"{chunk_id}_phase2_prompt.txt").write_text(relation_prompt, encoding='utf-8')

            # Phase 2 LLM í˜¸ì¶œ
            phase2_response = self._call_llm_for_kg(relation_prompt, llm_config)

            if not phase2_response.get("success"):
                error_msg = f"{chunk_id} Phase 2 LLM í˜¸ì¶œ ì‹¤íŒ¨: {phase2_response.get('error')}"
                self.logger.error(f"âŒ {error_msg}")
                if debug_dir:
                    (debug_dir / f"{chunk_id}_phase2_error.txt").write_text(phase2_response.get('error', ''), encoding='utf-8')
                raise ValueError(error_msg)

            phase2_raw = phase2_response.get("response", "")

            # ë””ë²„ê·¸: Phase 2 ì‘ë‹µ ì €ì¥
            if debug_dir:
                (debug_dir / f"{chunk_id}_phase2_response.txt").write_text(phase2_raw, encoding='utf-8')

            # Phase 2 íŒŒì‹±
            relations_data = self._parse_kg_response(phase2_raw)
            relationships = relations_data.get('relationships', relations_data.get('edges', []))

            self.logger.info(f"âœ… {chunk_id} Phase 2 ì™„ë£Œ: {len(relationships)}ê°œ ê´€ê³„ ì¶”ì¶œ")

            # === ê²°ê³¼ ë³‘í•© ===
            kg_data = {
                "nodes": entities,
                "edges": relationships
            }

            # ë””ë²„ê·¸: ìµœì¢… KG ì €ì¥
            if debug_dir:
                (debug_dir / f"{chunk_id}_kg_2phase.json").write_text(
                    json.dumps(kg_data, ensure_ascii=False, indent=2),
                    encoding='utf-8'
                )

            self.logger.info(
                f"âœ… {chunk_id} 2-Phase ì¶”ì¶œ ì™„ë£Œ: "
                f"{len(entities)}ê°œ ì—”í‹°í‹°, {len(relationships)}ê°œ ê´€ê³„"
            )

            return kg_data

        except Exception as e:
            self.logger.error(f"âŒ {chunk_id} 2-Phase KG ì¶”ì¶œ ì‹¤íŒ¨: {e}", exc_info=True)
            if debug_dir:
                (debug_dir / f"{chunk_id}_2phase_exception.txt").write_text(str(e), encoding='utf-8')
            raise

    def _extract_kg_from_chunk(
        self,
        chunk_text: str,
        chunk_id: str,
        parent_context: str,
        structure_info: Optional[Dict[str, Any]],
        llm_config: Dict[str, Any],
        debug_dir: Optional[Path] = None
    ) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ì²­í¬ì—ì„œ KG ì¶”ì¶œ"""
        try:
            from prompts.templates import KnowledgeGraphPrompts

            # êµ¬ì¡° ì •ë³´ ìš”ì•½
            structure_summary = self._summarize_structure(structure_info) if structure_info else "êµ¬ì¡° ì •ë³´ ì—†ìŒ"

            # ìƒì„¸ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = KnowledgeGraphPrompts.DETAILED_KG_EXTRACTION.format(
                text=chunk_text,
                chunk_id=chunk_id,
                structure_info=structure_summary,
                parent_context=parent_context
            )

            # ë””ë²„ê·¸: ì²­í¬ í…ìŠ¤íŠ¸ ì €ì¥
            if debug_dir:
                chunk_text_file = debug_dir / f"{chunk_id}_text.txt"
                chunk_text_file.write_text(chunk_text, encoding='utf-8')

            # ë””ë²„ê·¸: í”„ë¡¬í”„íŠ¸ ì €ì¥
            if debug_dir:
                prompt_file = debug_dir / f"{chunk_id}_prompt.txt"
                prompt_file.write_text(prompt, encoding='utf-8')
                self.logger.debug(f"ğŸ“ {chunk_id} í”„ë¡¬í”„íŠ¸ ì €ì¥: {prompt_file}")

            # LLM í˜¸ì¶œ
            llm_response = self._call_llm_for_kg(prompt, llm_config)

            if not llm_response.get("success"):
                error_msg = f"{chunk_id} LLM í˜¸ì¶œ ì‹¤íŒ¨: {llm_response.get('error')}"
                self.logger.error(f"âŒ {error_msg}")
                # ë””ë²„ê·¸: ì˜¤ë¥˜ ì €ì¥
                if debug_dir:
                    error_file = debug_dir / f"{chunk_id}_error.txt"
                    error_file.write_text(llm_response.get('error', 'Unknown error'), encoding='utf-8')
                # ì¹˜ëª…ì  ì˜¤ë¥˜ë¡œ ì˜ˆì™¸ ë°œìƒì‹œì¼œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ë‹¨
                raise ValueError(error_msg)

            # ì‘ë‹µ íŒŒì‹±
            raw_response = llm_response.get("response", "")

            # ë””ë²„ê·¸: LLM ì‘ë‹µ ì €ì¥
            if debug_dir:
                response_file = debug_dir / f"{chunk_id}_response.txt"
                response_file.write_text(raw_response, encoding='utf-8')
                self.logger.debug(f"ğŸ“ {chunk_id} ì‘ë‹µ ì €ì¥: {response_file}")

            kg_data = self._parse_kg_response(raw_response)

            # í•µì‹¬ ê²€ì¦: íŒŒì‹± ê²°ê³¼ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¹˜ëª…ì  ì˜¤ë¥˜ë¡œ ì²˜ë¦¬
            if not kg_data.get('nodes') and not kg_data.get('edges'):
                error_msg = f"{chunk_id} KG ì¶”ì¶œ ì‹¤íŒ¨: LLM ì‘ë‹µ íŒŒì‹± ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. JSON í˜•ì‹ ì˜¤ë¥˜ ë˜ëŠ” max_tokens ì´ˆê³¼ ê°€ëŠ¥ì„±"
                self.logger.error(f"âŒ {error_msg}")

                # ë””ë²„ê·¸: íŒŒì‹± ì‹¤íŒ¨ ìƒì„¸ ì •ë³´ ì €ì¥
                if debug_dir:
                    error_detail_file = debug_dir / f"{chunk_id}_parse_error.txt"
                    error_detail_file.write_text(
                        f"Error: {error_msg}\n\n"
                        f"Response length: {len(raw_response)}\n"
                        f"Response preview (last 500 chars):\n{raw_response[-500:]}\n\n"
                        f"Parsed result: {json.dumps(kg_data, ensure_ascii=False, indent=2)}",
                        encoding='utf-8'
                    )

                # ì¹˜ëª…ì  ì˜¤ë¥˜ë¡œ None ë°˜í™˜í•˜ì—¬ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ë‹¨
                raise ValueError(error_msg)

            # ë””ë²„ê·¸: íŒŒì‹±ëœ KG ë°ì´í„° ì €ì¥
            if debug_dir:
                kg_file = debug_dir / f"{chunk_id}_kg.json"
                kg_file.write_text(
                    json.dumps(kg_data, ensure_ascii=False, indent=2),
                    encoding='utf-8'
                )
                self.logger.debug(f"ğŸ“ {chunk_id} KG ì €ì¥: {kg_file}")

            self.logger.info(
                f"âœ… {chunk_id} ì¶”ì¶œ ì™„ë£Œ: "
                f"{len(kg_data.get('nodes', []))}ê°œ ì—”í‹°í‹°, "
                f"{len(kg_data.get('edges', []))}ê°œ ê´€ê³„"
            )

            return kg_data

        except Exception as e:
            self.logger.error(f"âŒ {chunk_id} KG ì¶”ì¶œ ì‹¤íŒ¨: {e}", exc_info=True)
            # ë””ë²„ê·¸: ì˜ˆì™¸ ì €ì¥
            if debug_dir:
                exception_file = debug_dir / f"{chunk_id}_exception.txt"
                exception_file.write_text(str(e), encoding='utf-8')
            # ì¹˜ëª…ì  ì˜¤ë¥˜ì´ë¯€ë¡œ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ë‹¨
            raise

    def _merge_chunk_graphs(self, chunk_graphs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ì²­í¬ì˜ KGë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©"""
        merged_nodes = []
        merged_edges = []
        node_id_map = {}  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ë§¤í•‘

        for chunk_data in chunk_graphs:
            chunk_id = chunk_data["chunk_id"]
            graph = chunk_data["graph"]

            # ë…¸ë“œ ë³‘í•© (ID ì¶©ëŒ ë°©ì§€)
            for node in graph.get("nodes", []):
                original_id = node["id"]
                new_id = f"{chunk_id}_{original_id}"

                # ë™ì¼í•œ ì—”í‹°í‹° ì¤‘ë³µ ì²´í¬ (ì´ë¦„ê³¼ íƒ€ì…ì´ ê°™ìœ¼ë©´ ë³‘í•©)
                node_key = (node.get("type"), node.get("properties", {}).get("name"))

                if node_key in node_id_map:
                    # ê¸°ì¡´ ë…¸ë“œ ID ì‚¬ìš© (ì¤‘ë³µ ì œê±°)
                    node_id_map[original_id] = node_id_map[node_key]
                else:
                    # ìƒˆ ë…¸ë“œ ì¶”ê°€
                    node["id"] = new_id
                    merged_nodes.append(node)
                    node_id_map[original_id] = new_id
                    node_id_map[node_key] = new_id

            # ê´€ê³„ ë³‘í•© (ID ì—…ë°ì´íŠ¸)
            for edge in graph.get("edges", []):
                # ì›ë³¸ IDë¥¼ ë³‘í•©ëœ IDë¡œ ë³€í™˜
                source = edge.get("source", "")
                target = edge.get("target", "")

                # chunk_id ì ‘ë‘ì‚¬ ì œê±° í›„ ë§¤í•‘
                source_base = source.split("_", 1)[-1] if "_" in source else source
                target_base = target.split("_", 1)[-1] if "_" in target else target

                new_source = node_id_map.get(source_base, f"{chunk_id}_{source}")
                new_target = node_id_map.get(target_base, f"{chunk_id}_{target}")

                edge["source"] = new_source
                edge["target"] = new_target
                edge["id"] = f"{chunk_id}_{edge.get('id', len(merged_edges))}"

                merged_edges.append(edge)

        self.logger.info(
            f"ğŸ”— ë³‘í•© ì™„ë£Œ: {len(merged_nodes)}ê°œ ì—”í‹°í‹° (ì¤‘ë³µ ì œê±° í›„), "
            f"{len(merged_edges)}ê°œ ê´€ê³„"
        )

        return {
            "nodes": merged_nodes,
            "edges": merged_edges
        }
