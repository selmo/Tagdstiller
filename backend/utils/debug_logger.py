"""
ë””ë²„ê·¸ ë¡œê¹… ì‹œìŠ¤í…œ
í‚¤ì›Œë“œ ì¶”ì¶œ ê³¼ì •ì˜ ê° ë‹¨ê³„ë³„ ì¤‘ê°„ ê²°ê³¼ë¬¼ì„ ì €ì¥í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆë„ë¡ ì§€ì›
"""

import json
import os
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
import numpy as np


class DebugLogger:
    """í‚¤ì›Œë“œ ì¶”ì¶œ ê³¼ì •ì˜ ë””ë²„ê·¸ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¡œê¹…í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, base_dir: str = "tests/debug_outputs", enable_debug: bool = False):
        """
        Args:
            base_dir: ë””ë²„ê·¸ ë¡œê·¸ë¥¼ ì €ì¥í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬
            enable_debug: ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
        """
        self.enable_debug = enable_debug or os.getenv("ENABLE_KEYWORD_DEBUG", "false").lower() == "true"
        
        if self.enable_debug:
            self.base_dir = Path(base_dir)
            self.base_dir.mkdir(exist_ok=True, parents=True)
            self.session_id = str(uuid.uuid4())[:8]
            self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ì„¸ì…˜ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
            self.session_dir = self.base_dir / f"{self.timestamp}_{self.session_id}"
            self.session_dir.mkdir(exist_ok=True, parents=True)
            
            self.debug_data = {
                "session_info": {
                    "session_id": self.session_id,
                    "timestamp": self.timestamp,
                    "start_time": datetime.now().isoformat()
                },
                "extraction_steps": []
            }
            
            print(f"ğŸ› ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” - ì„¸ì…˜: {self.session_id}")
            print(f"ğŸ“ ë¡œê·¸ ì €ì¥ ìœ„ì¹˜: {self.session_dir}")
    
    def start_extraction(self, extractor_name: str, file_info: Dict, text: str, config: Dict = None):
        """í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘ ë¡œê¹…"""
        if not self.enable_debug:
            return
            
        step_data = {
            "step": "start_extraction",
            "extractor": extractor_name,
            "timestamp": datetime.now().isoformat(),
            "file_info": {
                "filename": file_info.get("filename", "unknown"),
                "file_id": file_info.get("id"),
                "file_size": len(text) if text else 0,
                "text_preview": text[:200] + "..." if text and len(text) > 200 else text
            },
            "config": config or {},
            "text_stats": self._analyze_text(text) if text else {}
        }
        
        self.debug_data["extraction_steps"].append(step_data)
        self._save_text_file("input_text.txt", text or "")
        
        print(f"ğŸ› [{extractor_name}] ì¶”ì¶œ ì‹œì‘ - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text) if text else 0}ì")
    
    def log_preprocessing(self, extractor_name: str, original_text: str, 
                         preprocessed_text: str, preprocessing_steps: List[str]):
        """ì „ì²˜ë¦¬ ë‹¨ê³„ ë¡œê¹…"""
        if not self.enable_debug:
            return
            
        step_data = {
            "step": "preprocessing",
            "extractor": extractor_name,
            "timestamp": datetime.now().isoformat(),
            "original_stats": self._analyze_text(original_text),
            "preprocessed_stats": self._analyze_text(preprocessed_text),
            "preprocessing_steps": preprocessing_steps,
            "text_change_ratio": len(preprocessed_text) / len(original_text) if original_text else 0
        }
        
        self.debug_data["extraction_steps"].append(step_data)
        self._save_text_file(f"{extractor_name}_preprocessed.txt", preprocessed_text)
        
        print(f"ğŸ› [{extractor_name}] ì „ì²˜ë¦¬ ì™„ë£Œ - {len(original_text)} â†’ {len(preprocessed_text)}ì")
    
    def log_candidate_generation(self, extractor_name: str, candidates: List[str], 
                               generation_method: str, params: Dict = None):
        """í‚¤ì›Œë“œ í›„ë³´ ìƒì„± ë‹¨ê³„ ë¡œê¹…"""
        if not self.enable_debug:
            return
            
        step_data = {
            "step": "candidate_generation", 
            "extractor": extractor_name,
            "timestamp": datetime.now().isoformat(),
            "method": generation_method,
            "params": params or {},
            "candidate_count": len(candidates),
            "candidates": candidates[:50],  # ì²˜ìŒ 50ê°œë§Œ ì €ì¥
            "candidate_stats": self._analyze_candidates(candidates)
        }
        
        self.debug_data["extraction_steps"].append(step_data)
        self._save_json_file(f"{extractor_name}_candidates.json", {
            "method": generation_method,
            "count": len(candidates),
            "candidates": candidates
        })
        
        print(f"ğŸ› [{extractor_name}] í›„ë³´ ìƒì„± ì™„ë£Œ - {generation_method}: {len(candidates)}ê°œ")
    
    def log_embeddings(self, extractor_name: str, model_name: str,
                      doc_embedding: Optional[np.ndarray] = None,
                      candidate_embeddings: Optional[np.ndarray] = None):
        """ì„ë² ë”© ê³„ì‚° ë‹¨ê³„ ë¡œê¹…"""
        if not self.enable_debug:
            return
            
        step_data = {
            "step": "embeddings",
            "extractor": extractor_name, 
            "timestamp": datetime.now().isoformat(),
            "model_name": model_name,
            "doc_embedding_shape": doc_embedding.shape if doc_embedding is not None else None,
            "candidate_embeddings_shape": candidate_embeddings.shape if candidate_embeddings is not None else None,
            "embedding_stats": {}
        }
        
        if doc_embedding is not None:
            step_data["embedding_stats"]["doc_embedding"] = {
                "mean": float(np.mean(doc_embedding)),
                "std": float(np.std(doc_embedding)),
                "norm": float(np.linalg.norm(doc_embedding))
            }
        
        if candidate_embeddings is not None:
            step_data["embedding_stats"]["candidate_embeddings"] = {
                "mean": float(np.mean(candidate_embeddings)),
                "std": float(np.std(candidate_embeddings)),
                "min_norm": float(np.min(np.linalg.norm(candidate_embeddings, axis=1))),
                "max_norm": float(np.max(np.linalg.norm(candidate_embeddings, axis=1)))
            }
            
            # ì„ë² ë”© ì €ì¥ (ì„ íƒì‚¬í•­ - ìš©ëŸ‰ ë§ì´ ì°¨ì§€í•¨)
            if os.getenv("SAVE_EMBEDDINGS", "false").lower() == "true":
                np.save(self.session_dir / f"{extractor_name}_doc_embedding.npy", doc_embedding)
                np.save(self.session_dir / f"{extractor_name}_candidate_embeddings.npy", candidate_embeddings)
        
        self.debug_data["extraction_steps"].append(step_data)
        print(f"ğŸ› [{extractor_name}] ì„ë² ë”© ì™„ë£Œ - ëª¨ë¸: {model_name}")
    
    def log_similarity_calculation(self, extractor_name: str, similarities: np.ndarray,
                                 candidates: List[str], method: str = "cosine"):
        """ìœ ì‚¬ë„ ê³„ì‚° ë‹¨ê³„ ë¡œê¹…"""
        if not self.enable_debug:
            return
            
        # ìœ ì‚¬ë„-í‚¤ì›Œë“œ ë§¤í•‘
        similarity_results = [
            {"candidate": candidate, "similarity": float(sim)}
            for candidate, sim in zip(candidates, similarities)
        ]
        
        # ìƒìœ„/í•˜ìœ„ ê²°ê³¼
        sorted_results = sorted(similarity_results, key=lambda x: x["similarity"], reverse=True)
        
        step_data = {
            "step": "similarity_calculation",
            "extractor": extractor_name,
            "timestamp": datetime.now().isoformat(),
            "method": method,
            "similarity_stats": {
                "count": len(similarities),
                "mean": float(np.mean(similarities)),
                "std": float(np.std(similarities)),
                "min": float(np.min(similarities)),
                "max": float(np.max(similarities)),
                "median": float(np.median(similarities))
            },
            "top_10": sorted_results[:10],
            "bottom_5": sorted_results[-5:] if len(sorted_results) > 5 else []
        }
        
        self.debug_data["extraction_steps"].append(step_data)
        self._save_json_file(f"{extractor_name}_similarities.json", {
            "method": method,
            "results": similarity_results
        })
        
        print(f"ğŸ› [{extractor_name}] ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ - {method}, ë²”ìœ„: {np.min(similarities):.3f}~{np.max(similarities):.3f}")
    
    def log_algorithm_application(self, extractor_name: str, algorithm: str,
                                input_candidates: List[tuple], output_keywords: List[tuple],
                                algorithm_params: Dict = None):
        """ì•Œê³ ë¦¬ì¦˜ ì ìš© ë‹¨ê³„ ë¡œê¹… (MMR, Max Sum ë“±)"""
        if not self.enable_debug:
            return
            
        step_data = {
            "step": "algorithm_application",
            "extractor": extractor_name,
            "timestamp": datetime.now().isoformat(),
            "algorithm": algorithm,
            "algorithm_params": algorithm_params or {},
            "input_count": len(input_candidates),
            "output_count": len(output_keywords),
            "input_candidates": input_candidates[:20],  # ì²˜ìŒ 20ê°œ
            "output_keywords": output_keywords,
            "selection_ratio": len(output_keywords) / len(input_candidates) if input_candidates else 0
        }
        
        # ì„ íƒ/ì œì™¸ëœ í‚¤ì›Œë“œ ë¶„ì„
        selected_keywords = {kw[0] for kw in output_keywords}
        excluded_keywords = [(kw, score) for kw, score in input_candidates if kw not in selected_keywords]
        step_data["excluded_keywords"] = excluded_keywords[:10]
        
        self.debug_data["extraction_steps"].append(step_data)
        self._save_json_file(f"{extractor_name}_{algorithm}_results.json", {
            "algorithm": algorithm,
            "params": algorithm_params,
            "input": input_candidates,
            "output": output_keywords,
            "excluded": excluded_keywords
        })
        
        print(f"ğŸ› [{extractor_name}] {algorithm} ì ìš© ì™„ë£Œ - {len(input_candidates)} â†’ {len(output_keywords)}ê°œ")
    
    def log_position_analysis(self, extractor_name: str, keywords_with_positions: List[Dict],
                            text: str, analysis_method: str = "simple_search"):
        """í‚¤ì›Œë“œ ìœ„ì¹˜ ë¶„ì„ ë‹¨ê³„ ë¡œê¹…"""
        if not self.enable_debug:
            return
            
        step_data = {
            "step": "position_analysis",
            "extractor": extractor_name,
            "timestamp": datetime.now().isoformat(),
            "analysis_method": analysis_method,
            "keywords_count": len(keywords_with_positions),
            "keywords_with_positions": keywords_with_positions[:10],  # ì²˜ìŒ 10ê°œë§Œ
            "position_stats": self._analyze_positions(keywords_with_positions, text)
        }
        
        self.debug_data["extraction_steps"].append(step_data)
        self._save_json_file(f"{extractor_name}_positions.json", {
            "method": analysis_method,
            "text_length": len(text),
            "keywords": keywords_with_positions
        })
        
        positioned_count = sum(1 for kw in keywords_with_positions if kw.get("positions"))
        print(f"ğŸ› [{extractor_name}] ìœ„ì¹˜ ë¶„ì„ ì™„ë£Œ - {positioned_count}/{len(keywords_with_positions)}ê°œ ìœ„ì¹˜ í™•ì¸")
    
    def log_final_results(self, extractor_name: str, final_keywords: List[Dict],
                         extraction_time: float, total_processing_time: float):
        """ìµœì¢… ê²°ê³¼ ë¡œê¹…"""
        if not self.enable_debug:
            return
            
        step_data = {
            "step": "final_results",
            "extractor": extractor_name,
            "timestamp": datetime.now().isoformat(),
            "final_keywords": final_keywords,
            "keyword_count": len(final_keywords),
            "extraction_time": extraction_time,
            "total_processing_time": total_processing_time,
            "performance_stats": {
                "keywords_per_second": len(final_keywords) / extraction_time if extraction_time > 0 else 0,
                "avg_score": np.mean([kw.get("score", 0) for kw in final_keywords]) if final_keywords else 0
            }
        }
        
        self.debug_data["extraction_steps"].append(step_data)
        self.debug_data["session_info"]["end_time"] = datetime.now().isoformat()
        self.debug_data["session_info"]["total_time"] = total_processing_time
        
        # ìµœì¢… ìš”ì•½ ì €ì¥
        self._save_final_summary(extractor_name, final_keywords, extraction_time)
        
        print(f"ğŸ› [{extractor_name}] ì¶”ì¶œ ì™„ë£Œ - {len(final_keywords)}ê°œ í‚¤ì›Œë“œ, {extraction_time:.2f}ì´ˆ")
    
    def save_debug_session(self):
        """ë””ë²„ê·¸ ì„¸ì…˜ ì „ì²´ ë°ì´í„°ë¥¼ ì €ì¥"""
        if not self.enable_debug:
            return
            
        # ë©”ì¸ ë””ë²„ê·¸ ë°ì´í„° ì €ì¥
        debug_file = self.session_dir / "debug_session.json"
        with open(debug_file, 'w', encoding='utf-8') as f:
            json.dump(self.debug_data, f, ensure_ascii=False, indent=2, default=str)
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_summary_report()
        
        print(f"ğŸ› ë””ë²„ê·¸ ì„¸ì…˜ ì €ì¥ ì™„ë£Œ: {debug_file}")
        print(f"ğŸ“Š ìš”ì•½ ë¦¬í¬íŠ¸: {self.session_dir / 'summary_report.html'}")
    
    def _analyze_text(self, text: str) -> Dict:
        """í…ìŠ¤íŠ¸ ê¸°ë³¸ í†µê³„ ë¶„ì„"""
        if not text:
            return {}
            
        words = text.split()
        sentences = text.split('.')
        
        return {
            "length": len(text),
            "word_count": len(words),
            "sentence_count": len([s for s in sentences if s.strip()]),
            "avg_word_length": np.mean([len(word) for word in words]) if words else 0,
            "unique_words": len(set(words)),
            "word_diversity": len(set(words)) / len(words) if words else 0
        }
    
    def _analyze_candidates(self, candidates: List[str]) -> Dict:
        """í‚¤ì›Œë“œ í›„ë³´ë“¤ í†µê³„ ë¶„ì„"""
        if not candidates:
            return {}
            
        lengths = [len(candidate) for candidate in candidates]
        word_counts = [len(candidate.split()) for candidate in candidates]
        
        return {
            "total_count": len(candidates),
            "unique_count": len(set(candidates)),
            "avg_length": np.mean(lengths),
            "avg_word_count": np.mean(word_counts),
            "single_word_ratio": sum(1 for wc in word_counts if wc == 1) / len(word_counts),
            "multi_word_ratio": sum(1 for wc in word_counts if wc > 1) / len(word_counts)
        }
    
    def _analyze_positions(self, keywords_with_positions: List[Dict], text: str) -> Dict:
        """í‚¤ì›Œë“œ ìœ„ì¹˜ ì •ë³´ í†µê³„ ë¶„ì„"""
        if not keywords_with_positions:
            return {}
            
        positioned_keywords = [kw for kw in keywords_with_positions if kw.get("positions")]
        all_positions = []
        
        for kw in positioned_keywords:
            positions = kw.get("positions", [])
            all_positions.extend([pos.get("start", 0) for pos in positions])
        
        return {
            "positioned_count": len(positioned_keywords),
            "total_positions": len(all_positions),
            "coverage_ratio": len(positioned_keywords) / len(keywords_with_positions) if keywords_with_positions else 0,
            "avg_positions_per_keyword": len(all_positions) / len(positioned_keywords) if positioned_keywords else 0,
            "text_coverage": {
                "start": min(all_positions) if all_positions else 0,
                "end": max(all_positions) if all_positions else 0,
                "span": max(all_positions) - min(all_positions) if all_positions else 0
            }
        }
    
    def _save_text_file(self, filename: str, content: str):
        """í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥"""
        if not self.enable_debug:
            return
            
        file_path = self.session_dir / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def _save_json_file(self, filename: str, data: Any):
        """JSON íŒŒì¼ ì €ì¥"""
        if not self.enable_debug:
            return
            
        file_path = self.session_dir / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2, default=str)
    
    def _save_final_summary(self, extractor_name: str, final_keywords: List[Dict], extraction_time: float):
        """ìµœì¢… ìš”ì•½ ì •ë³´ ì €ì¥"""
        summary = {
            "extractor": extractor_name,
            "session_id": self.session_id,
            "extraction_time": extraction_time,
            "keyword_count": len(final_keywords),
            "keywords": final_keywords,
            "top_keywords": sorted(final_keywords, key=lambda x: x.get("score", 0), reverse=True)[:5]
        }
        
        self._save_json_file(f"{extractor_name}_summary.json", summary)
    
    def _generate_summary_report(self):
        """HTML í˜•ì‹ì˜ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.enable_debug:
            return
            
        html_content = self._build_html_report()
        report_file = self.session_dir / "summary_report.html"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
    
    def _build_html_report(self) -> str:
        """HTML ë¦¬í¬íŠ¸ ë¹Œë“œ"""
        extractors = list(set([step.get("extractor") for step in self.debug_data["extraction_steps"] if step.get("extractor")]))
        
        html = f"""
        <!DOCTYPE html>
        <html lang="ko">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>í‚¤ì›Œë“œ ì¶”ì¶œ ë””ë²„ê·¸ ë¦¬í¬íŠ¸</title>
            <style>
                body {{ font-family: -apple-system, BlinkMacSystemFont, sans-serif; margin: 40px; }}
                .header {{ background: #f8f9fa; padding: 20px; border-radius: 8px; margin-bottom: 20px; }}
                .extractor {{ margin-bottom: 30px; padding: 20px; border: 1px solid #dee2e6; border-radius: 8px; }}
                .step {{ margin: 10px 0; padding: 15px; background: #f1f3f4; border-radius: 4px; }}
                .keyword {{ display: inline-block; padding: 4px 8px; margin: 2px; background: #e3f2fd; border-radius: 4px; }}
                .score {{ color: #1976d2; font-weight: bold; }}
                table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}
                th, td {{ padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }}
                th {{ background-color: #f2f2f2; }}
                .stats {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 15px 0; }}
                .stat-card {{ padding: 15px; background: white; border: 1px solid #e0e0e0; border-radius: 6px; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸ› í‚¤ì›Œë“œ ì¶”ì¶œ ë””ë²„ê·¸ ë¦¬í¬íŠ¸</h1>
                <p><strong>ì„¸ì…˜ ID:</strong> {self.session_id}</p>
                <p><strong>ìƒì„± ì‹œê°„:</strong> {self.timestamp}</p>
                <p><strong>ì¶”ì¶œê¸°:</strong> {', '.join(extractors)}</p>
            </div>
        """
        
        for extractor in extractors:
            extractor_steps = [step for step in self.debug_data["extraction_steps"] if step.get("extractor") == extractor]
            html += f"<div class='extractor'><h2>ğŸ“Š {extractor} ì¶”ì¶œê¸°</h2>"
            
            for step in extractor_steps:
                step_name = step.get("step", "unknown")
                html += f"<div class='step'><h3>{step_name}</h3>"
                
                if step_name == "final_results":
                    keywords = step.get("final_keywords", [])
                    html += f"<p><strong>ìµœì¢… í‚¤ì›Œë“œ {len(keywords)}ê°œ:</strong></p><div>"
                    for kw in keywords[:10]:  # ìƒìœ„ 10ê°œë§Œ
                        score = kw.get("score", 0)
                        html += f"<span class='keyword'>{kw.get('keyword', 'N/A')} <span class='score'>({score:.3f})</span></span>"
                    html += "</div>"
                
                html += "</div>"
            
            html += "</div>"
        
        html += """
            <div style="margin-top: 40px; padding: 20px; background: #f8f9fa; border-radius: 8px;">
                <h3>ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤</h3>
                <ul>
                    <li><code>debug_session.json</code> - ì „ì²´ ì„¸ì…˜ ë°ì´í„°</li>
                    <li><code>*_candidates.json</code> - í‚¤ì›Œë“œ í›„ë³´ë“¤</li>
                    <li><code>*_similarities.json</code> - ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼</li>
                    <li><code>*_positions.json</code> - í‚¤ì›Œë“œ ìœ„ì¹˜ ì •ë³´</li>
                    <li><code>*_summary.json</code> - ìµœì¢… ìš”ì•½</li>
                </ul>
            </div>
        </body>
        </html>
        """
        
        return html


# ì „ì—­ ë””ë²„ê·¸ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
debug_logger: Optional[DebugLogger] = None

def get_debug_logger() -> DebugLogger:
    """ì „ì—­ ë””ë²„ê·¸ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global debug_logger
    if debug_logger is None:
        debug_logger = DebugLogger()
    return debug_logger

def init_debug_logger(enable_debug: bool = None) -> DebugLogger:
    """ë””ë²„ê·¸ ë¡œê±° ì´ˆê¸°í™”"""
    global debug_logger
    if enable_debug is None:
        enable_debug = os.getenv("ENABLE_KEYWORD_DEBUG", "false").lower() == "true"
    
    debug_logger = DebugLogger(enable_debug=enable_debug)
    return debug_logger