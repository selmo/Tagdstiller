from typing import List, Dict, Any, Optional
from pathlib import Path
import json
import time
import logging
from .base import KeywordExtractor, Keyword
from utils.text_cleaner import TextCleaner
from utils.position_mapper import PositionMapper
from utils.debug_logger import get_debug_logger


class LangExtractExtractor(KeywordExtractor):
    """LangExtract ê¸°ë°˜ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œê¸°"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, db_session = None):
        super().__init__("langextract", config)
        self.langextract_client = None
        self.ollama_config = self._get_ollama_config()
        
    def _get_ollama_config(self) -> Dict[str, Any]:
        """ê¸°ì¡´ Ollama ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
        return {
            'base_url': self.config.get('ollama_base_url', 'http://localhost:11434'),
            'model': self.config.get('ollama_model', 'llama3.2'),
            'timeout': self.config.get('ollama_timeout', 30)
        }
    
    def load_model(self) -> bool:
        """LangExtract í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        logger = logging.getLogger(__name__)
        
        try:
            logger.info(f"LangExtract ì¶”ì¶œê¸° ë¡œë“œ ì‹œì‘ - Ollama ì„¤ì •: {self.ollama_config}")
            
            # LangExtract import ë° ì´ˆê¸°í™”
            import langextract
            from langextract import factory
            
            # Ollama ëª¨ë¸ ìƒì„± (ìƒˆë¡œìš´ API ì‚¬ìš©)
            model_id = self.ollama_config['model']
            logger.info(f"ğŸ”„ Ollama ëª¨ë¸ ìƒì„± ì‹œë„: {model_id}")
            
            # factoryë¥¼ ì‚¬ìš©í•˜ì—¬ Ollama ëª¨ë¸ ìƒì„±
            ollama_model = factory.create_model_from_id(
                model_id=model_id,
                provider="ollama",
                base_url=self.ollama_config['base_url'],
                timeout=self.ollama_config['timeout']
            )
            
            # Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ (ì˜ˆì œ ë°ì´í„° í¬í•¨)
            logger.info(f"ğŸ”„ Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
            
            # ì˜ˆì œ ë°ì´í„° ìƒì„±
            from langextract.data import ExampleData
            examples = [
                ExampleData(
                    text="Apple Inc. is a technology company based in Cupertino.",
                    extractions={
                        "keywords": [
                            {"text": "Apple Inc.", "category": "organization"},
                            {"text": "technology", "category": "concept"},
                            {"text": "Cupertino", "category": "location"}
                        ]
                    }
                )
            ]
            
            test_result = langextract.extract(
                "Test document for keyword extraction",
                prompt_description="Extract important keywords and their categories",
                examples=examples,
                model=ollama_model,
                max_char_buffer=100
            )
            if test_result:
                self.langextract_client = {
                    'langextract': langextract,
                    'model': ollama_model,
                    'schema': self._get_extraction_schema(),
                    'config': self._get_langextract_config()
                }
                self.is_loaded = True
                logger.info(f"âœ… LangExtract ì¶”ì¶œê¸° ë¡œë“œ ì„±ê³µ - Ollama ëª¨ë¸: {self.ollama_config['model']}")
            else:
                self.is_loaded = False
                logger.error(f"âŒ Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                
            return self.is_loaded
            
        except ImportError as e:
            logger.error(f"âŒ LangExtract ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {e}")
            # ëŒ€ì²´ êµ¬í˜„: ì§ì ‘ Ollama í˜¸ì¶œ ë°©ì‹ ì‚¬ìš©
            try:
                logger.info("ğŸ”„ ëŒ€ì²´ êµ¬í˜„ìœ¼ë¡œ ì „í™˜ - ì§ì ‘ Ollama í˜¸ì¶œ ë°©ì‹")
                import requests
                
                # Ollama ì—°ê²° í…ŒìŠ¤íŠ¸
                response = requests.get(f"{self.ollama_config['base_url']}/api/tags", timeout=5)
                if response.status_code == 200:
                    models = response.json().get("models", [])
                    model_found = any(model["name"].startswith(self.ollama_config['model']) for model in models)
                    
                    if model_found:
                        self.langextract_client = {
                            'mode': 'direct_ollama',
                            'schema': self._get_extraction_schema(),
                            'config': self._get_langextract_config()
                        }
                        self.is_loaded = True
                        logger.info(f"âœ… LangExtract ì¶”ì¶œê¸° ë¡œë“œ ì„±ê³µ (ì§ì ‘ Ollama ë°©ì‹) - ëª¨ë¸: {self.ollama_config['model']}")
                    else:
                        logger.error(f"âŒ ëª¨ë¸ '{self.ollama_config['model']}' ì—†ìŒ")
                        self.is_loaded = False
                else:
                    logger.error(f"âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {response.status_code}")
                    self.is_loaded = False
                    
                return self.is_loaded
                
            except Exception as fallback_e:
                logger.error(f"âŒ ëŒ€ì²´ êµ¬í˜„ë„ ì‹¤íŒ¨: {fallback_e}")
                self.is_loaded = False
                return False
        except Exception as e:
            logger.error(f"âŒ LangExtract ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            return False
    
    def _get_extraction_schema(self) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ì¶”ì¶œìš© ìŠ¤í‚¤ë§ˆ ì •ì˜"""
        return {
            "type": "object",
            "properties": {
                "keywords": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "text": {
                                "type": "string",
                                "description": "ì¶”ì¶œëœ í‚¤ì›Œë“œ ë˜ëŠ” í•µì‹¬ êµ¬ë¬¸"
                            },
                            "category": {
                                "type": "string",
                                "description": "í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: ê¸°ìˆ , ì¸ë¬¼, ê°œë…, ì¡°ì§ ë“±)"
                            },
                            "confidence": {
                                "type": "number",
                                "minimum": 0,
                                "maximum": 1,
                                "description": "ì¶”ì¶œ ì‹ ë¢°ë„ (0.0-1.0)"
                            },
                            "importance": {
                                "type": "string",
                                "enum": ["high", "medium", "low"],
                                "description": "ì¤‘ìš”ë„ ìˆ˜ì¤€"
                            },
                            "semantic_type": {
                                "type": "string",
                                "description": "ì˜ë¯¸ì  ìœ í˜• (ëª…ì‚¬, ë™ì‚¬, ì „ë¬¸ìš©ì–´ ë“±)"
                            }
                        },
                        "required": ["text", "confidence"]
                    }
                }
            },
            "required": ["keywords"]
        }
    
    def _get_langextract_config(self) -> Dict[str, Any]:
        """LangExtract ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
        return {
            'max_entities': self.config.get('max_keywords', 15),
            'chunk_size': self.config.get('chunk_size', 2000),
            'overlap': self.config.get('overlap', 200),
            'confidence_threshold': self.config.get('confidence_threshold', 0.6),
            'temperature': 0.1,
            'max_tokens': 1500
        }
    
    def extract(self, text: str, file_path: Optional[Path] = None) -> List[Keyword]:
        """LangExtractë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        logger = logging.getLogger(__name__)
        debug_logger = get_debug_logger()
        start_time = time.time()
        
        logger.info(f"ğŸ” LangExtract í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘ - ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        # ë””ë²„ê·¸ ë¡œê¹…: ì¶”ì¶œ ì‹œì‘
        debug_logger.start_extraction(
            extractor_name="langextract",
            file_info={"filename": str(file_path) if file_path else "unknown", "id": None},
            text=text,
            config=self.config
        )
        
        # ìœ„ì¹˜ ë§¤í•‘ ìƒì„±
        position_mapper = PositionMapper()
        position_map = position_mapper.create_position_map(text, file_path)
        logger.info(f"ğŸ“ ìœ„ì¹˜ ë§¤í•‘ ìƒì„± ì™„ë£Œ - ì´ {position_map['total_pages']}í˜ì´ì§€, {position_map['total_lines']}ì¤„")
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        original_text_copy = text
        cleaned_text = TextCleaner.clean_text(text)
        logger.info(f"ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ - ì •ì œëœ ê¸¸ì´: {len(cleaned_text)} ë¬¸ì")
        
        # ë””ë²„ê·¸ ë¡œê¹…: ì „ì²˜ë¦¬ ê²°ê³¼
        debug_logger.log_preprocessing(
            extractor_name="langextract",
            original_text=original_text_copy,
            preprocessed_text=cleaned_text,
            preprocessing_steps=["clean_text", "normalize_unicode", "langextract_preprocessing"]
        )
        
        if not self.is_loaded:
            self.load_model()
        
        if not self.is_loaded:
            return []
        
        try:
            # í…ìŠ¤íŠ¸ ì²­í‚¹ (ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬)
            chunks = self._chunk_text(cleaned_text)
            logger.info(f"ğŸ“„ í…ìŠ¤íŠ¸ ì²­í‚¹ ì™„ë£Œ - {len(chunks)}ê°œ ì²­í¬")
            
            all_keywords = []
            langextract_config = self._get_langextract_config()
            
            # ë””ë²„ê·¸ ë¡œê¹…: ëª¨ë¸ ì •ë³´
            debug_logger.log_embeddings(
                extractor_name="langextract",
                model_name=f"ollama:{self.ollama_config['model']}"
            )
            
            for i, chunk in enumerate(chunks, 1):
                logger.info(f"ğŸ”„ ì²­í¬ {i}/{len(chunks)} ì²˜ë¦¬ ì¤‘ (ê¸¸ì´: {len(chunk)} ë¬¸ì)...")
                
                # LangExtract í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = self._create_extraction_prompt(chunk, langextract_config)
                
                # Ollamaë¥¼ í†µí•œ ì¶”ì¶œ ì‹¤í–‰
                if self.langextract_client.get('mode') == 'direct_ollama':
                    # ì§ì ‘ Ollama í˜¸ì¶œ ë°©ì‹
                    response = self._call_ollama_direct(prompt, langextract_config)
                else:
                    # ìƒˆë¡œìš´ LangExtract API ë°©ì‹
                    try:
                        langextract_lib = self.langextract_client['langextract']
                        model = self.langextract_client['model']
                        
                        # ì˜ˆì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                        examples = self._get_extraction_examples()
                        
                        # ìƒˆë¡œìš´ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì¶œ
                        result = langextract_lib.extract(
                            chunk,
                            prompt_description=self._create_extraction_prompt_description(),
                            examples=examples,
                            model=model,
                            max_char_buffer=langextract_config.get('max_tokens', 1000),
                            temperature=langextract_config.get('temperature', 0.3)
                        )
                        
                        # ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                        response = str(result) if result else None
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ ìƒˆ API ì‹¤íŒ¨, ì§ì ‘ í˜¸ì¶œë¡œ ëŒ€ì²´: {e}")
                        response = self._call_ollama_direct(prompt, langextract_config)
                
                if response:
                    chunk_keywords = self._parse_langextract_response(
                        response, original_text_copy, position_mapper, position_map, i
                    )
                    all_keywords.extend(chunk_keywords)
                    logger.info(f"  âœ… ì²­í¬ {i} ì²˜ë¦¬ ì™„ë£Œ - {len(chunk_keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ")
                else:
                    logger.warning(f"  âš ï¸ ì²­í¬ {i} ì²˜ë¦¬ ì‹¤íŒ¨ - ì‘ë‹µ ì—†ìŒ")
            
            # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ì •ê·œí™”
            final_keywords = self._deduplicate_and_normalize(all_keywords, langextract_config)
            
            # ë””ë²„ê·¸ ë¡œê¹…: ìµœì¢… ê²°ê³¼
            extraction_time = time.time() - start_time
            debug_logger.log_final_results(
                extractor_name="langextract",
                final_keywords=[{
                    "keyword": kw.text,
                    "score": kw.score,
                    "category": kw.category,
                    "start_position": kw.start_position,
                    "end_position": kw.end_position,
                    "page_number": kw.page_number,
                    "line_number": kw.line_number,
                    "context": kw.context_snippet
                } for kw in final_keywords],
                extraction_time=extraction_time,
                total_processing_time=time.time() - start_time
            )
            
            logger.info(f"âœ… LangExtract ì¶”ì¶œ ì™„ë£Œ - {len(final_keywords)}ê°œ í‚¤ì›Œë“œ, ì²˜ë¦¬ì‹œê°„: {extraction_time:.2f}ì´ˆ")
            return final_keywords
            
        except Exception as e:
            logger.error(f"âŒ LangExtract ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ì‹¤íŒ¨ ì‹œì—ë„ ë””ë²„ê·¸ ë¡œê¹…
            extraction_time = time.time() - start_time
            debug_logger.log_final_results(
                extractor_name="langextract",
                final_keywords=[],
                extraction_time=extraction_time,
                total_processing_time=time.time() - start_time
            )
            return []
        finally:
            # ë””ë²„ê·¸ ì„¸ì…˜ ì €ì¥
            debug_logger.save_debug_session()
    
    def _chunk_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunk_size = self.config.get('chunk_size', 2000)
        overlap = self.config.get('overlap', 200)
        
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
            if end < len(text):
                last_period = text.rfind('.', start, end)
                last_newline = text.rfind('\n', start, end)
                
                if last_period > start + chunk_size // 2:
                    end = last_period + 1
                elif last_newline > start + chunk_size // 2:
                    end = last_newline + 1
            
            chunks.append(text[start:end])
            
            if end >= len(text):
                break
                
            start = end - overlap
        
        return chunks
    
    def _create_extraction_prompt(self, text: str, config: Dict[str, Any]) -> str:
        """LangExtractìš© ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        max_keywords = config.get('max_entities', 15)
        
        return f"""You are an expert keyword extraction system. Extract exactly {max_keywords} important keywords and key phrases from the given text.

For each keyword, determine:
1. text: The actual keyword or phrase
2. category: Type (technology, person, concept, organization, location, etc.)
3. confidence: How confident you are (0.0-1.0)
4. importance: high/medium/low
5. semantic_type: grammatical/semantic classification

TEXT TO ANALYZE:
{text[:1500]}

Return ONLY a valid JSON object with this exact structure:
{{
  "keywords": [
    {{
      "text": "example keyword",
      "category": "technology", 
      "confidence": 0.9,
      "importance": "high",
      "semantic_type": "noun"
    }}
  ]
}}

JSON OUTPUT:"""
    
    def _call_ollama_direct(self, prompt: str, config: Dict[str, Any]) -> str:
        """ì§ì ‘ Ollama API í˜¸ì¶œ"""
        import requests
        logger = logging.getLogger(__name__)
        
        try:
            payload = {
                "model": self.ollama_config['model'],
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": config.get('temperature', 0.1),
                    "num_predict": config.get('max_tokens', 1500)
                }
            }
            
            logger.debug(f"ğŸš€ Ollama API ì§ì ‘ í˜¸ì¶œ - ëª¨ë¸: {self.ollama_config['model']}")
            
            response = requests.post(
                f"{self.ollama_config['base_url']}/api/generate",
                json=payload,
                timeout=self.ollama_config['timeout']
            )
            
            if response.status_code == 200:
                result = response.json().get("response", "")
                logger.debug(f"âœ… Ollama API í˜¸ì¶œ ì„±ê³µ - ì‘ë‹µ ê¸¸ì´: {len(result)} ë¬¸ì")
                return result.strip()
            else:
                logger.error(f"âŒ Ollama API ì˜¤ë¥˜: {response.status_code}")
                return ""
                
        except Exception as e:
            logger.error(f"âŒ Ollama API ì§ì ‘ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    def _parse_langextract_response(self, response: str, original_text: str, 
                                  position_mapper: PositionMapper, position_map: Dict[str, any], 
                                  chunk_id: int) -> List[Keyword]:
        """LangExtract ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ í‚¤ì›Œë“œ ëª©ë¡ ìƒì„±"""
        logger = logging.getLogger(__name__)
        debug_logger = get_debug_logger()
        
        try:
            # JSON ì¶”ì¶œ
            json_str = self._extract_json_from_response(response)
            logger.debug(f"ì²­í¬ {chunk_id} JSON ì¶”ì¶œ ì™„ë£Œ: {len(json_str)} ë¬¸ì")
            
            data = json.loads(json_str)
            keywords_data = data.get('keywords', [])
            
            # ë””ë²„ê·¸ ë¡œê¹…: í›„ë³´ ìƒì„±
            raw_candidates = [item.get('text', '') for item in keywords_data if item.get('text')]
            debug_logger.log_candidate_generation(
                extractor_name="langextract",
                candidates=raw_candidates,
                generation_method="langextract_schema_based",
                params={
                    "chunk_id": chunk_id,
                    "schema_validation": True,
                    "response_length": len(response),
                    "json_length": len(json_str),
                    "parsed_items": len(keywords_data)
                }
            )
            
            results = []
            positioned_keywords = 0
            abstract_keywords = 0
            
            for i, item in enumerate(keywords_data, 1):
                keyword_text = item.get('text', '').strip()
                confidence = float(item.get('confidence', 0.7))
                category = item.get('category', 'general')
                importance = item.get('importance', 'medium')
                semantic_type = item.get('semantic_type', 'unknown')
                
                # í‚¤ì›Œë“œ ìœ íš¨ì„± ê²€ì‚¬
                if not TextCleaner.is_meaningful_keyword(keyword_text):
                    logger.debug(f"â© ê±´ë„ˆëœ€ (ìœ íš¨í•˜ì§€ ì•Šì€ í‚¤ì›Œë“œ): '{keyword_text}'")
                    continue
                
                # í‚¤ì›Œë“œ ì •ê·œí™”
                normalized_keyword = TextCleaner.normalize_keyword(keyword_text)
                if not normalized_keyword:
                    logger.debug(f"â© ê±´ë„ˆëœ€ (ì •ê·œí™” ì‹¤íŒ¨): '{keyword_text}'")
                    continue
                
                # í…ìŠ¤íŠ¸ì—ì„œ ìœ„ì¹˜ ì°¾ê¸°
                positions = self._find_keyword_positions(original_text, normalized_keyword)
                
                # ì ìˆ˜ ê³„ì‚° (ì‹ ë¢°ë„ + ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜)
                importance_weight = {'high': 1.0, 'medium': 0.8, 'low': 0.6}.get(importance, 0.8)
                final_score = confidence * importance_weight
                
                if positions:
                    positioned_keywords += 1
                    for start_pos, end_pos in positions[:1]:  # ì²« ë²ˆì§¸ ìœ„ì¹˜ë§Œ ì‚¬ìš©
                        context = self._extract_context(original_text, start_pos, end_pos)
                        page_number, line_number, column_number = position_mapper.get_position_info(start_pos, position_map)
                        
                        results.append(Keyword(
                            text=normalized_keyword,
                            score=final_score,
                            extractor=self.name,
                            category=f"{category}_{semantic_type}",
                            start_position=start_pos,
                            end_position=end_pos,
                            context_snippet=context,
                            page_number=page_number,
                            line_number=line_number,
                            column_number=column_number
                        ))
                        
                        # ìƒìœ„ í‚¤ì›Œë“œ ë¡œê¹…
                        if len(results) <= 3:
                            logger.info(f"  ğŸ“ [{i}] '{keyword_text}' (ì‹ ë¢°ë„: {confidence:.3f}, ì¤‘ìš”ë„: {importance}) - ìœ„ì¹˜: {start_pos}-{end_pos}")
                else:
                    abstract_keywords += 1
                    results.append(Keyword(
                        text=normalized_keyword,
                        score=final_score,
                        extractor=self.name,
                        category=f"{category}_{semantic_type}",
                        start_position=None,
                        end_position=None,
                        context_snippet=original_text[:100] + "..." if len(original_text) > 100 else original_text,
                        page_number=None,
                        line_number=None,
                        column_number=None
                    ))
                    
                    if abstract_keywords <= 3:
                        logger.info(f"  ğŸ”® [{i}] '{keyword_text}' (ì‹ ë¢°ë„: {confidence:.3f}, ì¤‘ìš”ë„: {importance}) - ì¶”ìƒ í‚¤ì›Œë“œ")
            
            # ë””ë²„ê·¸ ë¡œê¹…: ìœ„ì¹˜ ë¶„ì„ ê²°ê³¼
            keywords_with_positions = []
            for kw in results:
                kw_data = {
                    "keyword": kw.text,
                    "score": kw.score,
                    "category": kw.category,
                    "positions": []
                }
                if kw.start_position is not None:
                    kw_data["positions"].append({
                        "start": kw.start_position,
                        "end": kw.end_position,
                        "page": kw.page_number,
                        "line": kw.line_number,
                        "context": kw.context_snippet
                    })
                keywords_with_positions.append(kw_data)
            
            debug_logger.log_position_analysis(
                extractor_name="langextract",
                keywords_with_positions=keywords_with_positions,
                text=original_text,
                analysis_method="schema_based_text_search"
            )
            
            logger.debug(f"ì²­í¬ {chunk_id} í‚¤ì›Œë“œ ì²˜ë¦¬ ì™„ë£Œ - ì´ {len(results)}ê°œ (ìœ„ì¹˜ìˆìŒ: {positioned_keywords}, ì¶”ìƒ: {abstract_keywords})")
            return results
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ ì²­í¬ {chunk_id} JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            logger.error(f"ì‘ë‹µ ë‚´ìš©: {response[:200]}...")
            return []
        except Exception as e:
            logger.error(f"âŒ ì²­í¬ {chunk_id} ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_json_from_response(self, response: str) -> str:
        """ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ"""
        import re
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        response = re.sub(r'```json\s*', '', response, flags=re.IGNORECASE)
        response = re.sub(r'```\s*', '', response)
        
        # JSON ê°ì²´ ì°¾ê¸° (ì¤‘ê´„í˜¸ë¡œ ì‹œì‘í•˜ê³  ëë‚˜ëŠ” ê²ƒ)
        json_pattern = r'\{[^{}]*"keywords"[^{}]*\[[^\]]*\][^{}]*\}'
        matches = re.findall(json_pattern, response, re.DOTALL)
        if matches:
            return matches[0]
        
        # ë” ê´€ëŒ€í•œ JSON íŒ¨í„´
        broad_pattern = r'\{.*?"keywords".*?\}' 
        matches = re.findall(broad_pattern, response, re.DOTALL)
        if matches:
            return matches[0]
        
        # JSON ë§ˆì»¤ ì´í›„ì˜ ë‚´ìš© ì°¾ê¸°
        json_markers = ['JSON OUTPUT:', 'OUTPUT:', '{']
        for marker in json_markers:
            if marker in response:
                json_part = response[response.find(marker):].strip()
                if marker != '{':
                    json_part = json_part[len(marker):].strip()
                
                # ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€ ì¶”ì¶œ
                start_idx = json_part.find('{')
                if start_idx != -1:
                    end_idx = json_part.rfind('}')
                    if end_idx != -1 and end_idx > start_idx:
                        return json_part[start_idx:end_idx + 1]
        
        # ì‘ë‹µ ì „ì²´ê°€ JSONì¼ ìˆ˜ë„ ìˆìŒ
        return response.strip()
    
    def _find_keyword_positions(self, text: str, keyword: str) -> List[tuple]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        positions = []
        start = 0
        text_lower = text.lower()
        keyword_lower = keyword.lower()
        
        while True:
            pos = text_lower.find(keyword_lower, start)
            if pos == -1:
                break
            positions.append((pos, pos + len(keyword)))
            start = pos + 1
        return positions
    
    def _extract_context(self, text: str, start_pos: int, end_pos: int, context_size: int = 50) -> str:
        """í‚¤ì›Œë“œ ì£¼ë³€ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        context_start = max(0, start_pos - context_size)
        context_end = min(len(text), end_pos + context_size)
        
        context = text[context_start:context_end]
        
        # ì•ë’¤ì— ìƒëµ í‘œì‹œ ì¶”ê°€
        if context_start > 0:
            context = "..." + context
        if context_end < len(text):
            context = context + "..."
            
        return context
    
    def _deduplicate_and_normalize(self, keywords: List[Keyword], config: Dict[str, Any]) -> List[Keyword]:
        """ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ì •ê·œí™”"""
        # í‚¤ì›Œë“œ í…ìŠ¤íŠ¸ë³„ë¡œ ê·¸ë£¹í™”
        keyword_groups = {}
        for kw in keywords:
            key = kw.text.lower()
            if key not in keyword_groups:
                keyword_groups[key] = []
            keyword_groups[key].append(kw)
        
        # ê° ê·¸ë£¹ì—ì„œ ìµœê³  ì ìˆ˜ ì„ íƒ ë° ì ìˆ˜ ë³‘í•©
        final_keywords = []
        for group in keyword_groups.values():
            if len(group) == 1:
                final_keywords.append(group[0])
            else:
                # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜, ì ìˆ˜ëŠ” í‰ê·  ì‚¬ìš©
                best_kw = max(group, key=lambda x: x.score)
                avg_score = sum(kw.score for kw in group) / len(group)
                best_kw.score = avg_score
                final_keywords.append(best_kw)
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        final_keywords.sort(key=lambda x: x.score, reverse=True)
        
        # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        max_keywords = config.get('max_entities', 15)
        return final_keywords[:max_keywords]
    
    def _create_extraction_prompt_description(self) -> str:
        """ìƒˆë¡œìš´ LangExtract APIìš© ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ì„¤ëª…"""
        max_keywords = self.config.get('max_keywords', 15)
        return f"Extract {max_keywords} important keywords and key phrases from the text, including their categories and confidence scores in JSON format."
    
    def _get_extraction_examples(self):
        """LangExtract APIìš© ì˜ˆì œ ë°ì´í„° ìƒì„±"""
        from langextract.data import ExampleData
        
        examples = [
            ExampleData(
                text="Apple Inc. is a technology company based in Cupertino, California. The company develops smartphones and computers.",
                extractions={
                    "keywords": [
                        {"text": "Apple Inc.", "category": "organization", "confidence": 0.95},
                        {"text": "technology", "category": "concept", "confidence": 0.85},
                        {"text": "Cupertino", "category": "location", "confidence": 0.90},
                        {"text": "California", "category": "location", "confidence": 0.90},
                        {"text": "smartphones", "category": "technology", "confidence": 0.80},
                        {"text": "computers", "category": "technology", "confidence": 0.80}
                    ]
                }
            ),
            ExampleData(
                text="í•œêµ­ì€í–‰ì€ 2024ë…„ ê¸ˆë¦¬ë¥¼ ì¸ìƒí–ˆë‹¤. ê²½ì œ ì „ë¬¸ê°€ë“¤ì€ ì¸í”Œë ˆì´ì…˜ ì–µì œ íš¨ê³¼ë¥¼ ê¸°ëŒ€í•œë‹¤ê³  ë§í–ˆë‹¤.",
                extractions={
                    "keywords": [
                        {"text": "í•œêµ­ì€í–‰", "category": "organization", "confidence": 0.95},
                        {"text": "2024ë…„", "category": "time", "confidence": 0.90},
                        {"text": "ê¸ˆë¦¬", "category": "concept", "confidence": 0.85},
                        {"text": "ê²½ì œ ì „ë¬¸ê°€", "category": "person", "confidence": 0.80},
                        {"text": "ì¸í”Œë ˆì´ì…˜", "category": "concept", "confidence": 0.85}
                    ]
                }
            ),
            ExampleData(
                text="The research paper discusses machine learning algorithms for natural language processing. Deep learning models show promising results.",
                extractions={
                    "keywords": [
                        {"text": "machine learning", "category": "technology", "confidence": 0.90},
                        {"text": "algorithms", "category": "concept", "confidence": 0.80},
                        {"text": "natural language processing", "category": "technology", "confidence": 0.95},
                        {"text": "deep learning", "category": "technology", "confidence": 0.90},
                        {"text": "models", "category": "concept", "confidence": 0.75}
                    ]
                }
            )
        ]
        
        return examples