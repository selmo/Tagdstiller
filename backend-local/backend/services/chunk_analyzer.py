"""
ì²­í¬ë³„ ë¶„ì„ ë° ê²°ê³¼ í†µí•© ì„œë¹„ìŠ¤

êµ¬ì¡° ê¸°ë°˜ìœ¼ë¡œ ë¶„í• ëœ ë¬¸ì„œ ì²­í¬ë¥¼ ê°œë³„ ë¶„ì„í•˜ê³ ,
ë¶„ì„ ê²°ê³¼ë¥¼ ê³„ì¸µì ìœ¼ë¡œ ë³‘í•©í•˜ì—¬ ì „ì²´ ë¬¸ì„œ ìˆ˜ì¤€ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

from sqlalchemy.orm import Session

from services.document_chunker import DocumentChunker, DocumentNode, ChunkInfo
from services.local_file_analyzer import LocalFileAnalyzer
from services.document_parser_service import DocumentParserService
from services.chunk_prompt_manager import ChunkPromptManager, ChunkPromptRequest, ChunkPromptResult
from prompts.templates import (
    DocumentSummaryPrompts,
    KeywordExtractionPrompts,
    DocumentStructurePrompts,
    KnowledgeGraphPrompts,
    get_prompt_template
)


@dataclass
class ChunkAnalysisResult:
    """ì²­í¬ ë¶„ì„ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    chunk_id: str
    level: str  # "document", "chapter", "section", "subsection"
    content_length: int

    # í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼
    keywords: List[Dict[str, Any]]

    # ë¬¸ì„œ ìš”ì•½ ê²°ê³¼
    summary: Dict[str, Any]

    # êµ¬ì¡° ë¶„ì„ ê²°ê³¼ (LLM ê¸°ë°˜)
    structure_analysis: Optional[Dict[str, Any]]

    # ì§€ì‹ ê·¸ë˜í”„ ë°ì´í„°
    knowledge_graph: Optional[Dict[str, Any]]

    # ë©”íƒ€ë°ì´í„°
    analysis_metadata: Dict[str, Any]


@dataclass
class IntegratedAnalysisResult:
    """í†µí•© ë¶„ì„ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    total_chunks: int
    total_content_length: int

    # ê³„ì¸µì  í‚¤ì›Œë“œ (ì²­í¬ë³„ í‚¤ì›Œë“œë¥¼ ì¤‘ìš”ë„ ê¸°ì¤€ìœ¼ë¡œ í†µí•©)
    integrated_keywords: List[Dict[str, Any]]

    # ê³„ì¸µì  ìš”ì•½ (ì²­í¬ë³„ ìš”ì•½ì„ í†µí•©í•˜ì—¬ ì „ì²´ ìš”ì•½ ìƒì„±)
    hierarchical_summary: Dict[str, Any]

    # í†µí•© êµ¬ì¡° ë¶„ì„
    integrated_structure: Dict[str, Any]

    # í†µí•© ì§€ì‹ ê·¸ë˜í”„
    merged_knowledge_graph: Optional[Dict[str, Any]]

    # ì²­í¬ë³„ ìƒì„¸ ê²°ê³¼
    chunk_results: List[ChunkAnalysisResult]

    # í†µí•© ë©”íƒ€ë°ì´í„°
    integration_metadata: Dict[str, Any]


class ChunkAnalyzer:
    """ì²­í¬ë³„ ë¶„ì„ ë° ê²°ê³¼ í†µí•©ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤"""

    def __init__(self, db: Session):
        self.db = db
        self.logger = logging.getLogger(__name__)
        self.document_chunker = DocumentChunker()
        self.local_analyzer = LocalFileAnalyzer(db)
        self.prompt_manager = None  # ë‚˜ì¤‘ì— ì´ˆê¸°í™”

    def analyze_document_with_chunking(
        self,
        file_path: str,
        output_directory: str = None,
        max_chunk_size: int = 50000,
        use_llm: bool = True,
        extractors: List[str] = None,
        analysis_types: List[str] = None
    ) -> IntegratedAnalysisResult:
        """
        ë¬¸ì„œë¥¼ ì²­í‚¹í•˜ì—¬ ë¶„ì„í•˜ê³  ê²°ê³¼ë¥¼ í†µí•©í•©ë‹ˆë‹¤.

        Args:
            file_path: ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ
            output_directory: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ì„ íƒ)
            max_chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸° (ê¸°ë³¸ 50,000ì)
            use_llm: LLM ë¶„ì„ ì‚¬ìš© ì—¬ë¶€
            extractors: ì‚¬ìš©í•  í‚¤ì›Œë“œ ì¶”ì¶œê¸° ë¦¬ìŠ¤íŠ¸
            analysis_types: ìˆ˜í–‰í•  ë¶„ì„ ìœ í˜• ë¦¬ìŠ¤íŠ¸

        Returns:
            IntegratedAnalysisResult: í†µí•© ë¶„ì„ ê²°ê³¼
        """
        self.logger.info(f"ğŸ” ì²­í¬ ê¸°ë°˜ ë¬¸ì„œ ë¶„ì„ ì‹œì‘: {file_path}")

        # ê¸°ë³¸ê°’ ì„¤ì •
        extractors = extractors or ["KeyBERT", "spaCy NER", "LLM"]
        analysis_types = analysis_types or ["keywords", "summary", "structure", "knowledge_graph"]

        start_time = datetime.now()

        try:
            # íŒŒì¼ ê²½ë¡œ ë° ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
            file_path_obj = Path(file_path)
            final_output_dir = output_directory or str(file_path_obj.parent)

            # í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
            self.prompt_manager = ChunkPromptManager(final_output_dir)
            self.logger.info(f"ğŸ¯ í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ: {final_output_dir}")

            # 1ë‹¨ê³„: íŒŒì¼ íŒŒì‹± (PDF ë“± ë°”ì´ë„ˆë¦¬ íŒŒì¼ ì²˜ë¦¬)
            self.logger.info("ğŸ“„ 1ë‹¨ê³„: íŒŒì¼ íŒŒì‹±")

            # PDF íŒŒì¼ì´ë‚˜ ê¸°íƒ€ ë°”ì´ë„ˆë¦¬ íŒŒì¼ì˜ ê²½ìš° ë¨¼ì € íŒŒì‹±
            if file_path_obj.suffix.lower() in ['.pdf', '.docx', '.doc', '.hwp']:
                self.logger.info(f"ğŸ“‹ ë°”ì´ë„ˆë¦¬ íŒŒì¼ íŒŒì‹± ì‹œì‘: {file_path_obj.suffix}")
                parser_service = DocumentParserService()

                # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
                output_dir_path = Path(final_output_dir)

                # ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
                if parser_service.has_parsing_results(file_path_obj, output_dir_path):
                    self.logger.info("â™»ï¸ ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ ì‚¬ìš©")
                    parsing_results = parser_service.load_existing_parsing_results(file_path_obj, output_dir_path)
                else:
                    self.logger.info("ğŸ”„ ìƒˆë¡œìš´ íŒŒì‹± ìˆ˜í–‰")
                    parsing_results = parser_service.parse_document_comprehensive(
                        file_path=file_path_obj,
                        force_reparse=False,
                        directory=output_dir_path
                    )

                # ìµœê³  í’ˆì§ˆ íŒŒì„œì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                best_parser = parsing_results.get("summary", {}).get("best_parser")
                if not best_parser:
                    raise ValueError("íŒŒì‹± ê²°ê³¼ì—ì„œ ìµœì  íŒŒì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

                # í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°
                parsing_output_dir = parser_service.get_output_directory(file_path_obj, output_dir_path)
                text_file = parsing_output_dir / best_parser / f"{best_parser}_text.txt"

                if not text_file.exists():
                    raise FileNotFoundError(f"íŒŒì‹±ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {text_file}")

                document_text = text_file.read_text(encoding='utf-8')
                self.logger.info(f"âœ… íŒŒì‹± ì™„ë£Œ: {len(document_text):,}ì ì¶”ì¶œë¨")

                # í…ìŠ¤íŠ¸ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ì²­í‚¹ì— ì‚¬ìš© (ì§€ì •ëœ ì¶œë ¥ ë””ë ‰í† ë¦¬ì—)
                temp_text_file = output_dir_path / "parsed_document.txt"
                output_dir_path.mkdir(parents=True, exist_ok=True)
                temp_text_file.write_text(document_text, encoding='utf-8')
                text_file_for_chunking = str(temp_text_file)
            else:
                # í…ìŠ¤íŠ¸ íŒŒì¼ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                text_file_for_chunking = file_path

            # 2ë‹¨ê³„: ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ë° ì²­í‚¹
            self.logger.info("ğŸ” 2ë‹¨ê³„: ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ë° ì²­í‚¹")

            chunk_info = self.document_chunker.chunk_document(
                file_path=text_file_for_chunking,
                max_chunk_size=max_chunk_size,
                output_directory=final_output_dir
            )

            if not chunk_info.chunks:
                raise ValueError("ë¬¸ì„œ ì²­í‚¹ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

            self.logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunk_info.chunks)}ê°œ ì²­í¬ ìƒì„±")

            # ì²­í¬ í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ë””ë²„ê·¸)
            chunks_text_dir = Path(final_output_dir) / "chunks_text"
            if chunks_text_dir.exists():
                txt_files = list(chunks_text_dir.glob("*.txt"))
                self.logger.info(f"ğŸ“ ì²­í¬ í…ìŠ¤íŠ¸ íŒŒì¼ {len(txt_files)}ê°œ ë°œê²¬: {[f.name for f in txt_files]}")
            else:
                self.logger.warning(f"âš ï¸ ì²­í¬ í…ìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {chunks_text_dir}")

            # 3ë‹¨ê³„: ê° ì²­í¬ë³„ ë¶„ì„ ìˆ˜í–‰
            self.logger.info("ğŸ”¬ 3ë‹¨ê³„: ì²­í¬ë³„ ë¶„ì„ ìˆ˜í–‰")
            chunk_results = []

            for i, chunk in enumerate(chunk_info.chunks, 1):
                self.logger.info(f"ğŸ“‹ ì²­í¬ {i}/{len(chunk_info.chunks)} ë¶„ì„ ì¤‘: {chunk['chunk_id']}")

                # ì²­í¬ ë°ì´í„° êµ¬ì¡° ë””ë²„ê·¸ ë¡œê·¸
                self.logger.info(f"ğŸ” ì²­í¬ ë°ì´í„° í‚¤ë“¤: {list(chunk.keys())}")
                content_preview_len = len(chunk.get('content_preview', ''))
                self.logger.info(f"ğŸ“„ content_preview ê¸¸ì´: {content_preview_len}")

                # ì²­í¬ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì €ì¥
                if self.prompt_manager:
                    try:
                        # ì²­í¬ í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì „ì²´ ì½˜í…ì¸  ì½ê¸°
                        chunk_text_file = Path(final_output_dir) / "chunks_text" / f"{chunk['chunk_id']}.txt"
                        if chunk_text_file.exists():
                            full_content = chunk_text_file.read_text(encoding='utf-8')
                            # í—¤ë” ë¶€ë¶„ ì œê±° (ì‹¤ì œ ì½˜í…ì¸ ë§Œ ì¶”ì¶œ)
                            content_lines = full_content.split('\n')
                            content_start_idx = 0
                            for i, line in enumerate(content_lines):
                                if line.startswith('---') or line.startswith('Content:'):
                                    content_start_idx = i + 1
                                    break
                            actual_content = '\n'.join(content_lines[content_start_idx:]).strip()

                            # ì²­í¬ ì •ë³´ì— ì „ì²´ ì½˜í…ì¸  ì¶”ê°€
                            chunk_with_content = chunk.copy()
                            chunk_with_content['full_content'] = actual_content

                            self.logger.info(f"ğŸ“„ {chunk['chunk_id']} ì „ì²´ ì½˜í…ì¸  ë¡œë“œ: {len(actual_content)}ì")
                            self._generate_chunk_prompts(chunk_with_content, file_path, analysis_types)
                        else:
                            self.logger.warning(f"âš ï¸ {chunk['chunk_id']} í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ì–´ì„œ ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©")
                            self._generate_chunk_prompts(chunk, file_path, analysis_types)

                        self.logger.info(f"âœ… {chunk['chunk_id']} í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ")
                    except Exception as e:
                        self.logger.error(f"âŒ {chunk['chunk_id']} í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
                        import traceback
                        self.logger.error(f"í”„ë¡¬í”„íŠ¸ ìƒì„± ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")

                chunk_result = self._analyze_chunk(
                    chunk=chunk,
                    file_path=file_path,
                    extractors=extractors,
                    analysis_types=analysis_types,
                    use_llm=use_llm
                )
                chunk_results.append(chunk_result)

                # ì²­í¬ë³„ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
                if self.prompt_manager:
                    self._generate_chunk_analysis_report(chunk_result)

                self.logger.info(f"âœ… ì²­í¬ {i} ë¶„ì„ ì™„ë£Œ")

            # 3ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ í†µí•©
            self.logger.info("ğŸ”— 3ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ í†µí•©")
            integrated_result = self._integrate_analysis_results(
                chunk_results=chunk_results,
                original_document_info=chunk_info,
                total_content_length=sum(chunk['content_length'] for chunk in chunk_info.chunks)
            )

            # 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥
            if output_directory:
                self._save_integrated_results(
                    integrated_result=integrated_result,
                    output_directory=output_directory,
                    file_path=file_path
                )

            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()

            self.logger.info(f"ğŸ‰ ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ)")

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            integrated_result.integration_metadata.update({
                "processing_time_seconds": processing_time,
                "analysis_completion_time": end_time.isoformat(),
                "chunks_analyzed": len(chunk_results),
                "total_content_length": integrated_result.total_content_length
            })

            return integrated_result

        except Exception as e:
            self.logger.error(f"âŒ ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise

    def _analyze_chunk(
        self,
        chunk: Dict[str, Any],
        file_path: str,
        extractors: List[str],
        analysis_types: List[str],
        use_llm: bool
    ) -> ChunkAnalysisResult:
        """ê°œë³„ ì²­í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""

        chunk_content = chunk.get('content_preview', '')
        chunk_id = chunk['chunk_id']
        level = chunk['level']
        content_length = chunk['content_length']

        analysis_results = {
            'keywords': [],
            'summary': {},
            'structure_analysis': None,
            'knowledge_graph': None
        }

        # í‚¤ì›Œë“œ ì¶”ì¶œ ë¶„ì„
        if "keywords" in analysis_types:
            self.logger.debug(f"ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ: {chunk_id}")
            analysis_results['keywords'] = self._extract_keywords_from_chunk(
                content=chunk_content,
                extractors=extractors,
                use_llm=use_llm
            )

        # ë¬¸ì„œ ìš”ì•½ ë¶„ì„
        if "summary" in analysis_types:
            self.logger.debug(f"ğŸ“ ìš”ì•½ ìƒì„±: {chunk_id}")
            analysis_results['summary'] = self._summarize_chunk(
                content=chunk_content,
                chunk_level=level,
                use_llm=use_llm
            )

        # êµ¬ì¡° ë¶„ì„ (ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´)
        if "structure" in analysis_types:
            self.logger.debug(f"ğŸ—ï¸ êµ¬ì¡° ë¶„ì„: {chunk_id}")
            analysis_results['structure_analysis'] = self._analyze_chunk_structure_basic(
                content=chunk_content,
                chunk_info=chunk
            )

        # ì§€ì‹ ê·¸ë˜í”„ ìƒì„± (ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´)
        if "knowledge_graph" in analysis_types:
            self.logger.debug(f"ğŸ•¸ï¸ ì§€ì‹ ê·¸ë˜í”„ ìƒì„±: {chunk_id}")
            analysis_results['knowledge_graph'] = self._extract_knowledge_graph_basic(
                content=chunk_content,
                chunk_info=chunk
            )

        return ChunkAnalysisResult(
            chunk_id=chunk_id,
            level=level,
            content_length=content_length,
            keywords=analysis_results['keywords'],
            summary=analysis_results['summary'],
            structure_analysis=analysis_results['structure_analysis'],
            knowledge_graph=analysis_results['knowledge_graph'],
            analysis_metadata={
                'file_path': file_path,
                'chunk_info': chunk,
                'analysis_timestamp': datetime.now().isoformat(),
                'extractors_used': extractors,
                'analysis_types': analysis_types
            }
        )

    def _extract_keywords_from_chunk(
        self,
        content: str,
        extractors: List[str],
        use_llm: bool
    ) -> List[Dict[str, Any]]:
        """ì²­í¬ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""

        keywords = []

        # LLM ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œë¡œ ëŒ€ì²´)
        if "LLM" in extractors and use_llm:
            try:
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œ LLM ëŒ€ì‹  ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
                simple_keywords = self._extract_simple_keywords(content)
                for i, keyword in enumerate(simple_keywords):
                    keywords.append({
                        'keyword': keyword,
                        'score': 0.8 - (i * 0.1),  # ìˆœì„œë³„ë¡œ ì ìˆ˜ ê°ì†Œ
                        'category': 'noun',
                        'extractor': 'LLM_mock',
                        'chunk_source': True
                    })

            except Exception as e:
                self.logger.warning(f"âš ï¸ LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

        # ë‹¤ë¥¸ ì¶”ì¶œê¸°ë“¤ì€ í–¥í›„ í™•ì¥ ê°€ëŠ¥
        # KeyBERT, spaCy NER ë“±ì˜ êµ¬í˜„ì€ ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ì—°ë™

        return keywords

    def _extract_simple_keywords(self, content: str) -> List[str]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (í…ŒìŠ¤íŠ¸ìš©)"""
        import re

        # ê°„ë‹¨í•œ ì •ê·œì‹ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        words = re.findall(r'\b[ê°€-í£]{2,}|\b[a-zA-Z]{3,}\b', content)

        # ë¹ˆë„ ê³„ì‚°
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1

        # ë¹ˆë„ìˆœ ì •ë ¬ í›„ ìƒìœ„ 10ê°œ ë°˜í™˜
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:10]]

    def _summarize_chunk(
        self,
        content: str,
        chunk_level: str,
        use_llm: bool
    ) -> Dict[str, Any]:
        """ì²­í¬ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤."""

        # ê¸°ë³¸ ìš”ì•½ ìƒì„±
        lines = content.split('\n')
        non_empty_lines = [line.strip() for line in lines if line.strip()]

        # ê°„ë‹¨í•œ ìš”ì•½ ì •ë³´ ì¶”ì¶œ
        intro = non_empty_lines[0] if non_empty_lines else "ë‚´ìš© ì—†ìŒ"
        conclusion = non_empty_lines[-1] if len(non_empty_lines) > 1 else intro

        # ê°„ë‹¨í•œ ì£¼ì œ ì¶”ì¶œ (í‚¤ì›Œë“œ ê¸°ë°˜)
        keywords = self._extract_simple_keywords(content)
        topics = keywords[:5]  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë¥¼ ì£¼ì œë¡œ ì‚¬ìš©

        return {
            'summary_type': 'basic_generated',
            'level': chunk_level,
            'intro': intro[:100] + "..." if len(intro) > 100 else intro,
            'conclusion': conclusion[:100] + "..." if len(conclusion) > 100 else conclusion,
            'core': f"{chunk_level} ìˆ˜ì¤€ì˜ ë‚´ìš©ìœ¼ë¡œ {len(content)}ìì˜ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.",
            'topics': topics,
            'tone': 'informational',
            'content_length': len(content),
            'line_count': len(non_empty_lines)
        }

    def _analyze_chunk_structure_basic(
        self,
        content: str,
        chunk_info: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """ì²­í¬ì˜ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""

        try:
            lines = content.split('\n')
            headers = []
            lists = []

            for line in lines:
                line = line.strip()
                if line.startswith('#'):
                    headers.append(line)
                elif line.startswith('-') or line.startswith('*') or line.startswith('â€¢'):
                    lists.append(line)

            return {
                'analysis_method': 'basic_structure',
                'total_lines': len(lines),
                'headers_count': len(headers),
                'lists_count': len(lists),
                'headers': headers[:5],  # ìƒìœ„ 5ê°œ í—¤ë”
                'structure_features': {
                    'has_titles': len(headers) > 0,
                    'has_lists': len(lists) > 0,
                    'has_paragraphs': len([l for l in lines if len(l.strip()) > 50]) > 0
                },
                'chunk_info': chunk_info
            }

        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²­í¬ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

        return None

    def _extract_knowledge_graph_basic(
        self,
        content: str,
        chunk_info: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """ì²­í¬ì—ì„œ ê¸°ë³¸ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""

        try:
            # ê°„ë‹¨í•œ ì—”í‹°í‹° ì¶”ì¶œ (í‚¤ì›Œë“œ ê¸°ë°˜)
            keywords = self._extract_simple_keywords(content)

            entities = []
            relationships = []

            # í‚¤ì›Œë“œë¥¼ ì—”í‹°í‹°ë¡œ ë³€í™˜
            for i, keyword in enumerate(keywords[:5]):  # ìƒìœ„ 5ê°œë§Œ
                entities.append({
                    'id': f'entity_{i}',
                    'type': 'Concept',
                    'properties': {
                        'name': keyword,
                        'source_chunk': chunk_info.get('chunk_id', 'unknown')
                    }
                })

            # ê°„ë‹¨í•œ ê´€ê³„ ìƒì„± (ì¸ì ‘í•œ ì—”í‹°í‹°ë“¤ ê°„ì˜ ê´€ê³„)
            for i in range(len(entities) - 1):
                relationships.append({
                    'source': entities[i]['id'],
                    'target': entities[i + 1]['id'],
                    'type': 'RELATED_TO',
                    'properties': {
                        'relationship_name': 'CO_OCCURS',
                        'context': 'same_chunk'
                    }
                })

            return {
                'extraction_method': 'basic_kg',
                'entities': entities,
                'relationships': relationships,
                'source_chunk': chunk_info.get('chunk_id', 'unknown'),
                'entity_count': len(entities),
                'relationship_count': len(relationships)
            }

        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²­í¬ ì§€ì‹ ê·¸ë˜í”„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

        return None

    def _integrate_analysis_results(
        self,
        chunk_results: List[ChunkAnalysisResult],
        original_document_info: Any,
        total_content_length: int
    ) -> IntegratedAnalysisResult:
        """ì²­í¬ë³„ ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•©í•©ë‹ˆë‹¤."""

        self.logger.info("ğŸ”— ë¶„ì„ ê²°ê³¼ í†µí•© ì‹œì‘")

        # í‚¤ì›Œë“œ í†µí•© (ì¤‘ë³µ ì œê±° ë° ì¤‘ìš”ë„ ê¸°ì¤€ ì •ë ¬)
        integrated_keywords = self._integrate_keywords(chunk_results)

        # ê³„ì¸µì  ìš”ì•½ ìƒì„±
        hierarchical_summary = self._create_hierarchical_summary(chunk_results)

        # êµ¬ì¡° ì •ë³´ í†µí•©
        integrated_structure = self._integrate_structure_info(chunk_results, original_document_info)

        # ì§€ì‹ ê·¸ë˜í”„ ë³‘í•©
        merged_kg = self._merge_knowledge_graphs(chunk_results)

        # í†µí•© ë©”íƒ€ë°ì´í„° ìƒì„±
        integration_metadata = {
            'integration_timestamp': datetime.now().isoformat(),
            'total_chunks_processed': len(chunk_results),
            'integration_method': 'hierarchical_merge',
            'content_distribution': {
                result.level: result.content_length
                for result in chunk_results
            }
        }

        return IntegratedAnalysisResult(
            total_chunks=len(chunk_results),
            total_content_length=total_content_length,
            integrated_keywords=integrated_keywords,
            hierarchical_summary=hierarchical_summary,
            integrated_structure=integrated_structure,
            merged_knowledge_graph=merged_kg,
            chunk_results=chunk_results,
            integration_metadata=integration_metadata
        )

    def _integrate_keywords(self, chunk_results: List[ChunkAnalysisResult]) -> List[Dict[str, Any]]:
        """ì²­í¬ë³„ í‚¤ì›Œë“œë¥¼ í†µí•©í•˜ê³  ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤."""

        keyword_map = {}

        for chunk_result in chunk_results:
            for keyword_data in chunk_result.keywords:
                keyword_text = keyword_data.get('keyword', '').lower()
                if not keyword_text:
                    continue

                if keyword_text in keyword_map:
                    # ê¸°ì¡´ í‚¤ì›Œë“œì˜ ì ìˆ˜ì™€ ë¹„êµí•˜ì—¬ ë” ë†’ì€ ì ìˆ˜ ìœ ì§€
                    existing_score = keyword_map[keyword_text].get('score', 0)
                    new_score = keyword_data.get('score', 0)

                    if new_score > existing_score:
                        keyword_map[keyword_text] = {
                            **keyword_data,
                            'sources': keyword_map[keyword_text].get('sources', []) + [chunk_result.chunk_id],
                            'frequency': keyword_map[keyword_text].get('frequency', 1) + 1
                        }
                    else:
                        keyword_map[keyword_text]['sources'].append(chunk_result.chunk_id)
                        keyword_map[keyword_text]['frequency'] += 1
                else:
                    keyword_map[keyword_text] = {
                        **keyword_data,
                        'sources': [chunk_result.chunk_id],
                        'frequency': 1
                    }

        # ì ìˆ˜ì™€ ë¹ˆë„ë¥¼ ê³ ë ¤í•˜ì—¬ ì •ë ¬
        integrated_keywords = sorted(
            keyword_map.values(),
            key=lambda x: (x.get('score', 0) * x.get('frequency', 1)),
            reverse=True
        )

        return integrated_keywords[:50]  # ìƒìœ„ 50ê°œ í‚¤ì›Œë“œë§Œ ë°˜í™˜

    def _create_hierarchical_summary(self, chunk_results: List[ChunkAnalysisResult]) -> Dict[str, Any]:
        """ì²­í¬ë³„ ìš”ì•½ì„ ê³„ì¸µì ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤."""

        # ë ˆë²¨ë³„ë¡œ ìš”ì•½ ê·¸ë£¹í™”
        summaries_by_level = {}
        for result in chunk_results:
            level = result.level
            if level not in summaries_by_level:
                summaries_by_level[level] = []
            summaries_by_level[level].append(result.summary)

        # ê³„ì¸µì  êµ¬ì¡° ìƒì„±
        hierarchical_summary = {
            'document_level': {
                'total_chunks': len(chunk_results),
                'content_overview': "ì „ì²´ ë¬¸ì„œë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤."
            }
        }

        # ê° ë ˆë²¨ë³„ ìš”ì•½ í†µí•©
        for level, summaries in summaries_by_level.items():
            level_summary = {
                'count': len(summaries),
                'summaries': summaries,
                'combined_topics': []
            }

            # ê³µí†µ ì£¼ì œ ì¶”ì¶œ
            all_topics = []
            for summary in summaries:
                if isinstance(summary, dict) and 'topics' in summary:
                    all_topics.extend(summary['topics'])

            # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ ê³„ì‚°
            topic_frequency = {}
            for topic in all_topics:
                if isinstance(topic, str):
                    topic_frequency[topic] = topic_frequency.get(topic, 0) + 1

            # ë¹ˆë„ìˆœ ì •ë ¬
            level_summary['combined_topics'] = sorted(
                topic_frequency.items(),
                key=lambda x: x[1],
                reverse=True
            )[:10]  # ìƒìœ„ 10ê°œ ì£¼ì œ

            hierarchical_summary[f'{level}_level'] = level_summary

        return hierarchical_summary

    def _integrate_structure_info(
        self,
        chunk_results: List[ChunkAnalysisResult],
        original_document_info: Any
    ) -> Dict[str, Any]:
        """êµ¬ì¡° ì •ë³´ë¥¼ í†µí•©í•©ë‹ˆë‹¤."""

        integrated_structure = {
            'document_structure': {
                'total_chunks': len(chunk_results),
                'chunking_info': getattr(original_document_info, '__dict__', {}),
                'chunk_hierarchy': []
            },
            'detailed_structures': {}
        }

        # ì²­í¬ë³„ ìƒì„¸ êµ¬ì¡° ì •ë³´ ìˆ˜ì§‘
        for result in chunk_results:
            if result.structure_analysis:
                integrated_structure['detailed_structures'][result.chunk_id] = {
                    'level': result.level,
                    'content_length': result.content_length,
                    'structure_data': result.structure_analysis
                }

                # ê³„ì¸µ ì •ë³´ ì¶”ê°€
                integrated_structure['document_structure']['chunk_hierarchy'].append({
                    'chunk_id': result.chunk_id,
                    'level': result.level,
                    'content_length': result.content_length,
                    'has_structure_analysis': True
                })
            else:
                integrated_structure['document_structure']['chunk_hierarchy'].append({
                    'chunk_id': result.chunk_id,
                    'level': result.level,
                    'content_length': result.content_length,
                    'has_structure_analysis': False
                })

        return integrated_structure

    def _merge_knowledge_graphs(self, chunk_results: List[ChunkAnalysisResult]) -> Optional[Dict[str, Any]]:
        """ì²­í¬ë³„ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤."""

        merged_entities = {}
        merged_relationships = []

        for result in chunk_results:
            if not result.knowledge_graph:
                continue

            kg = result.knowledge_graph

            # ì—”í‹°í‹° ë³‘í•©
            if 'entities' in kg:
                for entity in kg['entities']:
                    entity_id = entity.get('id')
                    if entity_id:
                        if entity_id in merged_entities:
                            # ê¸°ì¡´ ì—”í‹°í‹°ì™€ ì†ì„± ë³‘í•©
                            existing_props = merged_entities[entity_id].get('properties', {})
                            new_props = entity.get('properties', {})
                            merged_props = {**existing_props, **new_props}
                            merged_entities[entity_id]['properties'] = merged_props
                        else:
                            merged_entities[entity_id] = entity

            # ê´€ê³„ ë³‘í•©
            if 'relationships' in kg:
                for rel in kg['relationships']:
                    # ì¤‘ë³µ ê´€ê³„ ì²´í¬
                    rel_signature = f"{rel.get('source')}->{rel.get('target')}:{rel.get('type')}"
                    if not any(
                        f"{r.get('source')}->{r.get('target')}:{r.get('type')}" == rel_signature
                        for r in merged_relationships
                    ):
                        merged_relationships.append(rel)

        if not merged_entities and not merged_relationships:
            return None

        return {
            'entities': list(merged_entities.values()),
            'relationships': merged_relationships,
            'merge_metadata': {
                'total_entities': len(merged_entities),
                'total_relationships': len(merged_relationships),
                'source_chunks': len([r for r in chunk_results if r.knowledge_graph])
            }
        }

    def _save_integrated_results(
        self,
        integrated_result: IntegratedAnalysisResult,
        output_directory: str,
        file_path: str
    ):
        """í†µí•© ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""

        output_path = Path(output_directory)
        output_path.mkdir(parents=True, exist_ok=True)

        # í†µí•© ê²°ê³¼ ì €ì¥
        integrated_file = output_path / "integrated_analysis_result.json"
        with open(integrated_file, 'w', encoding='utf-8') as f:
            # dataclassë¥¼ dictë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
            result_dict = {
                'total_chunks': integrated_result.total_chunks,
                'total_content_length': integrated_result.total_content_length,
                'integrated_keywords': integrated_result.integrated_keywords,
                'hierarchical_summary': integrated_result.hierarchical_summary,
                'integrated_structure': integrated_result.integrated_structure,
                'merged_knowledge_graph': integrated_result.merged_knowledge_graph,
                'integration_metadata': integrated_result.integration_metadata,
                'chunk_results_count': len(integrated_result.chunk_results)
            }
            json.dump(result_dict, f, ensure_ascii=False, indent=2)

        # ì²­í¬ë³„ ìƒì„¸ ê²°ê³¼ ì €ì¥
        chunks_detail_file = output_path / "chunks_detailed_results.json"
        chunks_data = []
        for chunk_result in integrated_result.chunk_results:
            chunk_data = {
                'chunk_id': chunk_result.chunk_id,
                'level': chunk_result.level,
                'content_length': chunk_result.content_length,
                'keywords': chunk_result.keywords,
                'summary': chunk_result.summary,
                'structure_analysis': chunk_result.structure_analysis,
                'knowledge_graph': chunk_result.knowledge_graph,
                'analysis_metadata': chunk_result.analysis_metadata
            }
            chunks_data.append(chunk_data)

        with open(chunks_detail_file, 'w', encoding='utf-8') as f:
            json.dump(chunks_data, f, ensure_ascii=False, indent=2)

        self.logger.info(f"ğŸ’¾ í†µí•© ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_directory}")

    def _generate_chunk_prompts(self, chunk: Dict[str, Any], source_file: str, analysis_types: List[str]):
        """ì²­í¬ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì €ì¥"""
        try:
            # ë¶„ì„ íƒ€ì…ì„ í”„ë¡¬í”„íŠ¸ ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘
            prompt_categories = []
            for analysis_type in analysis_types:
                if analysis_type == "keywords":
                    prompt_categories.append("keyword_extraction")
                elif analysis_type == "summary":
                    prompt_categories.append("document_summary")
                elif analysis_type == "structure":
                    prompt_categories.append("structure_analysis")
                elif analysis_type == "knowledge_graph":
                    prompt_categories.append("knowledge_graph")

            # í”„ë¡¬í”„íŠ¸ ìš”ì²­ ìƒì„±
            prompt_requests = self.prompt_manager.create_chunk_prompt_set(
                chunk_info=chunk,
                source_file=source_file,
                prompt_categories=prompt_categories
            )

            self.logger.debug(f"ğŸ“ ì²­í¬ {chunk['chunk_id']}: {len(prompt_requests)}ê°œ í”„ë¡¬í”„íŠ¸ ìƒì„±")

        except Exception as e:
            self.logger.error(f"âŒ ì²­í¬ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    def _generate_chunk_analysis_report(self, chunk_result: ChunkAnalysisResult):
        """ì²­í¬ ë¶„ì„ ì™„ë£Œ í›„ ë³´ê³ ì„œ ìƒì„±"""
        try:
            if not self.prompt_manager:
                return

            # ë¶„ì„ ê²°ê³¼ë¥¼ ìš”ì•½í•œ ê°œë³„ ë¡œê·¸ ìƒì„±
            chunk_summary = {
                'chunk_id': chunk_result.chunk_id,
                'level': chunk_result.level,
                'content_length': chunk_result.content_length,
                'keywords_count': len(chunk_result.keywords),
                'summary_available': bool(chunk_result.summary),
                'structure_analysis_available': bool(chunk_result.structure_analysis),
                'knowledge_graph_available': bool(chunk_result.knowledge_graph),
                'analysis_timestamp': chunk_result.analysis_metadata.get('analysis_timestamp'),
            }

            # ê°œë³„ ì²­í¬ ê²°ê³¼ íŒŒì¼ ì €ì¥
            chunk_result_file = self.prompt_manager.results_dir / f"{chunk_result.chunk_id}_analysis_summary.json"
            with open(chunk_result_file, 'w', encoding='utf-8') as f:
                json.dump(chunk_summary, f, ensure_ascii=False, indent=2)

            # ì²­í¬ë³„ ìƒì„¸ ë¶„ì„ ê²°ê³¼ íŒŒì¼
            chunk_detail_file = self.prompt_manager.results_dir / f"{chunk_result.chunk_id}_detailed_analysis.json"
            detailed_data = {
                'chunk_id': chunk_result.chunk_id,
                'level': chunk_result.level,
                'content_length': chunk_result.content_length,
                'keywords': chunk_result.keywords,
                'summary': chunk_result.summary,
                'structure_analysis': chunk_result.structure_analysis,
                'knowledge_graph': chunk_result.knowledge_graph,
                'analysis_metadata': chunk_result.analysis_metadata
            }

            with open(chunk_detail_file, 'w', encoding='utf-8') as f:
                json.dump(detailed_data, f, ensure_ascii=False, indent=2)

            # í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±
            report_text = self.prompt_manager.generate_chunk_report(chunk_result.chunk_id)
            report_file = self.prompt_manager.results_dir / f"{chunk_result.chunk_id}_analysis_report.md"

            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_text)
                f.write("\n\n## ë¶„ì„ ê²°ê³¼ ìš”ì•½\n\n")
                f.write(f"- **í‚¤ì›Œë“œ ìˆ˜**: {len(chunk_result.keywords)}\n")
                f.write(f"- **ìš”ì•½**: {'ìƒì„±ë¨' if chunk_result.summary else 'ì—†ìŒ'}\n")
                f.write(f"- **êµ¬ì¡° ë¶„ì„**: {'ì™„ë£Œ' if chunk_result.structure_analysis else 'ì—†ìŒ'}\n")
                f.write(f"- **ì§€ì‹ ê·¸ë˜í”„**: {'ìƒì„±ë¨' if chunk_result.knowledge_graph else 'ì—†ìŒ'}\n")

                if chunk_result.keywords:
                    f.write(f"\n### ì¶”ì¶œëœ í‚¤ì›Œë“œ\n\n")
                    for i, kw in enumerate(chunk_result.keywords[:10], 1):
                        keyword_text = kw.get('keyword', 'ì•Œ ìˆ˜ ì—†ìŒ')
                        score = kw.get('score', 0)
                        f.write(f"{i}. **{keyword_text}** (ì ìˆ˜: {score:.3f})\n")

            self.logger.debug(f"ğŸ“Š ì²­í¬ {chunk_result.chunk_id} ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"âŒ ì²­í¬ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    def execute_chunk_prompts_with_llm(self, chunk_id: str, llm_executor) -> List[ChunkPromptResult]:
        """
        ì²­í¬ì— ëŒ€í•´ ìƒì„±ëœ ëª¨ë“  í”„ë¡¬í”„íŠ¸ë¥¼ LLMìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

        Args:
            chunk_id: ì²­í¬ ID
            llm_executor: LLM ì‹¤í–‰ í•¨ìˆ˜ (prompt_text -> response_text)

        Returns:
            List[ChunkPromptResult]: ì‹¤í–‰ ê²°ê³¼ ëª©ë¡
        """
        if not self.prompt_manager:
            raise ValueError("í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ ìš”ì²­ ë¡œë“œ
        requests_file = self.prompt_manager.prompts_dir / f"{chunk_id}_prompt_requests.json"

        if not requests_file.exists():
            self.logger.warning(f"âš ï¸ ì²­í¬ {chunk_id}ì˜ í”„ë¡¬í”„íŠ¸ ìš”ì²­ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        with open(requests_file, 'r', encoding='utf-8') as f:
            requests_data = json.load(f)

        results = []
        requests = requests_data['requests']

        self.logger.info(f"ğŸš€ ì²­í¬ {chunk_id}: {len(requests)}ê°œ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ ì‹œì‘")

        for request_dict in requests:
            # ë”•ì…”ë„ˆë¦¬ë¥¼ ChunkPromptRequest ê°ì²´ë¡œ ë³€í™˜
            request = ChunkPromptRequest(**request_dict)

            # í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
            result = self.prompt_manager.execute_prompt(request, llm_executor)
            results.append(result)

            status = "âœ…" if result.success else "âŒ"
            self.logger.info(f"  {status} {request.prompt_category}.{request.prompt_template}")

        successful = len([r for r in results if r.success])
        self.logger.info(f"ğŸ‰ ì²­í¬ {chunk_id} í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ ì™„ë£Œ: {successful}/{len(results)} ì„±ê³µ")

        return results


__all__ = ["ChunkAnalyzer", "ChunkAnalysisResult", "IntegratedAnalysisResult"]