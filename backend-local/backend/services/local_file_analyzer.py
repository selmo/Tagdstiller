"""LLM-only document structure analyzer for the backend-local service."""

from __future__ import annotations

import json
import logging
import re
import time
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import requests
from sqlalchemy.orm import Session

from prompts.templates import DocumentStructurePrompts
from services.config_service import ConfigService
from utils.llm_logger import log_prompt_and_response

try:  # pragma: no cover - optional dependency
    from langchain_ollama import OllamaLLM  # type: ignore
except ImportError:  # pragma: no cover - optional dependency
    OllamaLLM = None


class LLMJsonError(RuntimeError):
    """Raised when the LLM response cannot be parsed as JSON."""

    def __init__(self, message: str, raw_response: str):
        super().__init__(message)
        self.raw_response = raw_response


class LocalFileAnalyzer:
    """Minimal analyzer that delegates structure extraction to an LLM."""

    def __init__(self, db: Session):
        self.db = db
        self.logger = logging.getLogger(__name__)

    def analyze_document_structure_with_llm(
        self,
        text: str,
        file_path: str,
        file_extension: str,
        overrides: Optional[Dict[str, Any]] = None,
        use_multistep: bool = False,  # ë‹¤ë‹¨ê³„ ëŒ€í™” ë°©ì‹ ì‚¬ìš© ì—¬ë¶€
    ) -> Dict[str, Any]:
        overrides = overrides or {}
        provider = overrides.get("provider") or ConfigService.get_config_value(self.db, "LLM_PROVIDER", "ollama")
        if not overrides.get("enabled") and not ConfigService.get_bool_config(self.db, "ENABLE_LLM_EXTRACTION", False):
            return self._fail_result("LLM extraction disabled", "")

        timeout_override = overrides.get("timeout")
        ollama_timeout_default = ConfigService.get_int_config(self.db, "OLLAMA_TIMEOUT", 120)

        if provider == "ollama":
            if OllamaLLM is None:
                return self._fail_result("langchain_ollama is not installed", "")
            base_url = overrides.get("base_url") or ConfigService.get_config_value(self.db, "OLLAMA_BASE_URL", "http://localhost:11434")
            model_name = overrides.get("model") or ConfigService.get_config_value(self.db, "OLLAMA_MODEL", "llama3.2")
            timeout = timeout_override or ollama_timeout_default
            fetch = lambda prompt: self._call_ollama(prompt, base_url, model_name, timeout)
            base_dir_provider = "ollama"
            base_url_meta = base_url
        elif provider == "openai":
            conf = {**ConfigService.get_openai_config(self.db), **overrides}
            if timeout_override is not None:
                conf["timeout"] = timeout_override
            conf.setdefault("timeout", 120)
            fetch = lambda prompt: self._call_openai_chat(prompt, conf)
            base_dir_provider = "openai"
            base_url_meta = conf.get("base_url", "https://api.openai.com/v1")
            model_name = conf.get("model", "")
        elif provider == "gemini":
            conf = {**ConfigService.get_gemini_config(self.db), **overrides}
            if timeout_override is not None:
                conf["timeout"] = timeout_override
            conf.setdefault("timeout", 120)
            conf.setdefault("response_mime_type", "application/json")
            fetch = lambda prompt: self._call_gemini_generate(prompt, conf)
            base_dir_provider = "gemini"
            base_url_meta = conf.get("base_url", "https://generativelanguage.googleapis.com")
            model_name = conf.get("model", "")
        else:
            return self._fail_result(f"Unsupported provider: {provider}", "")

        # max_tokens ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ í¬ê¸° ê³„ì‚°
        max_tokens = conf.get("max_tokens", 8000)
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ ì‘ë‹µì„ ìœ„í•œ í† í° ì˜ˆì•½ (ëŒ€ëµ 2000 í† í°)
        reserved_tokens = 2000
        available_tokens = max(max_tokens - reserved_tokens, 1000)  # ìµœì†Œ 1000 í† í° ë³´ì¥
        # í† í°ì„ ë¬¸ì ìˆ˜ë¡œ ë³€í™˜ (í•œêµ­ì–´ ê¸°ì¤€ ì•½ 1.5ì/í† í°)
        max_chars = int(available_tokens * 1.5)

        truncated_text = text[:max_chars] if len(text) > max_chars else text
        file_info = {
            "filename": Path(file_path).name,
            "extension": file_extension,
            "size": len(text),
            "truncated_size": len(truncated_text),
        }
        prompt_template = (
            DocumentStructurePrompts.STRUCTURE_ANALYSIS_LLM_GEMINI_FLASH25
            if provider == "gemini" and isinstance(model_name, str) and "2.5" in model_name
            else DocumentStructurePrompts.STRUCTURE_ANALYSIS_LLM
        )
        # ë‹¤ë‹¨ê³„ ëŒ€í™” ë°©ì‹ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        if use_multistep:
            self.logger.info("ğŸ”„ ë‹¤ë‹¨ê³„ ëŒ€í™” ë°©ì‹ìœ¼ë¡œ ë¶„ì„ ì§„í–‰")
            try:
                return self._analyze_with_multistep_conversation(
                    text=truncated_text,
                    file_info=file_info,
                    fetch=fetch,
                    model_name=model_name,
                    base_dir_provider=base_dir_provider,
                    base_url_meta=base_url_meta,
                    file_path=file_path,
                    overrides=overrides
                )
            except Exception as e:
                self.logger.error(f"âŒ ë‹¤ë‹¨ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}, ì¼ë°˜ ë°©ì‹ìœ¼ë¡œ í´ë°±")
                # í´ë°±: ì¼ë°˜ ë°©ì‹ìœ¼ë¡œ ì§„í–‰

        # ì¼ë°˜ ë°©ì‹ (ë‹¨ì¼ í”„ë¡¬í”„íŠ¸)
        prompt = prompt_template.format(
            file_info=json.dumps(file_info, ensure_ascii=False, indent=2),
            text=truncated_text,
        )

        try:
            response_text, raw_response = fetch(prompt)
        except LLMJsonError as exc:
            return self._fail_result(str(exc), exc.raw_response)
        except Exception as exc:  # pragma: no cover - transport errors
            self.logger.error("âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: %s", exc)
            return self._fail_result(str(exc), "")

        log_prompt_and_response(
            label="document_structure_analysis",
            provider=base_dir_provider,
            model=model_name,
            prompt=prompt,
            response=response_text,
            logger=self.logger,
            base_dir=self._default_output_dir(file_path),
            meta={
                "base_url": base_url_meta,
                "temperature": overrides.get("temperature", 0.2),
                "file_extension": file_extension,
                "text_length": len(text),
                "truncated_length": len(truncated_text),
            },
            raw_response=raw_response,
        )

        # JSON íŒŒì‹± ì‹œë„ with ì—ëŸ¬ ì²˜ë¦¬
        try:
            json_response = json.loads(response_text)
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            self.logger.error(f"   ì˜¤ë¥˜ ìœ„ì¹˜: line {e.lineno}, column {e.colno}, char {e.pos}")
            self.logger.error(f"   ì‘ë‹µ ê¸¸ì´: {len(response_text)} ë¬¸ì")

            # í† í° ì œí•œ ì´ˆê³¼ ì—¬ë¶€ í™•ì¸ (raw_responseì—ì„œ)
            is_token_limit = False
            if "MAX_TOKENS" in raw_response or "max_tokens" in raw_response.lower():
                is_token_limit = True
                self.logger.error(f"   âš ï¸ ì›ì¸: í† í° ì œí•œ ì´ˆê³¼ë¡œ ì‘ë‹µì´ ì˜ë¦° ê²ƒìœ¼ë¡œ ì¶”ì •ë¨")

            # ì‘ë‹µ í…ìŠ¤íŠ¸ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ ë¡œê¹… (ë””ë²„ê¹…ìš©)
            if len(response_text) > 200:
                self.logger.error(f"   ì‘ë‹µ ëë¶€ë¶„: ...{response_text[-200:]}")

            # ì˜ë¦° JSON ë¶€ë¶„ ë³µêµ¬ ì‹œë„
            if is_token_limit and response_text.strip():
                self.logger.info("ğŸ”§ ì˜ë¦° JSON ë³µêµ¬ ì‹œë„ ì¤‘...")
                recovered_json = self._try_recover_truncated_json(response_text)
                if recovered_json:
                    self.logger.info("âœ… ë¶€ë¶„ JSON ë³µêµ¬ ì„±ê³µ")
                    return {
                        "analysis_method": "llm_partial_recovery",
                        "llm_success": True,
                        "llm_model": model_name,
                        "llm_analysis": recovered_json,
                        "warning": "í† í° ì œí•œìœ¼ë¡œ ì‘ë‹µì´ ì˜ë ¸ìœ¼ë‚˜ ë¶€ë¶„ ë³µêµ¬ë¨"
                    }

            return self._fail_result(
                f"LLM ì‘ë‹µì´ ìœ íš¨í•œ JSONì´ ì•„ë‹™ë‹ˆë‹¤: {str(e)}" +
                (" (í† í° ì œí•œ ì´ˆê³¼ë¡œ ì¸í•œ ì˜ë¦¼ ê°€ëŠ¥ì„±)" if is_token_limit else ""),
                response_text
            )

        return {
            "analysis_method": "llm_only",
            "llm_success": True,
            "llm_model": model_name,
            "llm_analysis": json_response,
        }

    # ------------------------------------------------------------------
    # Multi-step conversation helpers
    # ------------------------------------------------------------------
    def _analyze_with_multistep_conversation(
        self,
        text: str,
        file_info: Dict[str, Any],
        fetch: callable,
        model_name: str,
        base_dir_provider: str,
        base_url_meta: str,
        file_path: str,
        overrides: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ë‹¤ë‹¨ê³„ ëŒ€í™” ë°©ì‹ìœ¼ë¡œ ë¬¸ì„œ ë¶„ì„

        ì „ëµ:
        1. ì „ì²´ êµ¬ì¡° ê°œìš” ìš”ì²­ (ì‘ì€ ì‘ë‹µ)
        2. ê° ì„¹ì…˜ë³„ ìƒì„¸ ë¶„ì„ ìš”ì²­ (ì—¬ëŸ¬ ë²ˆì˜ ì‘ì€ ì‘ë‹µ)
        3. ëª¨ë“  ì‘ë‹µ ë³‘í•©
        """
        self.logger.info("ğŸ“ 1ë‹¨ê³„: ë¬¸ì„œ êµ¬ì¡° ê°œìš” ì¶”ì¶œ")

        # Step 1: ë¬¸ì„œ êµ¬ì¡° ê°œìš”ë§Œ ì¶”ì¶œ
        overview_prompt = f"""ë‹¤ìŒ ë¬¸ì„œì˜ êµ¬ì¡°ë¥¼ **ê°„ëµí•˜ê²Œ** ë¶„ì„í•˜ì„¸ìš”.

íŒŒì¼ ì •ë³´:
{json.dumps(file_info, ensure_ascii=False, indent=2)}

í…ìŠ¤íŠ¸:
{text[:3000]}... (ì „ì²´ {len(text)}ì)

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "documentType": "ë¬¸ì„œ ìœ í˜•",
  "mainSections": ["ì„¹ì…˜1", "ì„¹ì…˜2", "ì„¹ì…˜3"],
  "estimatedPageCount": ìˆ«ì,
  "primaryLanguage": "ko/en"
}}
"""

        overview_text, overview_raw = fetch(overview_prompt)
        overview_json = json.loads(self._strip_markers(overview_text))

        self.logger.info(f"âœ… êµ¬ì¡° ê°œìš”: {len(overview_json.get('mainSections', []))}ê°œ ì„¹ì…˜ ë°œê²¬")

        # Step 2: ê° ì„¹ì…˜ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¶„í•  ì •ë³µ)
        sections = overview_json.get("mainSections", [])
        section_analyses = []

        for idx, section_name in enumerate(sections[:5]):  # ìµœëŒ€ 5ê°œ ì„¹ì…˜
            self.logger.info(f"ğŸ“ {idx+1}ë‹¨ê³„: '{section_name}' ì„¹ì…˜ ë¶„ì„ ì¤‘...")

            section_prompt = f"""ë¬¸ì„œì—ì„œ "{section_name}" ì„¹ì…˜ì˜ í•µì‹¬ í‚¤ì›Œë“œ 5ê°œë§Œ ì¶”ì¶œí•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text[:5000]}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "section": "{section_name}",
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3", "í‚¤ì›Œë“œ4", "í‚¤ì›Œë“œ5"]
}}
"""

            try:
                section_text, section_raw = fetch(section_prompt)
                section_json = json.loads(self._strip_markers(section_text))
                section_analyses.append(section_json)
                self.logger.info(f"âœ… '{section_name}' ë¶„ì„ ì™„ë£Œ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ '{section_name}' ë¶„ì„ ì‹¤íŒ¨: {e}")
                continue

        # Step 3: ê²°ê³¼ ë³‘í•©
        merged_analysis = {
            "structureAnalysis": {
                "documentType": overview_json.get("documentType"),
                "language": overview_json.get("primaryLanguage"),
                "sections": overview_json.get("mainSections", []),
                "estimatedPages": overview_json.get("estimatedPageCount", 0)
            },
            "keywordsBySection": section_analyses,
            "allKeywords": []
        }

        # ëª¨ë“  í‚¤ì›Œë“œ í†µí•©
        for section_analysis in section_analyses:
            merged_analysis["allKeywords"].extend(section_analysis.get("keywords", []))

        self.logger.info(f"âœ… ë‹¤ë‹¨ê³„ ë¶„ì„ ì™„ë£Œ: {len(section_analyses)}ê°œ ì„¹ì…˜, {len(merged_analysis['allKeywords'])}ê°œ í‚¤ì›Œë“œ")

        return {
            "analysis_method": "llm_multistep",
            "llm_success": True,
            "llm_model": model_name,
            "llm_analysis": merged_analysis,
            "steps_completed": len(section_analyses) + 1
        }

    # ------------------------------------------------------------------
    # Provider helpers
    # ------------------------------------------------------------------
    def _call_gemini_stream(
        self, base_url: str, model: str, api_key: str, payload: Dict[str, Any], conf: Dict[str, Any]
    ) -> Tuple[str, str]:
        """
        Gemini ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ - ì‘ë‹µì„ ì²­í¬ë¡œ ë°›ì•„ì„œ í•©ì¹¨

        ì¥ì :
        - í† í° ì œí•œì„ ë„˜ì–´ë„ ì „ì²´ ì‘ë‹µ ìˆ˜ì‹  ê°€ëŠ¥
        - ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ í™•ì¸ ê°€ëŠ¥
        """
        import time

        url = f"{base_url}/v1beta/{model}:streamGenerateContent?key={api_key}&alt=sse"

        self.logger.info("ğŸ“¡ Gemini ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ ì‹œì‘...")

        # ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        retry_delay = 2

        for attempt in range(max_retries):
            try:
                response = requests.post(
                    url,
                    json=payload,
                    timeout=conf.get("timeout", 180),  # ìŠ¤íŠ¸ë¦¬ë°ì€ ë” ê¸´ íƒ€ì„ì•„ì›ƒ
                    stream=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
                )
                response.raise_for_status()
                break
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 503 and attempt < max_retries - 1:
                    self.logger.warning(f"âš ï¸ Gemini Stream API 503 ì—ëŸ¬, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    raise

        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
        accumulated_text = []
        accumulated_responses = []
        total_chunks = 0
        last_log_time = time.time()

        for line in response.iter_lines():
            if not line:
                continue

            line_str = line.decode('utf-8')

            # SSE í˜•ì‹ íŒŒì‹±: "data: {JSON}"
            if line_str.startswith('data: '):
                json_str = line_str[6:]  # "data: " ì œê±°
                try:
                    chunk_data = json.loads(json_str)
                    accumulated_responses.append(chunk_data)

                    # í…ìŠ¤íŠ¸ ì²­í¬ ì¶”ì¶œ
                    for candidate in chunk_data.get("candidates", []):
                        parts = candidate.get("content", {}).get("parts", [])
                        for part in parts:
                            if isinstance(part, dict) and "text" in part:
                                text_chunk = part["text"]
                                accumulated_text.append(text_chunk)
                                total_chunks += 1

                                # ì£¼ê¸°ì  ë¡œê¹… (1ì´ˆë§ˆë‹¤)
                                current_time = time.time()
                                if current_time - last_log_time > 1.0:
                                    current_length = sum(len(t) for t in accumulated_text)
                                    self.logger.info(f"ğŸ“¥ ìŠ¤íŠ¸ë¦¬ë° ìˆ˜ì‹  ì¤‘... {total_chunks}ê°œ ì²­í¬, {current_length:,}ì")
                                    last_log_time = current_time

                except json.JSONDecodeError as e:
                    self.logger.warning(f"âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue

        # ì „ì²´ ì‘ë‹µ ë³‘í•©
        merged_text = "".join(accumulated_text).strip()

        # í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹… (ë§ˆì§€ë§‰ ì²­í¬ì—ì„œ)
        if accumulated_responses:
            last_response = accumulated_responses[-1]
            usage_metadata = last_response.get("usageMetadata", {})
            if usage_metadata:
                prompt_tokens = usage_metadata.get("promptTokenCount", 0)
                response_tokens = usage_metadata.get("candidatesTokenCount", 0)
                total_tokens = usage_metadata.get("totalTokenCount", 0)
                self.logger.info(
                    f"ğŸ“Š Gemini ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ - í”„ë¡¬í”„íŠ¸: {prompt_tokens}, ì‘ë‹µ: {response_tokens}, ì´í•©: {total_tokens}"
                )

        self.logger.info(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {total_chunks}ê°œ ì²­í¬ ë³‘í•©, ì´ {len(merged_text):,}ì")

        # raw_payloadëŠ” ì „ì²´ ì‘ë‹µ ë°°ì—´
        raw_payload = json.dumps(accumulated_responses, ensure_ascii=False)

        if not merged_text:
            raise LLMJsonError("Gemini streaming response is empty", raw_payload)

        return merged_text, raw_payload

    def _call_ollama(self, prompt: str, base_url: str, model: str, timeout: int) -> Tuple[str, str]:
        assert OllamaLLM is not None  # for type checkers
        client = OllamaLLM(base_url=base_url, model=model, timeout=timeout, temperature=0.2)
        response = client.invoke(prompt)
        return response, response

    def _call_openai_chat(self, prompt: str, conf: Dict[str, Any]) -> Tuple[str, str]:
        api_key = conf.get("api_key")
        if not api_key:
            raise RuntimeError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        payload = {
            "model": conf.get("model", "gpt-3.5-turbo"),
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": conf.get("max_tokens", 8000),
            "temperature": conf.get("temperature", 0.2),
        }
        response = requests.post(
            f"{conf.get('base_url', 'https://api.openai.com/v1')}/chat/completions",
            headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
            json=payload,
            timeout=conf.get("timeout", 120),
        )
        response.raise_for_status()
        text = response.json()["choices"][0]["message"]["content"]
        return text, text

    def _call_gemini_generate(self, prompt: str, conf: Dict[str, Any]) -> Tuple[str, str]:
        """
        Gemini API í˜¸ì¶œ - ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ìš°ì„  ì‚¬ìš©

        í† í° ì œí•œ ì´ˆê³¼ ë°©ì§€ë¥¼ ìœ„í•´:
        1. streamGenerateContent API ì‚¬ìš© (ì‘ë‹µì„ ì²­í¬ë¡œ ìˆ˜ì‹ )
        2. ì¼ë°˜ generateContentë¡œ í´ë°± (ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ)
        """
        import time

        api_key = conf.get("api_key")
        if not api_key:
            raise RuntimeError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        payload: Dict[str, Any] = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "temperature": conf.get("temperature", 0.2),
                "maxOutputTokens": conf.get("max_tokens", 8000),
            },
        }
        if conf.get("response_mime_type"):
            payload["generationConfig"]["responseMimeType"] = conf["response_mime_type"]

        base_url = conf.get('base_url', 'https://generativelanguage.googleapis.com')
        model = conf.get('model', 'models/gemini-1.5-pro')

        # ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        use_streaming = conf.get("use_streaming", True)

        if use_streaming:
            # ìŠ¤íŠ¸ë¦¬ë° API ì‹œë„
            try:
                return self._call_gemini_stream(base_url, model, api_key, payload, conf)
            except Exception as stream_error:
                self.logger.warning(f"âš ï¸ ìŠ¤íŠ¸ë¦¬ë° API ì‹¤íŒ¨, ì¼ë°˜ APIë¡œ í´ë°±: {stream_error}")
                # í´ë°±: ì¼ë°˜ API ì‚¬ìš©

        # ì¼ë°˜ API (non-streaming)
        url = f"{base_url}/v1beta/{model}:generateContent?key={api_key}"

        # ì¬ì‹œë„ ë¡œì§ (503 ì—ëŸ¬ ëŒ€ì‘)
        max_retries = 3
        retry_delay = 2  # ì´ˆ

        for attempt in range(max_retries):
            try:
                response = requests.post(url, json=payload, timeout=conf.get("timeout", 120))
                response.raise_for_status()
                break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 503 and attempt < max_retries - 1:
                    self.logger.warning(f"âš ï¸ Gemini API 503 ì—ëŸ¬, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                else:
                    raise  # ë§ˆì§€ë§‰ ì‹œë„ê±°ë‚˜ ë‹¤ë¥¸ ì—ëŸ¬ë©´ ì˜ˆì™¸ ë°œìƒ

        data = response.json()

        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ ë° ë¡œê¹…
        usage_metadata = data.get("usageMetadata", {})
        if usage_metadata:
            prompt_tokens = usage_metadata.get("promptTokenCount", 0)
            response_tokens = usage_metadata.get("candidatesTokenCount", 0)
            total_tokens = usage_metadata.get("totalTokenCount", 0)
            self.logger.info(
                f"ğŸ“Š Gemini í† í° ì‚¬ìš©ëŸ‰ - í”„ë¡¬í”„íŠ¸: {prompt_tokens}, ì‘ë‹µ: {response_tokens}, ì´í•©: {total_tokens}"
            )

        # finishReason í™•ì¸ (í† í° ì´ˆê³¼ ê°ì§€)
        candidates = data.get("candidates", [])
        if candidates:
            finish_reason = candidates[0].get("finishReason", "")
            if finish_reason == "MAX_TOKENS":
                self.logger.warning("âš ï¸ ì‘ë‹µì´ max_tokens ì œí•œìœ¼ë¡œ ì˜ë ¸ìŠµë‹ˆë‹¤ (finishReason: MAX_TOKENS)")
                # í† í° ì œí•œ ì´ˆê³¼ ì •ë³´ë¥¼ raw_payloadì— í‘œì‹œ
                max_tokens_exceeded = True
            else:
                max_tokens_exceeded = False
        else:
            max_tokens_exceeded = False

        raw_payload = json.dumps(data, ensure_ascii=False)
        text_chunks: list[str] = []
        for candidate in data.get("candidates", []):
            parts = candidate.get("content", {}).get("parts", [])
            for part in parts:
                if isinstance(part, dict):
                    text_part = part.get("text")
                    if isinstance(text_part, str):
                        text_chunks.append(text_part)

        if not text_chunks:
            raise LLMJsonError("Gemini response contained no text parts", raw_payload)

        merged = "".join(text_chunks).strip()
        self.logger.info("âœ… Gemini response length: %d characters", len(merged))
        if not merged:
            raise LLMJsonError("Gemini response text is empty", raw_payload)

        # í† í° ì´ˆê³¼ë¡œ ì˜ë¦° ê²½ìš° ê²½ê³  ë¡œê·¸
        if max_tokens_exceeded:
            self.logger.warning(
                f"âš ï¸ ì‘ë‹µì´ í† í° ì œí•œìœ¼ë¡œ ì˜ë ¸ì„ ê°€ëŠ¥ì„± ìˆìŒ - "
                f"ì‘ë‹µ ê¸¸ì´: {len(merged)}ì, max_tokens: {conf.get('max_tokens', 8000)}"
            )

        return merged, raw_payload

    # ------------------------------------------------------------------
    # JSON helpers
    # ------------------------------------------------------------------
    def _extract_json_from_response(self, response: str) -> Dict[str, Any]:
        json_text = self._strip_markers(response)
        try:
            return json.loads(json_text)
        except json.JSONDecodeError as exc:
            raise ValueError(f"Unable to parse JSON from LLM response: {exc}") from exc

    @staticmethod
    def _strip_markers(text: str) -> str:
        stripped = text.strip()
        stripped = re.sub(r'^```json\s*', '', stripped, flags=re.IGNORECASE)
        stripped = re.sub(r'^```\s*', '', stripped)
        stripped = re.sub(r'\s*```$', '', stripped)
        if stripped.startswith('\ufeff'):
            stripped = stripped.lstrip('\ufeff')
        return stripped

    def _default_output_dir(self, file_path: str) -> str:
        try:
            from services.document_parser_service import DocumentParserService

            parser_service = DocumentParserService()
            absolute_path = Path(file_path) if Path(file_path).is_absolute() else Path.cwd() / file_path
            output_dir = parser_service.get_output_directory(absolute_path)
            return str(output_dir)
        except Exception:
            return "llm_logs"

    def _try_recover_truncated_json(self, truncated_text: str) -> Optional[Dict[str, Any]]:
        """
        í† í° ì œí•œìœ¼ë¡œ ì˜ë¦° JSONì„ ë¶€ë¶„ì ìœ¼ë¡œ ë³µêµ¬ ì‹œë„

        ì „ëµ:
        1. ìœ íš¨í•œ JSON ê°ì²´ê°€ ì™„ì„±ëœ ë¶€ë¶„ê¹Œì§€ ì°¾ê¸°
        2. ë°°ì—´ì´ë‚˜ ê°ì²´ê°€ ë‹«íˆì§€ ì•Šì€ ê²½ìš° ê°•ì œë¡œ ë‹«ê¸°
        3. ìµœì†Œí•œì˜ í•„ìˆ˜ í•„ë“œë§Œì´ë¼ë„ ì¶”ì¶œ
        """
        try:
            # ì „ëµ 1: ë§ˆì§€ë§‰ ì™„ì „í•œ ê°ì²´/ë°°ì—´ê¹Œì§€ë§Œ íŒŒì‹±
            # ë’¤ì—ì„œë¶€í„° ìœ íš¨í•œ JSONì´ ëë‚˜ëŠ” ìœ„ì¹˜ ì°¾ê¸°
            for i in range(len(truncated_text) - 1, -1, -1):
                try:
                    partial = truncated_text[:i+1]
                    # ë‹«íˆì§€ ì•Šì€ ì¤‘ê´„í˜¸/ëŒ€ê´„í˜¸ ìˆ˜ ì„¸ê¸°
                    open_braces = partial.count('{') - partial.count('}')
                    open_brackets = partial.count('[') - partial.count(']')

                    # ë‹«íˆì§€ ì•Šì€ ê²ƒë“¤ì„ ê°•ì œë¡œ ë‹«ê¸°
                    closing = ']' * open_brackets + '}' * open_braces
                    fixed = partial + closing

                    parsed = json.loads(fixed)
                    self.logger.info(f"ğŸ”§ ë³µêµ¬ ì„±ê³µ: {i+1}/{len(truncated_text)} ë¬¸ìê¹Œì§€ ìœ íš¨ (ë‹«íŒ ê´„í˜¸: {len(closing)}ê°œ)")
                    return parsed
                except json.JSONDecodeError:
                    # 100ì ë‹¨ìœ„ë¡œ ê±´ë„ˆë›°ë©° ì‹œë„ (ì„±ëŠ¥ ìµœì í™”)
                    if i % 100 != 0:
                        continue

            # ì „ëµ 2: structureAnalysis í•„ë“œë§Œ ì¶”ì¶œ ì‹œë„
            match = re.search(r'"structureAnalysis"\s*:\s*({[^}]*})', truncated_text, re.DOTALL)
            if match:
                try:
                    structure_part = json.loads(match.group(1) + '}')
                    self.logger.info("ğŸ”§ ë¶€ë¶„ ë³µêµ¬: structureAnalysis í•„ë“œë§Œ ì¶”ì¶œ")
                    return {"structureAnalysis": structure_part}
                except:
                    pass

            self.logger.warning("âš ï¸ JSON ë³µêµ¬ ì‹¤íŒ¨: ìœ íš¨í•œ ë¶€ë¶„ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None

        except Exception as e:
            self.logger.error(f"âŒ JSON ë³µêµ¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def _fail_result(self, message: str, raw_response: str) -> Dict[str, Any]:
        self.logger.error("âŒ LLM êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: %s", message)
        if raw_response:
            self.logger.warning("âš ï¸ LLM ì›ë³¸ ì‘ë‹µ (200ì): %s", raw_response[:200].replace("\n", " "))
        return {
            "analysis_method": "llm_failed",
            "llm_success": False,
            "llm_error": message,
            "llm_raw_response": raw_response,
        }


__all__ = ["LocalFileAnalyzer", "LLMJsonError"]
