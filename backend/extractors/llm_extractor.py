from typing import List, Dict, Any, Optional
from pathlib import Path
import json
import time
import requests
from .base import KeywordExtractor, Keyword
from utils.text_cleaner import TextCleaner
from utils.position_mapper import PositionMapper
from utils.debug_logger import get_debug_logger
from prompts.templates import get_prompt_template
from prompts.config import PromptConfig
from utils.llm_logger import log_prompt_and_response

# LangChain imports
try:
    from langchain_ollama import OllamaLLM
except ImportError:
    try:
        from langchain_community.llms import Ollama as OllamaLLM
    except ImportError:
        OllamaLLM = None

class LLMExtractor(KeywordExtractor):
    """LLM ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸° (Ollama/OpenAI/Gemini)"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, db_session = None):
        super().__init__("llm", config)
        self.client = None
        self.provider = config.get('provider', 'ollama') if config else 'ollama'
        self.model_name = config.get('model', 'llama3.2') if config else 'llama3.2'
        self.base_url = config.get('base_url', 'http://localhost:11434') if config else 'http://localhost:11434'
        
        # í”„ë¡¬í”„íŠ¸ ì„¤ì • ì´ˆê¸°í™”
        self.prompt_config = PromptConfig(config, db_session)
        
        # LangChain Ollama ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
        self.ollama_client = None
    
    def load_model(self) -> bool:
        """LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        import logging
        logger = logging.getLogger(__name__)
        
        try:
            logger.info(f"LLM ì¶”ì¶œê¸° ë¡œë“œ ì‹œì‘: provider={self.provider}, model={self.model_name}, base_url={self.base_url}")
            
            if self.provider == 'ollama':
                # LangChain Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                try:
                    if not OllamaLLM:
                        raise ImportError("LangChain Ollama not available")
                    
                    self.ollama_client = OllamaLLM(
                        base_url=self.base_url,
                        model=self.model_name,
                        timeout=self.config.get('timeout', 30) if self.config else 30
                    )
                    
                    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ì—°ê²° í™•ì¸
                    logger.info(f"LangChain Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
                    test_response = self.ollama_client.invoke("Hello")
                    logger.info(f"âœ… LangChain Ollama ì—°ê²° ì„±ê³µ: {len(test_response)} ë¬¸ì ì‘ë‹µ")
                    
                    self.is_loaded = True
                    logger.info(f"âœ… LLM ì¶”ì¶œê¸° ë¡œë“œ ì„±ê³µ: {self.model_name}")
                    
                except Exception as e:
                    logger.error(f"âŒ LangChain Ollama ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    
                    # í´ë°±: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì—°ê²° í™•ì¸
                    logger.info("ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ Ollama ì—°ê²° í™•ì¸ ì¤‘...")
                    response = requests.get(f"{self.base_url}/api/tags", timeout=5)
                    logger.info(f"Ollama ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                    
                    if response.status_code == 200:
                        models = response.json().get("models", [])
                        model_names = [m['name'] for m in models]
                        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸: {model_names}")
                        
                        # ëª¨ë¸ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                        model_found = any(model["name"].startswith(self.model_name) for model in models)
                        logger.info(f"ëª¨ë¸ '{self.model_name}' ê²€ìƒ‰ ê²°ê³¼: {model_found}")
                        
                        if model_found:
                            # LangChain í´ë¼ì´ì–¸íŠ¸ ë‹¤ì‹œ ì‹œë„
                            try:
                                self.ollama_client = OllamaLLM(
                                    base_url=self.base_url,
                                    model=self.model_name,
                                    timeout=self.config.get('timeout', 30) if self.config else 30
                                )
                                self.is_loaded = True
                                logger.info(f"âœ… LLM ì¶”ì¶œê¸° ë¡œë“œ ì„±ê³µ (ì¬ì‹œë„): {self.model_name}")
                            except Exception as retry_e:
                                logger.error(f"âŒ LangChain ì¬ì‹œë„ ì‹¤íŒ¨: {retry_e}")
                                self.is_loaded = False
                        else:
                            logger.error(f"âŒ ëª¨ë¸ '{self.model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {model_names}")
                            self.is_loaded = False
                    else:
                        logger.error(f"âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {self.base_url} (ìƒíƒœ: {response.status_code})")
                        self.is_loaded = False
            
            elif self.provider == 'openai':
                # OpenAIëŠ” REST í˜¸ì¶œ ê¸°ë°˜. API í‚¤ë§Œ í™•ì¸
                import os
                api_key = self.config.get('api_key') if self.config else None
                if not api_key:
                    api_key = os.environ.get('OPENAI_API_KEY')
                if not api_key:
                    logger.error("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    self.is_loaded = False
                else:
                    self.is_loaded = True
                    logger.info("âœ… OpenAI provider ì¤€ë¹„ ì™„ë£Œ")
            elif self.provider == 'gemini':
                # Geminië„ REST í˜¸ì¶œ ê¸°ë°˜. API í‚¤ í™•ì¸
                import os
                api_key = self.config.get('api_key') if self.config else None
                if not api_key:
                    api_key = os.environ.get('GEMINI_API_KEY')
                if not api_key:
                    logger.error("âŒ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    self.is_loaded = False
                else:
                    self.is_loaded = True
                    logger.info("âœ… Gemini provider ì¤€ë¹„ ì™„ë£Œ")
            
            logger.info(f"LLM ì¶”ì¶œê¸° ìµœì¢… ìƒíƒœ: is_loaded={self.is_loaded}")
            return self.is_loaded
        except Exception as e:
            logger.error(f"âŒ LLM ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            return False
    
    def extract(self, text: str, file_path: Optional[Path] = None) -> List[Keyword]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        import logging
        logger = logging.getLogger(__name__)
        debug_logger = get_debug_logger()
        start_time = time.time()
        
        logger.info(f"ğŸ” LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘ - ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        # ë””ë²„ê·¸ ë¡œê¹…: ì¶”ì¶œ ì‹œì‘
        debug_logger.start_extraction(
            extractor_name="llm",
            file_info={"filename": str(file_path) if file_path else "unknown", "id": None},
            text=text,
            config=self.config
        )
        
        # ìœ„ì¹˜ ë§¤í•‘ ìƒì„±
        position_mapper = PositionMapper()
        position_map = position_mapper.create_position_map(text, file_path)
        logger.info(f"ğŸ“ ìœ„ì¹˜ ë§¤í•‘ ìƒì„± ì™„ë£Œ - ì´ {position_map['total_pages']}í˜ì´ì§€, {position_map['total_lines']}ì¤„")
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        original_text_copy = text  # ì›ë³¸ ë³´ê´€
        cleaned_text = TextCleaner.clean_text(text)
        logger.info(f"ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ - ì •ì œëœ ê¸¸ì´: {len(cleaned_text)} ë¬¸ì")
        
        # ë””ë²„ê·¸ ë¡œê¹…: ì „ì²˜ë¦¬ ê²°ê³¼
        debug_logger.log_preprocessing(
            extractor_name="llm",
            original_text=original_text_copy,
            preprocessed_text=cleaned_text,
            preprocessing_steps=["clean_text", "normalize_unicode", "llm_preprocessing"]
        )
        
        if not self.is_loaded:
            self.load_model()
        
        if not self.is_loaded:
            return []
        
        try:
            prompt = self._create_extraction_prompt(cleaned_text)
            
            # ë””ë²„ê·¸ ë¡œê¹…: ëª¨ë¸ ì •ë³´ ë° í”„ë¡¬í”„íŠ¸
            debug_logger.log_embeddings(
                extractor_name="llm",
                model_name=f"{self.provider}:{self.model_name}"
            )
            
            logger.info(f"ğŸ¯ LLM '{self.provider}:{self.model_name}'ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
            
            if self.provider == 'ollama':
                response, request_data, response_data = self._call_ollama_langchain(prompt)
                # í”„ë¡¬í”„íŠ¸/ì‘ë‹µ íŒŒì¼ ì €ì¥ ë° ë¡œê·¸ ê¸°ë¡ (JSON í¬í•¨)
                log_prompt_and_response(
                    label="keyword_extraction",
                    provider=self.provider,
                    model=self.model_name,
                    prompt=prompt,
                    response=response or "",
                    logger=logger,
                    meta={
                        "base_url": self.base_url,
                        "file_path": str(file_path) if file_path else None,
                        "config_max_keywords": self.config.get('max_keywords') if self.config else None,
                        "langchain_version": True,
                    },
                    request_data=request_data,
                    response_data=response_data,
                )
                if response:
                    keywords = self._parse_llm_response(response, text, position_mapper, position_map)
                    
                    # ë””ë²„ê·¸ ë¡œê¹…: ìµœì¢… ê²°ê³¼
                    extraction_time = time.time() - start_time
                    debug_logger.log_final_results(
                        extractor_name="llm",
                        final_keywords=[{
                            "keyword": kw.text,
                            "score": kw.score,
                            "category": kw.category,
                            "start_position": kw.start_position,
                            "end_position": kw.end_position,
                            "page_number": kw.page_number,
                            "line_number": kw.line_number,
                            "context": kw.context_snippet
                        } for kw in keywords],
                        extraction_time=extraction_time,
                        total_processing_time=time.time() - start_time
                    )
                    
                    return keywords
            elif self.provider == 'openai':
                response = self._call_openai(prompt)
                # í”„ë¡¬í”„íŠ¸/ì‘ë‹µ íŒŒì¼ ì €ì¥ ë° ë¡œê·¸ ê¸°ë¡
                log_prompt_and_response(
                    label="keyword_extraction",
                    provider=self.provider,
                    model=self.model_name,
                    prompt=prompt,
                    response=response or "",
                    logger=logger,
                    meta={
                        "file_path": str(file_path) if file_path else None,
                        "config_max_keywords": self.config.get('max_keywords') if self.config else None,
                    },
                )
                if response:
                    keywords = self._parse_llm_response(response, text, position_mapper, position_map)
                    
                    # ë””ë²„ê·¸ ë¡œê¹…: ìµœì¢… ê²°ê³¼
                    extraction_time = time.time() - start_time
                    debug_logger.log_final_results(
                        extractor_name="llm",
                        final_keywords=[{
                            "keyword": kw.text,
                            "score": kw.score,
                            "category": kw.category,
                            "start_position": kw.start_position,
                            "end_position": kw.end_position,
                            "context": kw.context_snippet
                        } for kw in keywords],
                        extraction_time=extraction_time,
                        total_processing_time=time.time() - start_time
                    )
                    
                    return keywords
            elif self.provider == 'gemini':
                response = self._call_gemini(prompt)
                # í”„ë¡¬í”„íŠ¸/ì‘ë‹µ íŒŒì¼ ì €ì¥ ë° ë¡œê·¸ ê¸°ë¡
                log_prompt_and_response(
                    label="keyword_extraction",
                    provider=self.provider,
                    model=self.model_name,
                    prompt=prompt,
                    response=response or "",
                    logger=logger,
                    meta={
                        "file_path": str(file_path) if file_path else None,
                        "config_max_keywords": self.config.get('max_keywords') if self.config else None,
                    },
                )
                if response:
                    keywords = self._parse_llm_response(response, text, position_mapper, position_map)
                    
                    # ë””ë²„ê·¸ ë¡œê¹…: ìµœì¢… ê²°ê³¼
                    extraction_time = time.time() - start_time
                    debug_logger.log_final_results(
                        extractor_name="llm",
                        final_keywords=[{
                            "keyword": kw.text,
                            "score": kw.score,
                            "category": kw.category,
                            "start_position": kw.start_position,
                            "end_position": kw.end_position,
                            "context": kw.context_snippet
                        } for kw in keywords],
                        extraction_time=extraction_time,
                        total_processing_time=time.time() - start_time
                    )
                    
                    return keywords
            
            # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            empty_result = []
            extraction_time = time.time() - start_time
            debug_logger.log_final_results(
                extractor_name="llm",
                final_keywords=[],
                extraction_time=extraction_time,
                total_processing_time=time.time() - start_time
            )
            return empty_result
        except Exception as e:
            logger.error(f"âŒ LLM ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ì‹¤íŒ¨ ì‹œì—ë„ ë””ë²„ê·¸ ë¡œê¹…
            extraction_time = time.time() - start_time
            debug_logger.log_final_results(
                extractor_name="llm",
                final_keywords=[],
                extraction_time=extraction_time,
                total_processing_time=time.time() - start_time
            )
            return []
        finally:
            # ë””ë²„ê·¸ ì„¸ì…˜ ì €ì¥
            debug_logger.save_debug_session()
    
    
    def _create_extraction_prompt(self, text: str) -> str:
        """í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        import logging
        logger = logging.getLogger(__name__)
        
        try:
            # í…œí”Œë¦¿ ì´ë¦„ ê²°ì •
            template_name = self.prompt_config.get_template_name('keyword_extraction')
            
            # í…œí”Œë¦¿ ë³€ìˆ˜ ìƒì„± (í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œì„ 15000ìë¡œ ì¦ê°€)
            variables = self.prompt_config.get_template_variables('keyword_extraction', text[:15000])
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = get_prompt_template('keyword_extraction', template_name, **variables)
            
            logger.debug(f"ğŸ¯ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©: keyword_extraction.{template_name}")
            return prompt
            
        except Exception as e:
            logger.warning(f"âš ï¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš© ì‹¤íŒ¨, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {e}")
            
            # í´ë°±: ê¸°ì¡´ ë°©ì‹
            max_keywords = min(self.config.get('max_keywords', 20), 8)
            return f"""You are a keyword extraction system. Extract exactly {max_keywords} important keywords from the text below.

Text: {text[:15000]}

Return ONLY a JSON array with this exact format (no other text):
[{{"keyword":"word1","score":0.9,"category":"noun"}},{{"keyword":"word2","score":0.8,"category":"technology"}}]

Output:"""
    
    def _call_ollama_langchain(self, prompt: str) -> tuple[str, dict, dict]:
        """LangChainì„ ì‚¬ìš©í•˜ì—¬ Ollama APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. (JSON ë°ì´í„° í¬í•¨ ë°˜í™˜)"""
        import logging
        logger = logging.getLogger(__name__)
        debug_logger = get_debug_logger()
        
        try:
            if not self.ollama_client:
                logger.error("âŒ LangChain Ollama í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return "", {}, {}
            
            # í”„ë¡¬í”„íŠ¸ ì„¤ì •ì—ì„œ LLM íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
            llm_params = self.prompt_config.get_llm_params('keyword_extraction')
            
            # ìš”ì²­ ë°ì´í„° êµ¬ì„± (LangChainì—ì„œ ì‚¬ìš©ë  ë°ì´í„°)
            request_data = {
                "model": self.model_name,
                "prompt": prompt,
                "base_url": self.base_url,
                "options": llm_params,
                "langchain_version": True
            }
            
            logger.info(f"ğŸš€ LangChain Ollama í˜¸ì¶œ ì‹œì‘ - ëª¨ë¸: {self.model_name}")
            logger.debug(f"LLM íŒŒë¼ë¯¸í„°: {llm_params}")
            
            # LangChainì„ í†µí•´ Ollama í˜¸ì¶œ
            start_time = time.time()
            result = self.ollama_client.invoke(prompt)
            call_duration = time.time() - start_time
            
            # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
            response_data = {
                "response": result,
                "model": self.model_name,
                "created_at": time.time(),
                "done": True,
                "total_duration": call_duration * 1e9,  # nanoseconds (Ollama API í˜¸í™˜)
                "load_duration": 0,
                "prompt_eval_count": len(prompt.split()),
                "eval_count": len(result.split()) if result else 0,
                "langchain_wrapper": True
            }
            
            logger.info(f"âœ… LangChain Ollama í˜¸ì¶œ ì„±ê³µ - ì‘ë‹µ ê¸¸ì´: {len(result)} ë¬¸ì, ì†Œìš”ì‹œê°„: {call_duration:.2f}ì´ˆ")
            logger.info(f"ğŸ” JSON ë°ì´í„° ìƒì„± í™•ì¸ - request_data: {bool(request_data)}, response_data: {bool(response_data)}")
            
            # ë””ë²„ê·¸ ë¡œê¹…: LLM ì‘ë‹µ ë¶„ì„
            debug_logger.log_algorithm_application(
                extractor_name="llm",
                algorithm=f"{self.provider}_langchain_generation",
                input_candidates=[("prompt", len(prompt))],  # í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ê°„ì£¼
                output_keywords=[("response", len(result))],  # ì‘ë‹µì„ ì¶œë ¥ìœ¼ë¡œ ê°„ì£¼
                algorithm_params={
                    "provider": self.provider,
                    "model": self.model_name,
                    "langchain_version": True,
                    "call_duration": call_duration,
                    **llm_params
                }
            )
            
            return result.strip(), request_data, response_data
                
        except Exception as e:
            logger.error(f"âŒ LangChain Ollama í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ì¡´ requests ë°©ì‹ìœ¼ë¡œ ì‹œë„
            logger.info("ê¸°ì¡´ requests ë°©ì‹ìœ¼ë¡œ í´ë°± ì‹œë„...")
            return self._call_ollama_fallback(prompt)
    
    def _call_ollama_fallback(self, prompt: str) -> tuple[str, dict, dict]:
        """ê¸°ì¡´ requests ë°©ì‹ì˜ Ollama API í˜¸ì¶œ (í´ë°±ìš©). JSON ë°ì´í„° í¬í•¨ ë°˜í™˜."""
        import logging
        logger = logging.getLogger(__name__)
        debug_logger = get_debug_logger()
        
        try:
            # í”„ë¡¬í”„íŠ¸ ì„¤ì •ì—ì„œ LLM íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
            llm_params = self.prompt_config.get_llm_params('keyword_extraction')
            
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": llm_params
            }
            
            logger.info(f"ğŸš€ Ollama API í´ë°± í˜¸ì¶œ ì‹œì‘ - ëª¨ë¸: {self.model_name}")
            
            timeout = self.config.get('timeout', 30) if self.config else 30
            response = requests.post(
                f"{self.base_url}/api/generate", 
                json=payload, 
                timeout=timeout
            )
            
            if response.status_code == 200:
                response_json = response.json()
                result = response_json.get("response", "")
                logger.info(f"âœ… Ollama API í´ë°± ì„±ê³µ - ì‘ë‹µ ê¸¸ì´: {len(result)} ë¬¸ì")
                
                # ë””ë²„ê·¸ ë¡œê¹…: LLM ì‘ë‹µ ë¶„ì„
                debug_logger.log_algorithm_application(
                    extractor_name="llm",
                    algorithm=f"{self.provider}_fallback_generation",
                    input_candidates=[("prompt", len(prompt))],  # í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ê°„ì£¼
                    output_keywords=[("response", len(result))],  # ì‘ë‹µì„ ì¶œë ¥ìœ¼ë¡œ ê°„ì£¼
                    algorithm_params={
                        "provider": self.provider,
                        "model": self.model_name,
                        "fallback_mode": True,
                        "timeout": timeout,
                        **llm_params
                    }
                )
                
                return result.strip(), payload, response_json
            else:
                logger.error(f"âŒ Ollama API í´ë°± ì˜¤ë¥˜: {response.status_code} - {response.text}")
                return "", payload, {"error": response.text, "status_code": response.status_code}
                
        except requests.exceptions.Timeout:
            logger.error(f"âŒ Ollama API í´ë°± íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)")
            return "", {}, {"error": "timeout", "timeout": timeout}
        except Exception as e:
            logger.error(f"âŒ Ollama API í´ë°± í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return "", {}, {"error": str(e)}
    
    def _call_openai(self, prompt: str) -> str:
        """OpenAI Chat Completions REST í˜¸ì¶œ."""
        import os
        base_url = (self.config or {}).get('base_url') or os.environ.get('OPENAI_BASE_URL', 'https://api.openai.com/v1')
        api_key = (self.config or {}).get('api_key') or os.environ.get('OPENAI_API_KEY')
        if not api_key:
            return ""
        model = self.model_name or 'gpt-3.5-turbo'
        max_tokens = (self.config or {}).get('max_tokens', 1000)
        temperature = (self.config or {}).get('temperature', 0.2)
        url = f"{base_url}/chat/completions"
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
        try:
            r = requests.post(url, headers=headers, json=payload, timeout=(self.config or {}).get('timeout', 120))
            if r.status_code != 200:
                return ""
            data = r.json()
            return data.get('choices', [{}])[0].get('message', {}).get('content', '')
        except Exception:
            return ""

    def _call_gemini(self, prompt: str) -> str:
        """Google Generative Language (Gemini) REST í˜¸ì¶œ."""
        import os
        base_url = (self.config or {}).get('base_url') or os.environ.get('GEMINI_API_BASE', 'https://generativelanguage.googleapis.com')
        api_key = (self.config or {}).get('api_key') or os.environ.get('GEMINI_API_KEY')
        if not api_key:
            return ""
        model = self.model_name or 'models/gemini-1.5-pro'
        max_tokens = (self.config or {}).get('max_tokens', 1000)
        temperature = (self.config or {}).get('temperature', 0.2)
        url = f"{base_url}/v1beta/{model}:generateContent?key={api_key}"
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens
            }
        }
        try:
            r = requests.post(url, json=payload, timeout=(self.config or {}).get('timeout', 120))
            if r.status_code != 200:
                return ""
            data = r.json()
            candidates = data.get('candidates', [])
            if not candidates:
                return ""
            parts = candidates[0].get('content', {}).get('parts', [])
            return "\n".join(part.get('text', '') for part in parts if isinstance(part, dict))
        except Exception:
            return ""
    
    def _parse_llm_response(self, response: str, original_text: str, position_mapper: PositionMapper, position_map: Dict[str, any]) -> List[Keyword]:
        """LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ í‚¤ì›Œë“œ ëª©ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        import logging
        logger = logging.getLogger(__name__)
        debug_logger = get_debug_logger()
        
        try:
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ ì‹œë„
            json_str = self._extract_json_from_response(response)
            logger.info(f"ğŸ” LLM ì‘ë‹µ íŒŒì‹± ì‹œì‘ - JSON ì¶”ì¶œ ì™„ë£Œ: {len(json_str)} ë¬¸ì")
            
            data = json.loads(json_str)
            results = []
            
            # ë””ë²„ê·¸ ë¡œê¹…: í›„ë³´ ìƒì„± (LLMì´ ìƒì„±í•œ í‚¤ì›Œë“œë“¤)
            raw_candidates = [item.get('keyword', '') for item in data if item.get('keyword')]
            debug_logger.log_candidate_generation(
                extractor_name="llm",
                candidates=raw_candidates,
                generation_method="llm_generation",
                params={
                    "provider": self.provider,
                    "model": self.model_name,
                    "response_length": len(response),
                    "json_length": len(json_str),
                    "parsed_items": len(data)
                }
            )
            
            logger.info(f"ğŸ“ LLMì—ì„œ {len(data)}ê°œ í‚¤ì›Œë“œ í›„ë³´ ì¶”ì¶œë¨")
            
            positioned_keywords = 0
            abstract_keywords = 0
            
            for i, item in enumerate(data, 1):
                keyword = item.get('keyword', '')
                score = item.get('score', 0.0)
                category = item.get('category', 'general')
                
                # í‚¤ì›Œë“œ ìœ íš¨ì„± ê²€ì‚¬
                if not TextCleaner.is_meaningful_keyword(keyword):
                    logger.debug(f"â© ê±´ë„ˆëœ€ (ìœ íš¨í•˜ì§€ ì•Šì€ í‚¤ì›Œë“œ): '{keyword}'")
                    continue
                
                # í‚¤ì›Œë“œ ì •ê·œí™”
                normalized_keyword = TextCleaner.normalize_keyword(keyword)
                if not normalized_keyword:
                    logger.debug(f"â© ê±´ë„ˆëœ€ (ì •ê·œí™” ì‹¤íŒ¨): '{keyword}'")
                    continue
                
                # í…ìŠ¤íŠ¸ì—ì„œ ìœ„ì¹˜ ì°¾ê¸°
                positions = self._find_keyword_positions(original_text, normalized_keyword)
                
                if positions:
                    positioned_keywords += 1
                    for start_pos, end_pos in positions[:1]:
                        context = self._extract_context(original_text, start_pos, end_pos)
                        page_number, line_number, column_number = position_mapper.get_position_info(start_pos, position_map)
                        results.append(Keyword(
                            text=normalized_keyword,
                            score=score,
                            extractor=self.name,
                            category=category,
                            start_position=start_pos,
                            end_position=end_pos,
                            context_snippet=context,
                            page_number=page_number,
                            line_number=line_number,
                            column_number=column_number
                        ))
                        
                        # ê°œë³„ í‚¤ì›Œë“œ ìœ„ì¹˜ ë¡œê¹… (ìƒìœ„ 3ê°œë§Œ)
                        if len(results) <= 3:
                            logger.info(f"  ğŸ“ [{i}/{len(data)}] '{keyword}' (ì ìˆ˜: {score:.3f}) - ìœ„ì¹˜: {start_pos}-{end_pos}, ì»¨í…ìŠ¤íŠ¸: '{context[:50]}{'...' if len(context) > 50 else ''}'")
                else:
                    abstract_keywords += 1
                    results.append(Keyword(
                        text=normalized_keyword,
                        score=score,
                        extractor=self.name,
                        category=category,
                        start_position=None,
                        end_position=None,
                        context_snippet=original_text[:100] + "..." if len(original_text) > 100 else original_text,
                        page_number=None,
                        line_number=None,
                        column_number=None
                    ))
                    
                    # ì¶”ìƒ í‚¤ì›Œë“œ ë¡œê¹… (ìƒìœ„ 3ê°œë§Œ)
                    if abstract_keywords <= 3:
                        logger.info(f"  ğŸ”® [{i}/{len(data)}] '{keyword}' (ì ìˆ˜: {score:.3f}) - ì¶”ìƒ í‚¤ì›Œë“œ (í…ìŠ¤íŠ¸ì—ì„œ ì •í™•í•œ ìœ„ì¹˜ ì—†ìŒ)")
            
            # ë””ë²„ê·¸ ë¡œê¹…: ìœ„ì¹˜ ë¶„ì„ ê²°ê³¼
            keywords_with_positions = []
            for kw in results:
                kw_data = {
                    "keyword": kw.text,
                    "score": kw.score,
                    "category": kw.category,
                    "positions": []
                }
                if kw.start_position is not None:
                    kw_data["positions"].append({
                        "start": kw.start_position,
                        "end": kw.end_position,
                        "page": kw.page_number,
                        "line": kw.line_number,
                        "context": kw.context_snippet
                    })
                keywords_with_positions.append(kw_data)
            
            debug_logger.log_position_analysis(
                extractor_name="llm",
                keywords_with_positions=keywords_with_positions,
                text=original_text,
                analysis_method="simple_text_search"
            )
            
            # ìµœì¢… í‚¤ì›Œë“œ í†µê³„ ë¡œê¹…
            logger.info(f"ğŸ“‹ LLM í‚¤ì›Œë“œ ì²˜ë¦¬ ì™„ë£Œ - ì´ {len(results)}ê°œ (ìœ„ì¹˜ìˆìŒ: {positioned_keywords}, ì¶”ìƒ: {abstract_keywords})")
            
            # ìµœì¢… ê²°ê³¼ ë¡œê¹…
            if results:
                top_keywords = [f"{kw.text}({kw.category},{kw.score:.3f})" for kw in results[:5]]
                logger.info(f"âœ… LLM ì¶”ì¶œ ì™„ë£Œ - {len(results)}ê°œ í‚¤ì›Œë“œ, ìƒìœ„: {', '.join(top_keywords)}")
            else:
                logger.warning("âš ï¸ LLM ì²˜ë¦¬ í›„ ìœ íš¨í•œ í‚¤ì›Œë“œê°€ ì—†ìŒ")
            
            return results
        except json.JSONDecodeError as e:
            logger.error(f"âŒ LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            logger.error(f"ì‘ë‹µ ë‚´ìš©: {response[:200]}...")
            return []
        except Exception as e:
            logger.error(f"âŒ LLM ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_json_from_response(self, response: str) -> str:
        """ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        import re
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        response = re.sub(r'```json\s*', '', response)
        response = re.sub(r'```\s*', '', response)
        
        # ëŒ€ê´„í˜¸ë¡œ ì‹œì‘í•˜ê³  ëë‚˜ëŠ” JSON ë°°ì—´ ì°¾ê¸°
        json_pattern = r'\[\s*\{.*?\}\s*\]'
        matches = re.findall(json_pattern, response, re.DOTALL)
        if matches:
            # ê°€ì¥ ì™„ì „í•œ JSON ë°°ì—´ ì„ íƒ
            longest_match = max(matches, key=len)
            return longest_match
        
        # ë‹¨ì¼ JSON ê°ì²´ë¥¼ ë°°ì—´ë¡œ ë³€í™˜
        object_pattern = r'\{[^{}]*"keyword"[^{}]*\}'
        objects = re.findall(object_pattern, response, re.DOTALL)
        if objects:
            return '[' + ','.join(objects) + ']'
        
        # JSON ë§ˆì»¤ ì´í›„ì˜ ë‚´ìš© ì°¾ê¸°
        json_markers = ['Output:', 'JSON:', 'json:', '[', '{']
        for marker in json_markers:
            if marker in response:
                json_part = response[response.find(marker):].strip()
                if marker in ['Output:', 'JSON:', 'json:']:
                    json_part = json_part[len(marker):].strip()
                
                # ì²« ë²ˆì§¸ [ ë¶€í„° ë§ˆì§€ë§‰ ] ê¹Œì§€ ì¶”ì¶œ
                start_idx = json_part.find('[')
                if start_idx != -1:
                    end_idx = json_part.rfind(']')
                    if end_idx != -1 and end_idx > start_idx:
                        return json_part[start_idx:end_idx + 1]
        
        # ì‘ë‹µ ì „ì²´ê°€ JSONì¼ ìˆ˜ë„ ìˆìŒ
        return response.strip()
    
    def _find_keyword_positions(self, text: str, keyword: str) -> List[tuple]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        positions = []
        start = 0
        text_lower = text.lower()
        keyword_lower = keyword.lower()
        
        while True:
            pos = text_lower.find(keyword_lower, start)
            if pos == -1:
                break
            positions.append((pos, pos + len(keyword)))
            start = pos + 1
        return positions
    
    def _extract_context(self, text: str, start_pos: int, end_pos: int, context_size: int = 50) -> str:
        """í‚¤ì›Œë“œ ì£¼ë³€ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        context_start = max(0, start_pos - context_size)
        context_end = min(len(text), end_pos + context_size)
        
        context = text[context_start:context_end]
        
        # ì•ë’¤ì— ìƒëµ í‘œì‹œ ì¶”ê°€
        if context_start > 0:
            context = "..." + context
        if context_end < len(text):
            context = context + "..."
            
        return context
