"""Knowledge Graph ìƒì„± ì—”ë“œí¬ì¸íŠ¸ - ì²­í¬ ê¸°ë°˜ ë¶„ì„ í†µí•©."""
from __future__ import annotations

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List

from fastapi import APIRouter, Depends, HTTPException, Query
from pydantic import BaseModel
from sqlalchemy.orm import Session

from dependencies import get_db
from services.document_parser_service import DocumentParserService
from services.local_file_analyzer import LocalFileAnalyzer
from services.chunk_analyzer import ChunkAnalyzer
from services.image_analyzer import ImageAnalyzer

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/local-analysis", tags=["local-analysis"])


class StructureAnalysisRequest(BaseModel):
    file_path: str
    directory: Optional[str] = None
    force_reparse: bool = False
    force_reanalyze: bool = False
    force_rebuild: bool = False
    llm: Optional[Dict[str, Any]] = None

    # ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì˜µì…˜ ì¶”ê°€
    use_chunking: bool = False
    max_chunk_size: int = 50000
    extractors: List[str] = ["KeyBERT", "spaCy NER", "LLM"]
    analysis_types: List[str] = ["keywords", "summary", "structure", "knowledge_graph"]

    # ì´ë¯¸ì§€ ë¶„ì„ ì˜µì…˜ ì¶”ê°€
    analyze_images: bool = False
    extract_images: bool = True


def _ensure_absolute(path: Path) -> Path:
    return path if path.is_absolute() else (Path.cwd() / path).resolve()


def _validate_file(file_path: Path) -> None:
    if not file_path.exists():
        raise HTTPException(status_code=404, detail=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    if file_path.is_dir():
        raise HTTPException(status_code=400, detail="ë””ë ‰í† ë¦¬ê°€ ì•„ë‹Œ íŒŒì¼ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
    if file_path.stat().st_size == 0:
        raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
    supported_extensions = {".pdf", ".docx", ".doc", ".txt", ".md", ".html", ".xml", ".hwp"}
    if file_path.suffix.lower() not in supported_extensions:
        raise HTTPException(status_code=400, detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_path.suffix}")


@router.post("/knowledge-graph")
async def generate_knowledge_graph(req: StructureAnalysisRequest, db: Session = Depends(get_db)):
    file_path = _ensure_absolute(Path(req.file_path))
    _validate_file(file_path)

    directory_path = None
    if req.directory:
        directory_path = _ensure_absolute(Path(req.directory))
        directory_path.mkdir(parents=True, exist_ok=True)

    parser_service = DocumentParserService()
    analyzer = LocalFileAnalyzer(db)

    output_dir = parser_service.get_output_directory(file_path, directory_path)
    response_path = output_dir / "llm_structure_response.json"

    if not any([req.force_reparse, req.force_reanalyze, req.force_rebuild]) and response_path.exists():
        with response_path.open('r', encoding='utf-8') as f:
            return json.load(f)

    # 1. íŒŒì‹± ìˆ˜í–‰ (í•­ìƒ ìµœê³  í’ˆì§ˆ íŒŒì„œë¥¼ ì‚¬ìš©)
    try:
        if req.force_reparse or not parser_service.has_parsing_results(file_path, directory_path):
            parsing_results = parser_service.parse_document_comprehensive(
                file_path=file_path,
                force_reparse=req.force_reparse,
                directory=directory_path,
            )
        else:
            parsing_results = parser_service.load_existing_parsing_results(file_path, directory_path)
    except Exception as parsing_error:
        logger.error("âŒ ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨: {parsing_error}")

    # 2. ë¬¸ì„œ í¬ê¸° í™•ì¸ ë° ì²­í‚¹ ê²°ì •
    best_parser = parsing_results.get("summary", {}).get("best_parser")
    document_text = ""
    if best_parser and best_parser in parsing_results.get("parsing_results", {}):
        parser_dir = parser_service.get_output_directory(file_path, directory_path) / best_parser
        text_file = parser_dir / f"{best_parser}_text.txt"
        if text_file.exists():
            document_text = text_file.read_text(encoding='utf-8')

    # 3. ìŠ¤ìº” ë¬¸ì„œ ê°ì§€ ë° OCR ì²˜ë¦¬
    document_size = len(document_text)
    min_text_threshold = 500  # ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ ì„ê³„ê°’

    # ìŠ¤ìº”ëœ ë¬¸ì„œì¸ì§€ ê°ì§€ (í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì´ë¯¸ì§€ ê¸°ë°˜ ë¬¸ì„œë¡œ íŒë‹¨)
    is_scanned_document = document_size < min_text_threshold and file_path.suffix.lower() == ".pdf"

    if is_scanned_document and req.analyze_images:
        logger.info(f"ğŸ“¸ ìŠ¤ìº” ë¬¸ì„œ ê°ì§€ (í…ìŠ¤íŠ¸ {document_size}ì < {min_text_threshold}ì), OCR ì²˜ë¦¬ ì‹œì‘: {file_path}")

        try:
            image_analyzer = ImageAnalyzer(db)
            ocr_result = image_analyzer.extract_full_text_from_scanned_pdf(file_path, output_dir)

            if ocr_result.get("success") and ocr_result.get("extracted_text"):
                # OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ ê¸°ì¡´ í…ìŠ¤íŠ¸ì™€ ê²°í•©
                ocr_text = ocr_result["extracted_text"]
                document_text = f"{document_text}\n\n=== OCR ì¶”ì¶œ í…ìŠ¤íŠ¸ ===\n{ocr_text}"
                document_size = len(document_text)

                logger.info(f"âœ… OCR ì„±ê³µ: {ocr_result['pages_processed']}/{ocr_result['total_pages']}í˜ì´ì§€, {ocr_result['text_length']}ì ì¶”ì¶œ")
                logger.info(f"ğŸ“Š ì´ í…ìŠ¤íŠ¸ í¬ê¸°: {document_size:,}ì (ê¸°ì¡´ + OCR ê²°í•©)")

                # OCR ê²°ê³¼ë¥¼ íŒŒì‹± ê²°ê³¼ì— ì¶”ê°€
                if "ocr_results" not in parsing_results:
                    parsing_results["ocr_results"] = {}
                parsing_results["ocr_results"]["full_document_ocr"] = ocr_result

            else:
                logger.warning(f"âš ï¸ OCR ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ ì—†ìŒ: {ocr_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

        except Exception as ocr_error:
            logger.error(f"âŒ OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {ocr_error}", exc_info=True)
            # OCR ì‹¤íŒ¨í•´ë„ ê¸°ì¡´ í…ìŠ¤íŠ¸ë¡œ ê³„ì† ì§„í–‰

    # LLM max_tokensì™€ ë¬¸ì„œ í¬ê¸°ë¥¼ ë¹„êµí•˜ì—¬ ìë™ìœ¼ë¡œ ì²­í‚¹ ê²°ì •
    llm_max_tokens = req.llm.get("max_tokens", 4000) if req.llm else 4000  # ê¸°ë³¸ê°’ 4000

    # ëŒ€ëµì ì¸ í† í°-ë¬¸ì ë¹„ìœ¨ (í•œêµ­ì–´ ê¸°ì¤€ ì•½ 1:2, ì—¬ìœ ë¥¼ ë‘ì–´ 1:1.5 ì‚¬ìš©)
    estimated_tokens = document_size / 1.5
    should_use_chunking = req.use_chunking or estimated_tokens > (llm_max_tokens * 0.8)  # 80% ì—¬ìœ  ë‘ê¸°

    if should_use_chunking:
        logger.info(f"ğŸ§© ë¬¸ì„œ í¬ê¸° ({document_size:,}ì, ì˜ˆìƒ í† í°: {estimated_tokens:,.0f})ê°€ LLM í† í° ì œí•œ ({llm_max_tokens:,} í† í°ì˜ 80%)ì„ ì´ˆê³¼í•˜ì—¬ ì²­í¬ ê¸°ë°˜ ë¶„ì„ ëª¨ë“œë¡œ ì „í™˜: {file_path}")
        # max_chunk_sizeë¥¼ LLM í† í° ì œí•œì— ë§ì¶° ë™ì  ì„¤ì •
        req.max_chunk_size = int(llm_max_tokens * 0.8 * 1.5)  # í† í°ì„ ë¬¸ì ìˆ˜ë¡œ ë³€í™˜
        return await _generate_chunk_based_knowledge_graph(req, db, file_path, directory_path)

    # ê¸°ì¡´ ë°©ì‹: ì „ì²´ ë¬¸ì„œ ë¶„ì„
    logger.info(f"ğŸ“„ ì „ì²´ ë¬¸ì„œ ë¶„ì„ ëª¨ë“œë¡œ ì§€ì‹ ê·¸ë˜í”„ ìƒì„±: {file_path} (ë¬¸ì„œ í¬ê¸°: {document_size:,}ì, ì˜ˆìƒ í† í°: {estimated_tokens:,.0f}, LLM ì œí•œ: {llm_max_tokens:,} í† í°)")

    # 2. LLM ê¸°ë°˜ êµ¬ì¡° ë¶„ì„ (ê¸°ì¡´ ê²°ê³¼ ì¬ì‚¬ìš© ê°€ëŠ¥)
    structure_result_path = output_dir / "llm_structure_analysis.json"
    llm_overrides = req.llm.copy() if req.llm else {}
    llm_overrides.setdefault("enabled", True)

    if req.force_reanalyze or req.force_rebuild or not structure_result_path.exists():
        structure_results = analyzer.analyze_document_structure_with_llm(
            text=document_text,
            file_path=str(file_path),
            file_extension=file_path.suffix.lower(),
            overrides=llm_overrides,
        )

        structure_results["file_info"] = parsing_results["file_info"]
        structure_results["analysis_timestamp"] = datetime.now().isoformat()
        structure_results["source_parser"] = best_parser

        with structure_result_path.open('w', encoding='utf-8') as f:
            json.dump(structure_results, f, ensure_ascii=False, indent=2)
    else:
        with structure_result_path.open('r', encoding='utf-8') as f:
            structure_results = json.load(f)

    if not structure_results.get("llm_success"):
        raise HTTPException(
            status_code=502,
            detail={
                "message": structure_results.get("llm_error", "LLM êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨"),
                "raw_response": structure_results.get("llm_raw_response", ""),
                "analysis_file": str(structure_result_path),
            },
        )

    llm_analysis = structure_results.get("llm_analysis")
    if not llm_analysis or not llm_analysis.get("structureAnalysis"):
        raise HTTPException(
            status_code=502,
            detail={
                "message": "LLM êµ¬ì¡° ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤",
                "analysis_file": str(structure_result_path),
            },
        )

    # 3. ì´ë¯¸ì§€ ë¶„ì„ (PDF íŒŒì¼ì¸ ê²½ìš°, ì˜µì…˜ í™œì„±í™” ì‹œ)
    image_analysis_result = None
    if file_path.suffix.lower() == ".pdf" and (req.analyze_images or req.extract_images):
        image_analyzer = ImageAnalyzer(db)

        if req.analyze_images:
            logger.info(f"ğŸ–¼ï¸ PDF ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {file_path}")
            # LLM ì„¤ì • ì „ë‹¬
            llm_config = req.llm.copy() if req.llm else {}
            image_analysis_result = image_analyzer.analyze_document_with_images(
                file_path=file_path,
                text_content=document_text,
                output_dir=output_dir,
                llm_config=llm_config
            )
            logger.info(f"âœ… ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {image_analysis_result.get('images_count', 0)}ê°œ ì¶”ì¶œ, "
                       f"{image_analysis_result.get('successful_analyses', 0)}ê°œ ë¶„ì„ ì„±ê³µ")
        elif req.extract_images:
            # ì´ë¯¸ì§€ ì¶”ì¶œë§Œ (ë¶„ì„ ì—†ìŒ)
            logger.info(f"ğŸ“¸ PDF ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œì‘: {file_path}")
            images_info = image_analyzer.extract_images_from_pdf(
                file_path=file_path,
                output_dir=output_dir / "images"
            )
            image_analysis_result = {
                "success": True,
                "images_count": len(images_info),
                "extraction_only": True,
                "images_info": images_info
            }

    # 3. ê²°ê³¼ ì €ì¥ ë° ì‘ë‹µ êµ¬ì„±
    output_dir.mkdir(exist_ok=True)

    best_parser = parsing_results.get("summary", {}).get("best_parser")

    saved_files = []
    if structure_result_path.exists():
        saved_files.append({
            "type": "structure_analysis",
            "path": str(structure_result_path),
            "description": "LLM ê¸°ë°˜ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ê²°ê³¼",
        })

    parsing_result_path = parser_service.get_parsing_result_path(file_path, directory_path)
    if parsing_result_path.exists():
        saved_files.append({
            "type": "parsing_summary",
            "path": str(parsing_result_path),
            "description": "íŒŒì‹± ê²°ê³¼ ì¢…í•©",
        })

    if parsing_results.get("parsing_results"):
        for parser_name, parser_result in parsing_results["parsing_results"].items():
            if not parser_result.get("success"):
                continue
            parser_dir = output_dir / parser_name
            text_file = parser_dir / f"{parser_name}_text.txt"
            if text_file.exists():
                saved_files.append({
                    "type": "extracted_text",
                    "parser": parser_name,
                    "path": str(text_file),
                    "description": f"{parser_name} íŒŒì„œë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸",
                })
            metadata_file = parser_dir / f"{parser_name}_metadata.json"
            if metadata_file.exists():
                saved_files.append({
                    "type": "metadata",
                    "parser": parser_name,
                    "path": str(metadata_file),
                    "description": f"{parser_name} íŒŒì„œ ë©”íƒ€ë°ì´í„°",
                })
            structure_file = parser_dir / f"{parser_name}_structure.json"
            if structure_file.exists():
                saved_files.append({
                    "type": "parser_structure",
                    "parser": parser_name,
                    "path": str(structure_file),
                    "description": f"{parser_name} íŒŒì„œ êµ¬ì¡° ì •ë³´",
                })

    # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ íŒŒì¼ ì¶”ê°€
    if image_analysis_result and image_analysis_result.get("success"):
        # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ JSON íŒŒì¼
        image_result_file = output_dir / "image_analysis.json"
        if image_result_file.exists():
            saved_files.append({
                "type": "image_analysis",
                "path": str(image_result_file),
                "description": f"ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ({image_analysis_result.get('images_count', 0)}ê°œ ì´ë¯¸ì§€)",
            })

        # ì¶”ì¶œëœ ì´ë¯¸ì§€ íŒŒì¼ë“¤
        images_dir = output_dir / "images"
        if images_dir.exists() and any(images_dir.glob("*.png")):
            image_files = list(images_dir.glob("*.png"))
            saved_files.append({
                "type": "extracted_images",
                "path": str(images_dir),
                "description": f"ì¶”ì¶œëœ ì´ë¯¸ì§€ íŒŒì¼ë“¤ ({len(image_files)}ê°œ)",
                "count": len(image_files)
            })

    api_response = {
        "saved_files": saved_files,
        "output_directory": str(output_dir),
        "generation_timestamp": datetime.now().isoformat(),
        "source_parser": best_parser,
        "image_analysis": image_analysis_result if image_analysis_result else None,
        "statistics": {
            "total_saved_files": len(saved_files),
            "file_types": {},
            "images_count": image_analysis_result.get("images_count", 0) if image_analysis_result else 0,
            "successful_image_analyses": image_analysis_result.get("successful_analyses", 0) if image_analysis_result else 0,
        },
    }

    for file_info in saved_files:
        file_type = file_info.get("type", "unknown")
        api_response["statistics"]["file_types"][file_type] = api_response["statistics"]["file_types"].get(file_type, 0) + 1

    with response_path.open('w', encoding='utf-8') as f:
        json.dump(api_response, f, ensure_ascii=False, indent=2)

    return api_response


async def _generate_chunk_based_knowledge_graph(
    req: StructureAnalysisRequest,
    db: Session,
    file_path: Path,
    directory_path: Optional[Path]
):
    """ì²­í¬ ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„±"""

    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if directory_path:
            output_dir = directory_path
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = file_path.parent / f"chunk_kg_{file_path.stem}_{timestamp}"

        output_dir.mkdir(parents=True, exist_ok=True)

        # ì²­í¬ ë¶„ì„ê¸° ì´ˆê¸°í™”
        chunk_analyzer = ChunkAnalyzer(db)

        # ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì‹¤í–‰
        start_time = datetime.now()

        integrated_result = chunk_analyzer.analyze_document_with_chunking(
            file_path=str(file_path),
            output_directory=str(output_dir),
            max_chunk_size=req.max_chunk_size,
            use_llm=req.llm is not None and req.llm.get("enabled", True),
            extractors=req.extractors,
            analysis_types=req.analysis_types
        )

        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()

        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        chunk_files = []

        # ì£¼ìš” ê²°ê³¼ íŒŒì¼ë“¤
        main_files = [
            ("integrated_analysis_result.json", "í†µí•© ë¶„ì„ ê²°ê³¼"),
            ("chunks_detailed_results.json", "ì²­í¬ë³„ ìƒì„¸ ê²°ê³¼"),
            ("chunks_info.json", "ì²­í¬ ì •ë³´"),
            ("document_structure.json", "ë¬¸ì„œ êµ¬ì¡°")
        ]

        for filename, description in main_files:
            file_path_obj = output_dir / filename
            if file_path_obj.exists():
                chunk_files.append({
                    "type": "analysis_result",
                    "path": str(file_path_obj),
                    "description": description
                })

        # ì²­í¬ë³„ íŒŒì¼ë“¤
        chunks_text_dir = output_dir / "chunks_text"
        if chunks_text_dir.exists():
            for chunk_file in chunks_text_dir.glob("*.txt"):
                chunk_files.append({
                    "type": "chunk_text",
                    "path": str(chunk_file),
                    "description": f"ì²­í¬ í…ìŠ¤íŠ¸: {chunk_file.stem}"
                })

        # í”„ë¡¬í”„íŠ¸ íŒŒì¼ë“¤
        prompts_dir = output_dir / "chunk_prompts"
        if prompts_dir.exists():
            prompt_count = len(list(prompts_dir.glob("*.txt")))
            chunk_files.append({
                "type": "prompts_directory",
                "path": str(prompts_dir),
                "description": f"ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ({prompt_count}ê°œ)"
            })

        # ì‹¤í–‰ ê²°ê³¼ íŒŒì¼ë“¤
        results_dir = output_dir / "chunk_results"
        if results_dir.exists():
            result_count = len(list(results_dir.glob("*")))
            chunk_files.append({
                "type": "results_directory",
                "path": str(results_dir),
                "description": f"ì‹¤í–‰ ê²°ê³¼ ({result_count}ê°œ)"
            })

        response_data = {
            "analysis_method": "chunk_based_knowledge_graph",
            "success": True,
            "chunks_analyzed": integrated_result.total_chunks,
            "total_content_length": integrated_result.total_content_length,
            "processing_time_seconds": processing_time,
            "integrated_keywords_count": len(integrated_result.integrated_keywords),
            "saved_files": chunk_files,
            "output_directory": str(output_dir),
            "generation_timestamp": end_time.isoformat(),
            "chunk_summary": [
                {
                    "chunk_id": chunk_result.chunk_id,
                    "level": chunk_result.level,
                    "content_length": chunk_result.content_length,
                    "keywords_count": len(chunk_result.keywords),
                    "has_knowledge_graph": bool(chunk_result.knowledge_graph)
                }
                for chunk_result in integrated_result.chunk_results
            ],
            "statistics": {
                "total_saved_files": len(chunk_files),
                "file_types": {},
            }
        }

        # íŒŒì¼ íƒ€ì…ë³„ í†µê³„
        for file_info in chunk_files:
            file_type = file_info.get("type", "unknown")
            response_data["statistics"]["file_types"][file_type] = response_data["statistics"]["file_types"].get(file_type, 0) + 1

        # ì‘ë‹µ ì €ì¥
        response_path = output_dir / "chunk_knowledge_graph_response.json"
        with response_path.open('w', encoding='utf-8') as f:
            json.dump(response_data, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… ì²­í¬ ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ)")

        return response_data

    except Exception as e:
        logger.error(f"âŒ ì²­í¬ ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")


@router.get("/knowledge-graph")
async def get_knowledge_graph(
    file_path: str,
    directory: Optional[str] = None,
    force_reparse: bool = False,
    force_reanalyze: bool = False,
    force_rebuild: bool = False,
    use_chunking: bool = Query(False, description="ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì‚¬ìš© ì—¬ë¶€"),
    max_chunk_size: int = Query(50000, description="ìµœëŒ€ ì²­í¬ í¬ê¸°"),
    extractors: str = Query("KeyBERT,spaCy NER,LLM", description="ì¶”ì¶œê¸° ëª©ë¡ (ì½¤ë§ˆ êµ¬ë¶„)"),
    analysis_types: str = Query("keywords,summary,structure,knowledge_graph", description="ë¶„ì„ ìœ í˜• (ì½¤ë§ˆ êµ¬ë¶„)"),
    analyze_images: bool = Query(False, description="ì´ë¯¸ì§€ ë¶„ì„ í™œì„±í™” (ë©€í‹°ëª¨ë‹¬ LLM ì‚¬ìš©)"),
    extract_images: bool = Query(True, description="ì´ë¯¸ì§€ ì¶”ì¶œ í™œì„±í™”"),
    db: Session = Depends(get_db),
):
    """GET ë°©ì‹ìœ¼ë¡œ ì§€ì‹ ê·¸ë˜í”„ ìƒì„± (ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì§€ì›)"""

    # ë¬¸ìì—´ íŒŒë¼ë¯¸í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    extractors_list = [ext.strip() for ext in extractors.split(",")]
    analysis_types_list = [analysis_type.strip() for analysis_type in analysis_types.split(",")]

    request = StructureAnalysisRequest(
        file_path=file_path,
        directory=directory,
        force_reparse=force_reparse,
        force_reanalyze=force_reanalyze,
        force_rebuild=force_rebuild,
        use_chunking=use_chunking,
        max_chunk_size=max_chunk_size,
        extractors=extractors_list,
        analysis_types=analysis_types_list,
        analyze_images=analyze_images,
        extract_images=extract_images
    )
    return await generate_knowledge_graph(request, db)
