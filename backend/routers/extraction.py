import logging
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List, Optional
from pydantic import BaseModel
from dependencies import get_db
from db.models import Project, File, KeywordOccurrence
from extractors.keybert_extractor import KeyBERTExtractor
from extractors.spacy_ner_extractor import SpaCyNERExtractor
from extractors.llm_extractor import LLMExtractor
from extractors.konlpy_extractor import KoNLPyExtractor
from extractors.langextract_extractor import LangExtractExtractor
from extractors.metadata_extractor import MetadataExtractor
from services.config_service import ConfigService
from services.statistics_cache_service import StatisticsCacheService

logger = logging.getLogger(__name__)

router = APIRouter(tags=["extraction"])

class ExtractionRequest(BaseModel):
    methods: List[str]  # ["keybert", "spacy_ner", "llm", "konlpy", "langextract", "metadata"]
    
class KeywordOccurrenceResponse(BaseModel):
    keyword: str
    extractor_name: str
    score: float
    category: Optional[str] = None
    start_position: Optional[int] = None
    end_position: Optional[int] = None
    context_snippet: Optional[str] = None
    page_number: Optional[int] = None
    line_number: Optional[int] = None
    column_number: Optional[int] = None

class ExtractionResponse(BaseModel):
    project_id: Optional[int] = None
    file_id: Optional[int] = None
    keywords: List[KeywordOccurrenceResponse]
    extractors_used: List[str]
    total_keywords: int

class ExtractorManager:
    """ì¶”ì¶œê¸° ê´€ë¦¬ í´ë˜ìŠ¤ (ì‹±ê¸€í†¤)"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls, db_session: Session = None):
        if cls._instance is None:
            cls._instance = super(ExtractorManager, cls).__new__(cls)
        return cls._instance
    
    def __init__(self, db_session: Session = None):
        if not ExtractorManager._initialized:
            self.db_session = db_session
            self.extractors = {}
            self._available_extractors_cache = None
            self._cache_timestamp = 0
            self._cache_duration = 300  # 5ë¶„ ìºì‹œ
            self._initialization_logged = False
            self._initialize_extractors()
            ExtractorManager._initialized = True
            logger.info("ğŸ­ ExtractorManager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            logger.debug("ğŸ”„ ExtractorManager ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©")
    
    def _initialize_extractors(self):
        """ì„¤ì •ì— ë”°ë¼ ì¶”ì¶œê¸°ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        logger.info("ğŸš€ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì‹œì‘...")
        extractor_config = ConfigService.get_extractor_config(self.db_session)
        
        # KeyBERT ì¶”ì¶œê¸°
        if extractor_config.get("keybert_enabled", True):
            logger.info("ğŸ“¦ KeyBERT ì¶”ì¶œê¸° ë“±ë¡ ì¤‘...")
            self.extractors["keybert"] = KeyBERTExtractor({
                "model": extractor_config.get("keybert_model", "all-MiniLM-L6-v2"),
                "max_keywords": extractor_config.get("keybert_max_keywords", 10),
                "use_mmr": extractor_config.get("keybert_use_mmr", True),
                "use_maxsum": extractor_config.get("keybert_use_maxsum", False),
                "diversity": extractor_config.get("keybert_diversity", 0.5),
                "keyphrase_ngram_range": extractor_config.get("keybert_keyphrase_ngram_range", [1, 2]),
                "stop_words": extractor_config.get("keybert_stop_words", "english")
            }, db_session=self.db_session)
            logger.info("âœ… KeyBERT ì¶”ì¶œê¸° ë“±ë¡ ì™„ë£Œ")
        
        # spaCy NER ì¶”ì¶œê¸°
        if extractor_config.get("ner_enabled", True):
            logger.info("ğŸ“¦ spaCy NER ì¶”ì¶œê¸° ë“±ë¡ ì¤‘...")
            self.extractors["spacy_ner"] = SpaCyNERExtractor({
                "model": extractor_config.get("ner_model", "ko_core_news_sm"),
                "auto_download": extractor_config.get("ner_auto_download", True),
                "max_keywords": extractor_config.get("max_keywords", 20)
            }, db_session=self.db_session)
            logger.info("âœ… spaCy NER ì¶”ì¶œê¸° ë“±ë¡ ì™„ë£Œ")
        
        # LLM ì¶”ì¶œê¸°
        llm_enabled = extractor_config.get("llm_enabled", False)
        if llm_enabled:
            logger.info("ğŸ“¦ LLM ì¶”ì¶œê¸° ë“±ë¡ ì¤‘...")
            ollama_config = ConfigService.get_ollama_config(self.db_session)
            llm_extractor = LLMExtractor({
                "provider": extractor_config.get("llm_provider", "ollama"),
                "model": ollama_config.get("model", "llama3.2"),
                "base_url": ollama_config.get("base_url", "http://localhost:11434"),
                "max_keywords": extractor_config.get("max_keywords", 20)
            }, db_session=self.db_session)
            self.extractors["llm"] = llm_extractor
            logger.info("âœ… LLM ì¶”ì¶œê¸° ë“±ë¡ ì™„ë£Œ")
        
        # KoNLPy ì¶”ì¶œê¸°
        if extractor_config.get("konlpy_enabled", False):
            logger.info("ğŸ“¦ KoNLPy ì¶”ì¶œê¸° ë“±ë¡ ì¤‘...")
            self.extractors["konlpy"] = KoNLPyExtractor({
                "tagger": extractor_config.get("konlpy_tagger", "Okt"),
                "min_length": extractor_config.get("konlpy_min_length", 2),
                "min_frequency": extractor_config.get("konlpy_min_frequency", 1),
                "max_keywords": extractor_config.get("konlpy_max_keywords", 15)
            }, db_session=self.db_session)
            logger.info("âœ… KoNLPy ì¶”ì¶œê¸° ë“±ë¡ ì™„ë£Œ")
        
        # LangExtract ì¶”ì¶œê¸° (API í˜¸í™˜ì„± ë¬¸ì œë¡œ ê¸°ë³¸ ë¹„í™œì„±í™”)
        if extractor_config.get("langextract_enabled", False):
            logger.info("ğŸ“¦ LangExtract ì¶”ì¶œê¸° ë“±ë¡ ì¤‘...")
            ollama_config = ConfigService.get_ollama_config(self.db_session)
            self.extractors["langextract"] = LangExtractExtractor({
                "ollama_base_url": ollama_config.get("base_url", "http://localhost:11434"),
                "ollama_model": ollama_config.get("model", "llama3.2"),
                "ollama_timeout": ollama_config.get("timeout", 30),
                "max_keywords": extractor_config.get("langextract_max_keywords", 15),
                "chunk_size": extractor_config.get("langextract_chunk_size", 2000),
                "overlap": extractor_config.get("langextract_overlap", 200),
                "confidence_threshold": extractor_config.get("langextract_confidence_threshold", 0.6)
            }, db_session=self.db_session)
            logger.info("âœ… LangExtract ì¶”ì¶œê¸° ë“±ë¡ ì™„ë£Œ")
        
        # Metadata ì¶”ì¶œê¸°
        if extractor_config.get("metadata_enabled", True):
            logger.info("ğŸ“¦ Metadata ì¶”ì¶œê¸° ë“±ë¡ ì¤‘...")
            self.extractors["metadata"] = MetadataExtractor({
                "extract_structure": extractor_config.get("metadata_extract_structure", True),
                "extract_statistics": extractor_config.get("metadata_extract_statistics", True),
                "extract_content": extractor_config.get("metadata_extract_content", True),
                "extract_file_info": extractor_config.get("metadata_extract_file_info", True),
                "extract_summary": extractor_config.get("metadata_extract_summary", True),
                "include_filename": extractor_config.get("metadata_include_filename", True),
                "min_heading_length": extractor_config.get("metadata_min_heading_length", 2),
                "max_metadata_keywords": extractor_config.get("metadata_max_metadata_keywords", 20),
                # LLM ì„¤ì • ì¶”ê°€
                "llm_enabled": extractor_config.get("llm_enabled", False),
                "llm_summary": extractor_config.get("metadata_llm_summary", True),
                "ollama_base_url": ollama_config.get("base_url", "http://localhost:11434"),
                "ollama_model": ollama_config.get("model", "gemma3n:latest"),
                "ollama_timeout": ollama_config.get("timeout", 30)
            }, db_session=self.db_session)
            logger.info("âœ… Metadata ì¶”ì¶œê¸° ë“±ë¡ ì™„ë£Œ")
        
        logger.info(f"ğŸ‰ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ - ë“±ë¡ëœ ì¶”ì¶œê¸°: {list(self.extractors.keys())}")
    
    def get_available_extractors(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì¶”ì¶œê¸° ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤ (ìºì‹± ì ìš©)."""
        import time
        
        current_time = time.time()
        
        # ìºì‹œê°€ ìœ íš¨í•œ ê²½ìš° ìºì‹œëœ ê²°ê³¼ ë°˜í™˜
        if (self._available_extractors_cache is not None and 
            current_time - self._cache_timestamp < self._cache_duration):
            logger.debug(f"ğŸ”„ ìºì‹œëœ ì¶”ì¶œê¸° ëª©ë¡ ë°˜í™˜: {self._available_extractors_cache}")
            return self._available_extractors_cache
        
        # ìºì‹œê°€ ì—†ê±°ë‚˜ ë§Œë£Œëœ ê²½ìš° ë‹¤ì‹œ ê³„ì‚°
        available = []
        
        # ì²« ë²ˆì§¸ í™•ì¸ì´ê±°ë‚˜ ìºì‹œ ë§Œë£Œ ì‹œì—ë§Œ ìƒì„¸ ë¡œê¹…
        should_log_details = (self._available_extractors_cache is None or 
                            not self._initialization_logged)
        
        if should_log_details:
            logger.info(f"ğŸ” ì¶”ì¶œê¸° ê°€ìš©ì„± í™•ì¸ - ë“±ë¡ëœ ì¶”ì¶œê¸°: {list(self.extractors.keys())}")
        else:
            logger.debug(f"ğŸ” ì¶”ì¶œê¸° ê°€ìš©ì„± ì¬í™•ì¸ - ë“±ë¡ëœ ì¶”ì¶œê¸°: {list(self.extractors.keys())}")
        
        for name, extractor in self.extractors.items():
            # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ë¡œë“œ ì‹œë„
            if not extractor.is_loaded:
                if should_log_details:
                    logger.info(f"ğŸ”„ ì¶”ì¶œê¸° '{name}' ëª¨ë¸ ë¡œë“œ ì‹œë„...")
                load_success = extractor.load_model()
                if should_log_details:
                    logger.info(f"ğŸ“Š ì¶”ì¶œê¸° '{name}': load_model={load_success}, is_available={extractor.is_available()}")
            else:
                logger.debug(f"âœ… ì¶”ì¶œê¸° '{name}': ì´ë¯¸ ë¡œë“œë¨, is_available={extractor.is_available()}")
            
            if extractor.is_available():
                available.append(name)
        
        # ê²°ê³¼ ìºì‹±
        self._available_extractors_cache = available
        self._cache_timestamp = current_time
        self._initialization_logged = True
        
        if should_log_details:
            logger.info(f"âœ… ëª¨ë“  ì¶”ì¶œê¸° ìœ íš¨ì„± í™•ì¸ ì™„ë£Œ")
            logger.info(f"ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ ì¶”ì¶œê¸°: {available}")
        else:
            logger.debug(f"âœ… ì¶”ì¶œê¸° ì¬í™•ì¸ ì™„ë£Œ - ì‚¬ìš© ê°€ëŠ¥: {available}")
        
        return available
    
    def invalidate_cache(self):
        """ì¶”ì¶œê¸° ìºì‹œë¥¼ ë¬´íš¨í™”í•©ë‹ˆë‹¤."""
        self._available_extractors_cache = None
        self._cache_timestamp = 0
        self._initialization_logged = False
        logger.debug("ğŸ—‘ï¸ ì¶”ì¶œê¸° ìºì‹œ ë¬´íš¨í™”ë¨")
    
    def extract_keywords(self, text: str, methods: List[str], filename: str = None, file_path: str = None) -> List[KeywordOccurrenceResponse]:
        """ì§€ì •ëœ ë°©ë²•ë“¤ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        all_keywords = []
        
        logger.info(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘ - íŒŒì¼: {filename or 'ì•Œ ìˆ˜ ì—†ìŒ'}, ì¶”ì¶œê¸°: {methods}, í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        for method in methods:
            if method in self.extractors:
                extractor = self.extractors[method]
                try:
                    logger.info(f"  '{method}' ì¶”ì¶œê¸°ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
                    from pathlib import Path
                    file_path_obj = Path(file_path) if file_path else None
                    keywords = extractor.extract(text, file_path_obj)
                    
                    method_keywords = []
                    for keyword in keywords:
                        kw_response = KeywordOccurrenceResponse(
                            keyword=keyword.text,
                            extractor_name=keyword.extractor,
                            score=keyword.score,
                            category=keyword.category,
                            start_position=keyword.start_position,
                            end_position=keyword.end_position,
                            context_snippet=keyword.context_snippet,
                            page_number=keyword.page_number,
                            line_number=keyword.line_number,
                            column_number=keyword.column_number
                        )
                        all_keywords.append(kw_response)
                        method_keywords.append(kw_response)
                    
                    # ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¡œê·¸ (ìƒìœ„ 5ê°œ)
                    top_keywords = sorted(method_keywords, key=lambda x: x.score, reverse=True)[:5]
                    keyword_texts = [f"{kw.keyword}({kw.score:.3f})" for kw in top_keywords]
                    logger.info(f"  âœ“ '{method}': {len(method_keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ - ìƒìœ„: {', '.join(keyword_texts)}")
                    
                except Exception as e:
                    logger.error(f"  âœ— '{method}' ì¶”ì¶œê¸° ì˜¤ë¥˜: {str(e)}")
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        all_keywords.sort(key=lambda x: x.score, reverse=True)
        logger.info(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ - ì´ {len(all_keywords)}ê°œ í‚¤ì›Œë“œ")
        return all_keywords

@router.post("/projects/{project_id}/extract_keywords", response_model=ExtractionResponse)
def extract_keywords_from_project(
    project_id: int,
    request: ExtractionRequest,
    db: Session = Depends(get_db)
):
    """í”„ë¡œì íŠ¸ì˜ ëª¨ë“  íŒŒì¼ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    
    logger.info(f"ğŸš€ í”„ë¡œì íŠ¸ {project_id} í‚¤ì›Œë“œ ì¶”ì¶œ ìš”ì²­ - ì¶”ì¶œê¸°: {request.methods}")
    
    # í”„ë¡œì íŠ¸ ì¡´ì¬ í™•ì¸
    project = db.query(Project).filter(Project.id == project_id).first()
    if not project:
        logger.error(f"âŒ í”„ë¡œì íŠ¸ {project_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        raise HTTPException(status_code=404, detail="Project not found")
    
    logger.info(f"ğŸ“ í”„ë¡œì íŠ¸ '{project.name}' ë°œê²¬")
    
    # í”„ë¡œì íŠ¸ì˜ íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°
    files = db.query(File).filter(File.project_id == project_id).all()
    if not files:
        logger.error(f"âŒ í”„ë¡œì íŠ¸ {project_id}ì— íŒŒì¼ì´ ì—†ìŒ")
        raise HTTPException(status_code=404, detail="No files found in project")
    
    logger.info(f"ğŸ“„ ì²˜ë¦¬í•  íŒŒì¼ {len(files)}ê°œ ë°œê²¬")
    
    # ì¶”ì¶œê¸° ê´€ë¦¬ì ì´ˆê¸°í™”
    extractor_manager = ExtractorManager(db)
    
    # ìš”ì²­ëœ ë°©ë²•ë“¤ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
    available_extractors = extractor_manager.get_available_extractors()
    invalid_methods = [method for method in request.methods if method not in available_extractors]
    if invalid_methods:
        logger.error(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì¶”ì¶œê¸°: {invalid_methods}, ì‚¬ìš© ê°€ëŠ¥: {available_extractors}")
        raise HTTPException(
            status_code=400, 
            detail=f"Invalid extraction methods: {invalid_methods}. Available: {available_extractors}"
        )
    
    logger.info(f"âœ… ëª¨ë“  ì¶”ì¶œê¸° ìœ íš¨ì„± í™•ì¸ ì™„ë£Œ")
    
    all_keywords = []
    processed_files = 0
    failed_files = 0
    
    # ê° íŒŒì¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    for file_index, file in enumerate(files, 1):
        try:
            logger.info(f"\nğŸ“„ [{file_index}/{len(files)}] íŒŒì¼ '{file.filename}' ì²˜ë¦¬ ì‹œì‘")
            
            # íŒŒì‹±ëœ í…ìŠ¤íŠ¸ ë‚´ìš© ì‚¬ìš© (íŒŒì‹±ì´ ì•ˆëœ ê²½ìš° ìë™ íŒŒì‹±)
            if file.content and file.parse_status == "success":
                text = file.content
                logger.info(f"  ğŸ“ íŒŒì‹±ëœ ì½˜í…ì¸  ì‚¬ìš© ({len(text)} ë¬¸ì)")
            else:
                # íŒŒì‹±ì´ ì•ˆëœ ê²½ìš° ìë™ íŒŒì‹± ì‹œë„
                logger.info(f"  ğŸ”„ íŒŒì¼ '{file.filename}' ìë™ íŒŒì‹± ì‹œì‘...")
                try:
                    from services.parser import AutoParser
                    from pathlib import Path
                    
                    parser = AutoParser()
                    file_path = Path(file.filepath)
                    
                    if not file_path.exists():
                        logger.error(f"  âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file.filepath}")
                        failed_files += 1
                        continue
                    
                    parse_result = parser.parse(file_path)
                    
                    if parse_result.success:
                        text = parse_result.text
                        logger.info(f"  âœ… ìë™ íŒŒì‹± ì„±ê³µ ({len(text)} ë¬¸ì)")
                        
                        # íŒŒì‹± ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                        file.content = text
                        file.parse_status = "success"
                        file.parse_error = None
                        db.commit()
                        db.refresh(file)
                        logger.info(f"  ğŸ’¾ íŒŒì‹± ê²°ê³¼ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
                    else:
                        logger.error(f"  âŒ íŒŒì‹± ì‹¤íŒ¨: {parse_result.error_message}")
                        text = ""
                        
                        # íŒŒì‹± ì‹¤íŒ¨ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                        file.parse_status = "failed"
                        file.parse_error = parse_result.error_message
                        db.commit()
                        
                        failed_files += 1
                        continue
                        
                except Exception as parse_error:
                    logger.error(f"  âŒ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {parse_error}")
                    
                    # íŒŒì‹± ì˜¤ë¥˜ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    file.parse_status = "failed"
                    file.parse_error = str(parse_error)
                    db.commit()
                    
                    failed_files += 1
                    continue
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = extractor_manager.extract_keywords(text, request.methods, file.filename, file.filepath)
            
            # ê¸°ì¡´ í‚¤ì›Œë“œ ì‚­ì œ (ì¬ì¶”ì¶œì„ ìœ„í•´)
            deleted_count = db.query(KeywordOccurrence).filter(
                KeywordOccurrence.file_id == file.id,
                KeywordOccurrence.extractor_name.in_(request.methods)
            ).delete()
            
            if deleted_count > 0:
                logger.info(f"  ğŸ—‘ï¸ ê¸°ì¡´ í‚¤ì›Œë“œ {deleted_count}ê°œ ì‚­ì œ")
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            for keyword_data in keywords:
                keyword_occurrence = KeywordOccurrence(
                    file_id=file.id,
                    keyword=keyword_data.keyword,
                    extractor_name=keyword_data.extractor_name,
                    score=keyword_data.score,
                    category=keyword_data.category,
                    start_position=keyword_data.start_position,
                    end_position=keyword_data.end_position,
                    context_snippet=keyword_data.context_snippet,
                    page_number=keyword_data.page_number,
                    line_number=keyword_data.line_number,
                    column_number=keyword_data.column_number
                )
                db.add(keyword_occurrence)
            
            all_keywords.extend(keywords)
            processed_files += 1
            logger.info(f"  âœ… íŒŒì¼ '{file.filename}' ì²˜ë¦¬ ì™„ë£Œ - {len(keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ")
            
        except Exception as e:
            logger.error(f"  âŒ íŒŒì¼ {file.filename} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            failed_files += 1
    
    db.commit()
    logger.info(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
    
    # í†µê³„ ìºì‹œ ë¬´íš¨í™”
    try:
        cache_service = StatisticsCacheService(db)
        cache_service.invalidate_global_cache()
        cache_service.invalidate_project_cache(project_id)
        logger.info(f"ğŸ—‘ï¸ í‚¤ì›Œë“œ í†µê³„ ìºì‹œ ë¬´íš¨í™” ì™„ë£Œ (í”„ë¡œì íŠ¸ {project_id})")
    except Exception as cache_error:
        logger.warning(f"âš ï¸ í†µê³„ ìºì‹œ ë¬´íš¨í™” ì‹¤íŒ¨: {str(cache_error)}")
    
    # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ìˆœ ì •ë ¬
    unique_keywords = {}
    for kw in all_keywords:
        key = (kw.keyword, kw.extractor_name)
        if key not in unique_keywords or unique_keywords[key].score < kw.score:
            unique_keywords[key] = kw
    
    final_keywords = list(unique_keywords.values())
    final_keywords.sort(key=lambda x: x.score, reverse=True)
    
    logger.info(f"ğŸ‰ í”„ë¡œì íŠ¸ {project_id} í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
    logger.info(f"  ğŸ“Š í†µê³„: ì²˜ë¦¬ëœ íŒŒì¼ {processed_files}ê°œ, ì‹¤íŒ¨ {failed_files}ê°œ")
    logger.info(f"  ğŸ† ìµœì¢… í‚¤ì›Œë“œ: {len(final_keywords)}ê°œ (ì¤‘ë³µ ì œê±° í›„)")
    
    # ìƒìœ„ í‚¤ì›Œë“œ ë¡œê·¸
    top_keywords = final_keywords[:10]
    if top_keywords:
        top_keyword_texts = [f"{kw.keyword}({kw.score:.3f})" for kw in top_keywords]
        logger.info(f"  ğŸ” ìƒìœ„ í‚¤ì›Œë“œ: {', '.join(top_keyword_texts)}")
    
    return ExtractionResponse(
        project_id=project_id,
        keywords=final_keywords,
        extractors_used=request.methods,
        total_keywords=len(final_keywords)
    )

@router.post("/files/{file_id}/extract_keywords", response_model=ExtractionResponse)
def extract_keywords_from_file(
    file_id: int,
    request: ExtractionRequest,
    db: Session = Depends(get_db)
):
    """íŠ¹ì • íŒŒì¼ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    
    logger.info(f"ğŸ¯ íŒŒì¼ {file_id} í‚¤ì›Œë“œ ì¶”ì¶œ ìš”ì²­ - ì¶”ì¶œê¸°: {request.methods}")
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    file = db.query(File).filter(File.id == file_id).first()
    if not file:
        logger.error(f"âŒ íŒŒì¼ {file_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        raise HTTPException(status_code=404, detail="File not found")
    
    logger.info(f"ğŸ“„ íŒŒì¼ '{file.filename}' ë°œê²¬")
    
    # ì¶”ì¶œê¸° ê´€ë¦¬ì ì´ˆê¸°í™”
    extractor_manager = ExtractorManager(db)
    
    # ìš”ì²­ëœ ë°©ë²•ë“¤ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
    available_extractors = extractor_manager.get_available_extractors()
    invalid_methods = [method for method in request.methods if method not in available_extractors]
    if invalid_methods:
        logger.error(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì¶”ì¶œê¸°: {invalid_methods}, ì‚¬ìš© ê°€ëŠ¥: {available_extractors}")
        raise HTTPException(
            status_code=400, 
            detail=f"Invalid extraction methods: {invalid_methods}. Available: {available_extractors}"
        )
    
    logger.info(f"âœ… ëª¨ë“  ì¶”ì¶œê¸° ìœ íš¨ì„± í™•ì¸ ì™„ë£Œ")
    
    try:
        # íŒŒì‹±ëœ í…ìŠ¤íŠ¸ ë‚´ìš© ì‚¬ìš© (íŒŒì‹±ì´ ì•ˆëœ ê²½ìš° ìë™ íŒŒì‹±)
        if file.content and file.parse_status == "success":
            text = file.content
            logger.info(f"ğŸ“ íŒŒì‹±ëœ ì½˜í…ì¸  ì‚¬ìš© ({len(text)} ë¬¸ì)")
        else:
            # íŒŒì‹±ì´ ì•ˆëœ ê²½ìš° ìë™ íŒŒì‹± ì‹œë„
            logger.info(f"ğŸ”„ íŒŒì¼ '{file.filename}' ìë™ íŒŒì‹± ì‹œì‘...")
            try:
                from services.parser import AutoParser
                from pathlib import Path
                
                parser = AutoParser()
                file_path = Path(file.filepath)
                
                if not file_path.exists():
                    logger.error(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file.filepath}")
                    raise HTTPException(status_code=404, detail=f"Physical file not found: {file.filepath}")
                
                parse_result = parser.parse(file_path)
                
                if parse_result.success:
                    text = parse_result.text
                    logger.info(f"âœ… ìë™ íŒŒì‹± ì„±ê³µ ({len(text)} ë¬¸ì)")
                    
                    # íŒŒì‹± ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    file.content = text
                    file.parse_status = "success"
                    file.parse_error = None
                    db.commit()
                    db.refresh(file)
                    logger.info(f"ğŸ’¾ íŒŒì‹± ê²°ê³¼ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
                else:
                    logger.error(f"âŒ íŒŒì‹± ì‹¤íŒ¨: {parse_result.error_message}")
                    
                    # íŒŒì‹± ì‹¤íŒ¨ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    file.parse_status = "failed"
                    file.parse_error = parse_result.error_message
                    db.commit()
                    
                    raise HTTPException(
                        status_code=400, 
                        detail=f"íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {parse_result.error_message}"
                    )
                    
            except HTTPException:
                raise
            except Exception as parse_error:
                logger.error(f"âŒ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {parse_error}")
                
                # íŒŒì‹± ì˜¤ë¥˜ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                file.parse_status = "failed"
                file.parse_error = str(parse_error)
                db.commit()
                
                raise HTTPException(
                    status_code=500, 
                    detail=f"íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(parse_error)}"
                )
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extractor_manager.extract_keywords(text, request.methods, file.filename)
        
        # ê¸°ì¡´ í‚¤ì›Œë“œ ì‚­ì œ (ì¬ì¶”ì¶œì˜ ê²½ìš°)
        deleted_count = db.query(KeywordOccurrence).filter(
            KeywordOccurrence.file_id == file_id,
            KeywordOccurrence.extractor_name.in_(request.methods)
        ).delete()
        
        if deleted_count > 0:
            logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ í‚¤ì›Œë“œ {deleted_count}ê°œ ì‚­ì œ")
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        for keyword_data in keywords:
            keyword_occurrence = KeywordOccurrence(
                file_id=file.id,
                keyword=keyword_data.keyword,
                extractor_name=keyword_data.extractor_name,
                score=keyword_data.score,
                category=keyword_data.category,
                start_position=keyword_data.start_position,
                end_position=keyword_data.end_position,
                context_snippet=keyword_data.context_snippet,
                page_number=keyword_data.page_number,
                line_number=keyword_data.line_number,
                column_number=keyword_data.column_number
            )
            db.add(keyword_occurrence)
        
        db.commit()
        logger.info(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
        
        # í†µê³„ ìºì‹œ ë¬´íš¨í™” (íŒŒì¼ ê¸°ë°˜ ì¶”ì¶œ)
        try:
            cache_service = StatisticsCacheService(db)
            cache_service.invalidate_global_cache()
            cache_service.invalidate_project_cache(file.project_id)
            logger.info(f"ğŸ—‘ï¸ í‚¤ì›Œë“œ í†µê³„ ìºì‹œ ë¬´íš¨í™” ì™„ë£Œ (íŒŒì¼ {file_id})")
        except Exception as cache_error:
            logger.warning(f"âš ï¸ í†µê³„ ìºì‹œ ë¬´íš¨í™” ì‹¤íŒ¨: {str(cache_error)}")
        
        logger.info(f"ğŸ‰ íŒŒì¼ '{file.filename}' í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ - {len(keywords)}ê°œ í‚¤ì›Œë“œ")
        
        # ìƒìœ„ í‚¤ì›Œë“œ ë¡œê·¸
        top_keywords = keywords[:5]
        if top_keywords:
            top_keyword_texts = [f"{kw.keyword}({kw.score:.3f})" for kw in top_keywords]
            logger.info(f"ğŸ” ìƒìœ„ í‚¤ì›Œë“œ: {', '.join(top_keyword_texts)}")
        
        return ExtractionResponse(
            file_id=file_id,
            keywords=keywords,
            extractors_used=request.methods,
            total_keywords=len(keywords)
        )
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ {file.filename} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing file: {str(e)}")

@router.get("/extractors/available")
def get_available_extractors(db: Session = Depends(get_db)):
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì¶”ì¶œê¸° ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    extractor_manager = ExtractorManager(db)
    available = extractor_manager.get_available_extractors()
    
    # ê¸°ë³¸ ì¶”ì¶œê¸° ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    default_extractors = ConfigService.get_config_value(
        db, "extractor.default_methods", ["keybert", "spacy_ner"]
    )
    
    return {
        "available_extractors": available,
        "default_extractors": default_extractors,
        "total_available": len(available)
    }

@router.get("/projects/{project_id}/keywords")
def get_project_keywords(project_id: int, db: Session = Depends(get_db)):
    """í”„ë¡œì íŠ¸ì˜ ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    
    # í”„ë¡œì íŠ¸ ì¡´ì¬ í™•ì¸
    project = db.query(Project).filter(Project.id == project_id).first()
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")
    
    # í”„ë¡œì íŠ¸ì˜ ëª¨ë“  íŒŒì¼ì˜ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
    keywords = db.query(KeywordOccurrence).join(File).filter(
        File.project_id == project_id
    ).all()
    
    # í‚¤ì›Œë“œë³„ í†µê³„
    keyword_stats = {}
    for kw in keywords:
        key = kw.keyword
        if key not in keyword_stats:
            keyword_stats[key] = {
                "keyword": kw.keyword,
                "extractors": [],
                "max_score": 0,
                "occurrences": 0,
                "categories": set()
            }
        
        keyword_stats[key]["extractors"].append(kw.extractor_name)
        keyword_stats[key]["max_score"] = max(keyword_stats[key]["max_score"], kw.score)
        keyword_stats[key]["occurrences"] += 1
        if kw.category:
            keyword_stats[key]["categories"].add(kw.category)
    
    # ê²°ê³¼ ì •ë¦¬
    result = []
    for stats in keyword_stats.values():
        stats["categories"] = list(stats["categories"])
        stats["extractors"] = list(set(stats["extractors"]))
        result.append(stats)
    
    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    result.sort(key=lambda x: x["max_score"], reverse=True)
    
    return {
        "project_id": project_id,
        "keywords": result,
        "total_unique_keywords": len(result)
    }

@router.get("/files/{file_id}/keywords")
def get_file_keywords(file_id: int, db: Session = Depends(get_db)):
    """íŒŒì¼ì˜ ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    file = db.query(File).filter(File.id == file_id).first()
    if not file:
        raise HTTPException(status_code=404, detail="File not found")
    
    # íŒŒì¼ì˜ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
    keywords = db.query(KeywordOccurrence).filter(
        KeywordOccurrence.file_id == file_id
    ).order_by(KeywordOccurrence.score.desc()).all()
    
    keyword_responses = []
    for kw in keywords:
        keyword_responses.append(KeywordOccurrenceResponse(
            keyword=kw.keyword,
            extractor_name=kw.extractor_name,
            score=kw.score,
            category=kw.category,
            start_position=kw.start_position,
            end_position=kw.end_position,
            context_snippet=kw.context_snippet,
            page_number=kw.page_number,
            line_number=kw.line_number,
            column_number=kw.column_number
        ))
    
    return {
        "file_id": file_id,
        "filename": file.filename,
        "keywords": keyword_responses,
        "total_keywords": len(keyword_responses)
    }

@router.get("/keywords/statistics")
def get_keywords_statistics(
    project_id: Optional[int] = None, 
    db: Session = Depends(get_db)
):
    """í‚¤ì›Œë“œ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. project_idê°€ ì§€ì •ë˜ë©´ í•´ë‹¹ í”„ë¡œì íŠ¸ë§Œ, ì—†ìœ¼ë©´ ì „ì²´ë¥¼ í”„ë¡œì íŠ¸ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    logger.info(f"ğŸ“Š í‚¤ì›Œë“œ í†µê³„ ì¡°íšŒ ìš”ì²­ - í”„ë¡œì íŠ¸ ID: {project_id or 'ì „ì²´'}")
    
    if project_id:
        # íŠ¹ì • í”„ë¡œì íŠ¸ì˜ í‚¤ì›Œë“œ í†µê³„
        project = db.query(Project).filter(Project.id == project_id).first()
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")
        
        logger.info(f"ğŸ“ í”„ë¡œì íŠ¸ '{project.name}' í‚¤ì›Œë“œ í†µê³„ ì¡°íšŒ")
        
        # í”„ë¡œì íŠ¸ì˜ í‚¤ì›Œë“œë“¤ ê°€ì ¸ì˜¤ê¸°
        keywords = db.query(KeywordOccurrence).join(File).filter(
            File.project_id == project_id
        ).all()
        
        # í‚¤ì›Œë“œë³„ í†µê³„ ê³„ì‚°
        keyword_stats = {}
        extractor_stats = {}
        category_stats = {}
        
        for kw in keywords:
            # í‚¤ì›Œë“œë³„ í†µê³„
            key = kw.keyword
            if key not in keyword_stats:
                keyword_stats[key] = {
                    "keyword": kw.keyword,
                    "extractors": set(),
                    "max_score": 0,
                    "occurrences": 0,
                    "categories": set(),
                    "files": set()
                }
            
            keyword_stats[key]["extractors"].add(kw.extractor_name)
            keyword_stats[key]["max_score"] = max(keyword_stats[key]["max_score"], kw.score)
            keyword_stats[key]["occurrences"] += 1
            keyword_stats[key]["files"].add(kw.file_id)
            if kw.category:
                keyword_stats[key]["categories"].add(kw.category)
            
            # ì¶”ì¶œê¸°ë³„ í†µê³„
            extractor = kw.extractor_name
            if extractor not in extractor_stats:
                extractor_stats[extractor] = {
                    "extractor": extractor,
                    "keywords_count": 0,
                    "unique_keywords": set(),
                    "avg_score": 0,
                    "scores": []
                }
            extractor_stats[extractor]["unique_keywords"].add(kw.keyword)
            extractor_stats[extractor]["keywords_count"] += 1
            extractor_stats[extractor]["scores"].append(kw.score)
            
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            if kw.category:
                if kw.category not in category_stats:
                    category_stats[kw.category] = {
                        "category": kw.category,
                        "keywords_count": 0,
                        "unique_keywords": set()
                    }
                category_stats[kw.category]["keywords_count"] += 1
                category_stats[kw.category]["unique_keywords"].add(kw.keyword)
        
        # í‚¤ì›Œë“œ í†µê³„ ì •ë¦¬
        keyword_result = []
        for stats in keyword_stats.values():
            stats["extractors"] = list(stats["extractors"])
            stats["categories"] = list(stats["categories"])
            stats["files_count"] = len(stats["files"])
            del stats["files"]  # íŒŒì¼ ID ëª©ë¡ì€ ì œê±°
            keyword_result.append(stats)
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        keyword_result.sort(key=lambda x: x["max_score"], reverse=True)
        
        # ì¶”ì¶œê¸° í†µê³„ ì •ë¦¬
        extractor_result = []
        for stats in extractor_stats.values():
            stats["unique_keywords_count"] = len(stats["unique_keywords"])
            stats["avg_score"] = round(sum(stats["scores"]) / len(stats["scores"]), 3) if stats["scores"] else 0
            del stats["unique_keywords"]
            del stats["scores"]
            extractor_result.append(stats)
        
        # ì¹´í…Œê³ ë¦¬ í†µê³„ ì •ë¦¬
        category_result = []
        for stats in category_stats.values():
            stats["unique_keywords_count"] = len(stats["unique_keywords"])
            del stats["unique_keywords"]
            category_result.append(stats)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì •ë ¬ (í‚¤ì›Œë“œ ìˆ˜ ê¸°ì¤€)
        category_result.sort(key=lambda x: x["keywords_count"], reverse=True)
        
        logger.info(f"âœ… í”„ë¡œì íŠ¸ '{project.name}' í†µê³„ ì™„ë£Œ - í‚¤ì›Œë“œ: {len(keyword_result)}ê°œ, ì¶”ì¶œê¸°: {len(extractor_result)}ê°œ, ì¹´í…Œê³ ë¦¬: {len(category_result)}ê°œ")
        
        return {
            "type": "single_project",
            "project": {
                "id": project.id,
                "name": project.name
            },
            "keywords": keyword_result,
            "extractors": extractor_result,
            "categories": category_result,
            "summary": {
                "total_keywords": len(keywords),
                "unique_keywords": len(keyword_result),
                "extractors_used": len(extractor_result),
                "categories_found": len(category_result)
            }
        }
    
    else:
        # ì „ì²´ í‚¤ì›Œë“œ í†µê³„ (í”„ë¡œì íŠ¸ë³„ êµ¬ë¶„)
        logger.info("ğŸŒ ì „ì²´ í”„ë¡œì íŠ¸ í‚¤ì›Œë“œ í†µê³„ ì¡°íšŒ")
        
        # ëª¨ë“  í”„ë¡œì íŠ¸ì™€ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        projects = db.query(Project).all()
        all_keywords = db.query(KeywordOccurrence).join(File).join(Project).all()
        
        # í”„ë¡œì íŠ¸ë³„ í†µê³„
        project_stats = {}
        global_keyword_stats = {}
        global_extractor_stats = {}
        global_category_stats = {}
        
        for kw in all_keywords:
            project_id = kw.file.project_id
            project_name = kw.file.project.name
            
            # í”„ë¡œì íŠ¸ë³„ í†µê³„
            if project_id not in project_stats:
                project_stats[project_id] = {
                    "project_id": project_id,
                    "project_name": project_name,
                    "keywords_count": 0,
                    "unique_keywords": set(),
                    "extractors": set(),
                    "categories": set(),
                    "files": set(),
                    "top_keywords": {},
                    "scores": []
                }
            
            project_stats[project_id]["keywords_count"] += 1
            project_stats[project_id]["unique_keywords"].add(kw.keyword)
            project_stats[project_id]["extractors"].add(kw.extractor_name)
            project_stats[project_id]["files"].add(kw.file_id)
            project_stats[project_id]["scores"].append(kw.score)
            if kw.category:
                project_stats[project_id]["categories"].add(kw.category)
            
            # í”„ë¡œì íŠ¸ë³„ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì 
            if kw.keyword not in project_stats[project_id]["top_keywords"]:
                project_stats[project_id]["top_keywords"][kw.keyword] = {
                    "score": kw.score,
                    "count": 1
                }
            else:
                project_stats[project_id]["top_keywords"][kw.keyword]["score"] = max(
                    project_stats[project_id]["top_keywords"][kw.keyword]["score"], 
                    kw.score
                )
                project_stats[project_id]["top_keywords"][kw.keyword]["count"] += 1
            
            # ì „ì—­ í‚¤ì›Œë“œ í†µê³„
            key = kw.keyword
            if key not in global_keyword_stats:
                global_keyword_stats[key] = {
                    "keyword": kw.keyword,
                    "projects": set(),
                    "extractors": set(),
                    "max_score": 0,
                    "occurrences": 0,
                    "categories": set()
                }
            
            global_keyword_stats[key]["projects"].add(project_id)
            global_keyword_stats[key]["extractors"].add(kw.extractor_name)
            global_keyword_stats[key]["max_score"] = max(global_keyword_stats[key]["max_score"], kw.score)
            global_keyword_stats[key]["occurrences"] += 1
            if kw.category:
                global_keyword_stats[key]["categories"].add(kw.category)
            
            # ì „ì—­ ì¶”ì¶œê¸° í†µê³„
            extractor = kw.extractor_name
            if extractor not in global_extractor_stats:
                global_extractor_stats[extractor] = {
                    "extractor": extractor,
                    "keywords_count": 0,
                    "unique_keywords": set(),
                    "projects": set(),
                    "scores": []
                }
            global_extractor_stats[extractor]["keywords_count"] += 1
            global_extractor_stats[extractor]["unique_keywords"].add(kw.keyword)
            global_extractor_stats[extractor]["projects"].add(project_id)
            global_extractor_stats[extractor]["scores"].append(kw.score)
            
            # ì „ì—­ ì¹´í…Œê³ ë¦¬ í†µê³„
            if kw.category:
                if kw.category not in global_category_stats:
                    global_category_stats[kw.category] = {
                        "category": kw.category,
                        "keywords_count": 0,
                        "unique_keywords": set(),
                        "projects": set()
                    }
                global_category_stats[kw.category]["keywords_count"] += 1
                global_category_stats[kw.category]["unique_keywords"].add(kw.keyword)
                global_category_stats[kw.category]["projects"].add(project_id)
        
        # í”„ë¡œì íŠ¸ë³„ í†µê³„ ì •ë¦¬
        project_result = []
        for stats in project_stats.values():
            # ìƒìœ„ í‚¤ì›Œë“œ ì„ ë³„ (ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ 5ê°œ)
            top_keywords_list = []
            for keyword, data in stats["top_keywords"].items():
                top_keywords_list.append({
                    "keyword": keyword,
                    "score": data["score"],
                    "count": data["count"]
                })
            top_keywords_list.sort(key=lambda x: x["score"], reverse=True)
            
            project_result.append({
                "project_id": stats["project_id"],
                "project_name": stats["project_name"],
                "keywords_count": stats["keywords_count"],
                "unique_keywords_count": len(stats["unique_keywords"]),
                "extractors_count": len(stats["extractors"]),
                "categories_count": len(stats["categories"]),
                "files_count": len(stats["files"]),
                "avg_score": round(sum(stats["scores"]) / len(stats["scores"]), 3) if stats["scores"] else 0,
                "extractors": list(stats["extractors"]),
                "categories": list(stats["categories"]),
                "top_keywords": top_keywords_list[:5]
            })
        
        # í”„ë¡œì íŠ¸ë¥¼ í‚¤ì›Œë“œ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        project_result.sort(key=lambda x: x["keywords_count"], reverse=True)
        
        # ì „ì—­ í‚¤ì›Œë“œ í†µê³„ ì •ë¦¬
        global_keyword_result = []
        for stats in global_keyword_stats.values():
            stats["projects_count"] = len(stats["projects"])
            stats["extractors"] = list(stats["extractors"])
            stats["categories"] = list(stats["categories"])
            del stats["projects"]  # í”„ë¡œì íŠ¸ ID ëª©ë¡ì€ ì œê±°
            global_keyword_result.append(stats)
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        global_keyword_result.sort(key=lambda x: x["max_score"], reverse=True)
        
        # ì „ì—­ ì¶”ì¶œê¸° í†µê³„ ì •ë¦¬
        global_extractor_result = []
        for stats in global_extractor_stats.values():
            stats["unique_keywords_count"] = len(stats["unique_keywords"])
            stats["projects_count"] = len(stats["projects"])
            stats["avg_score"] = round(sum(stats["scores"]) / len(stats["scores"]), 3) if stats["scores"] else 0
            del stats["unique_keywords"]
            del stats["projects"]
            del stats["scores"]
            global_extractor_result.append(stats)
        
        # ì „ì—­ ì¹´í…Œê³ ë¦¬ í†µê³„ ì •ë¦¬
        global_category_result = []
        for stats in global_category_stats.values():
            stats["unique_keywords_count"] = len(stats["unique_keywords"])
            stats["projects_count"] = len(stats["projects"])
            del stats["unique_keywords"]
            del stats["projects"]
            global_category_result.append(stats)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì •ë ¬ (í‚¤ì›Œë“œ ìˆ˜ ê¸°ì¤€)
        global_category_result.sort(key=lambda x: x["keywords_count"], reverse=True)
        
        logger.info(f"âœ… ì „ì²´ í†µê³„ ì™„ë£Œ - í”„ë¡œì íŠ¸: {len(project_result)}ê°œ, ì „ì—­ í‚¤ì›Œë“œ: {len(global_keyword_result)}ê°œ, ì¶”ì¶œê¸°: {len(global_extractor_result)}ê°œ, ì¹´í…Œê³ ë¦¬: {len(global_category_result)}ê°œ")
        
        return {
            "type": "all_projects",
            "projects": project_result,
            "global_keywords": global_keyword_result[:50],  # ìƒìœ„ 50ê°œë§Œ
            "global_extractors": global_extractor_result,
            "global_categories": global_category_result,
            "summary": {
                "total_projects": len(project_result),
                "total_keywords": len(all_keywords),
                "unique_keywords": len(global_keyword_result),
                "extractors_used": len(global_extractor_result),
                "categories_found": len(global_category_result)
            }
        }

@router.get("/keywords/list")
def get_keywords_list(
    project_id: Optional[int] = None,
    extractor: Optional[str] = None,
    category: Optional[str] = None,
    limit: Optional[int] = 100,
    offset: Optional[int] = 0,
    db: Session = Depends(get_db)
):
    """í‚¤ì›Œë“œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤. í•„í„°ë§ ì˜µì…˜ê³¼ í˜ì´ì§€ë„¤ì´ì…˜ì„ ì§€ì›í•©ë‹ˆë‹¤."""
    
    logger.info(f"ğŸ“‹ í‚¤ì›Œë“œ ëª©ë¡ ì¡°íšŒ - í”„ë¡œì íŠ¸: {project_id or 'ì „ì²´'}, ì¶”ì¶œê¸°: {extractor or 'ì „ì²´'}, ì¹´í…Œê³ ë¦¬: {category or 'ì „ì²´'}")
    
    # ê¸°ë³¸ ì¿¼ë¦¬ êµ¬ì„±
    query = db.query(KeywordOccurrence).join(File).join(Project)
    
    # í”„ë¡œì íŠ¸ í•„í„°
    if project_id:
        project = db.query(Project).filter(Project.id == project_id).first()
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")
        query = query.filter(File.project_id == project_id)
        logger.info(f"ğŸ“ í”„ë¡œì íŠ¸ '{project.name}' í•„í„° ì ìš©")
    
    # ì¶”ì¶œê¸° í•„í„°
    if extractor:
        query = query.filter(KeywordOccurrence.extractor_name == extractor)
        logger.info(f"ğŸ”§ ì¶”ì¶œê¸° '{extractor}' í•„í„° ì ìš©")
    
    # ì¹´í…Œê³ ë¦¬ í•„í„°
    if category:
        query = query.filter(KeywordOccurrence.category == category)
        logger.info(f"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬ '{category}' í•„í„° ì ìš©")
    
    # ì „ì²´ ê°œìˆ˜ ì¡°íšŒ
    total_count = query.count()
    
    # í˜ì´ì§€ë„¤ì´ì…˜ ì ìš© ë° ì ìˆ˜ ìˆœ ì •ë ¬
    keywords = query.order_by(KeywordOccurrence.score.desc()).offset(offset).limit(limit).all()
    
    # ê²°ê³¼ ì •ë¦¬
    keyword_list = []
    for kw in keywords:
        keyword_list.append({
            "keyword": kw.keyword,
            "score": kw.score,
            "extractor_name": kw.extractor_name,
            "category": kw.category,
            "start_position": kw.start_position,
            "end_position": kw.end_position,
            "context_snippet": kw.context_snippet,
            "file": {
                "id": kw.file.id,
                "filename": kw.file.filename,
                "project": {
                    "id": kw.file.project.id,
                    "name": kw.file.project.name
                }
            }
        })
    
    logger.info(f"âœ… í‚¤ì›Œë“œ ëª©ë¡ ì¡°íšŒ ì™„ë£Œ - {len(keyword_list)}ê°œ ë°˜í™˜ (ì „ì²´: {total_count}ê°œ)")
    
    return {
        "keywords": keyword_list,
        "pagination": {
            "total": total_count,
            "limit": limit,
            "offset": offset,
            "has_next": offset + limit < total_count,
            "has_prev": offset > 0
        },
        "filters": {
            "project_id": project_id,
            "extractor": extractor,
            "category": category
        }
    }

@router.get("/llm/test_connection")
def test_llm_connection(db: Session = Depends(get_db)):
    """LLM ì„œë²„ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        # ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        ollama_config = ConfigService.get_ollama_config(db)
        extractor_config = ConfigService.get_extractor_config(db)
        
        # LLM ì¶”ì¶œê¸° ì´ˆê¸°í™”
        llm_extractor = LLMExtractor({
            "provider": extractor_config.get("llm_provider", "ollama"),
            "model": ollama_config.get("model", "llama3.2"),
            "base_url": ollama_config.get("base_url", "http://localhost:11434"),
            "timeout": ollama_config.get("timeout", 30)
        }, db_session=db)
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        connection_success = llm_extractor.load_model()
        
        if connection_success:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            test_text = "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì€ í˜„ëŒ€ ê¸°ìˆ ì˜ í•µì‹¬ì…ë‹ˆë‹¤."
            keywords = llm_extractor.extract(test_text)
            
            return {
                "status": "success",
                "message": "LLM ì„œë²„ ì—°ê²° ì„±ê³µ",
                "provider": llm_extractor.provider,
                "model": llm_extractor.model_name,
                "base_url": llm_extractor.base_url,
                "test_keywords_count": len(keywords),
                "test_keywords": [kw.text for kw in keywords[:5]]  # ì²˜ìŒ 5ê°œë§Œ
            }
        else:
            return {
                "status": "error",
                "message": "LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨",
                "provider": llm_extractor.provider,
                "model": llm_extractor.model_name,
                "base_url": llm_extractor.base_url
            }
            
    except Exception as e:
        return {
            "status": "error",
            "message": f"LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "provider": "ollama",
            "model": "llama3.2",
            "base_url": "http://localhost:11434"
        }

@router.get("/llm/ollama/models")
def get_ollama_models(db: Session = Depends(get_db)):
    """Ollama ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        # ì„¤ì •ì—ì„œ Ollama ì„œë²„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        ollama_config = ConfigService.get_ollama_config(db)
        base_url = ollama_config.get("base_url", "http://localhost:11434")
        
        # Ollama APIë¥¼ í†µí•´ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        import requests
        response = requests.get(f"{base_url}/api/tags", timeout=5)
        
        if response.status_code == 200:
            models_data = response.json().get("models", [])
            
            # ëª¨ë¸ ì •ë³´ ì •ë¦¬
            models = []
            for model in models_data:
                model_name = model.get("name", "")
                size = model.get("size", 0)
                modified_at = model.get("modified_at", "")
                
                models.append({
                    "name": model_name,
                    "display_name": model_name.split(":")[0],  # íƒœê·¸ ì œê±°
                    "size": size,
                    "size_gb": round(size / (1024**3), 2) if size else 0,
                    "modified_at": modified_at
                })
            
            return {
                "status": "success",
                "base_url": base_url,
                "models": models,
                "total_models": len(models)
            }
        else:
            return {
                "status": "error",
                "message": f"Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}",
                "base_url": base_url,
                "models": [],
                "total_models": 0
            }
            
    except requests.exceptions.ConnectionError:
        return {
            "status": "error",
            "message": "Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "base_url": ollama_config.get("base_url", "http://localhost:11434"),
            "models": [],
            "total_models": 0
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "base_url": ollama_config.get("base_url", "http://localhost:11434"),
            "models": [],
            "total_models": 0
        }