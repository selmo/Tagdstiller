"""
êµ¬ì¡° ê¸°ë°˜ ë¬¸ì„œ ì²­í‚¹ ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ ë¬¸ì„œì˜ ë…¼ë¦¬ì  êµ¬ì¡°(Chapter, Section, Subsection)ë¥¼ ë¶„ì„í•˜ì—¬
ì§€ëŠ¥ì ìœ¼ë¡œ ì²­í¬ë¥¼ ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
"""

import re
import logging
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field
from pathlib import Path
from collections import defaultdict
import json

logger = logging.getLogger(__name__)


@dataclass
class DocumentNode:
    """ë¬¸ì„œ êµ¬ì¡° ë…¸ë“œ"""
    node_id: str
    node_type: str  # "document", "chapter", "section", "subsection"
    level: int      # 0=document, 1=chapter, 2=section, 3=subsection
    number: str     # "1", "1.1", "1.1.1"
    title: str
    content: str
    start_pos: int  # ë¬¸ì„œ ë‚´ ì‹œì‘ ìœ„ì¹˜
    end_pos: int    # ë¬¸ì„œ ë‚´ ë ìœ„ì¹˜
    children: List['DocumentNode'] = field(default_factory=list)
    parent: Optional['DocumentNode'] = None

    def get_full_path(self) -> str:
        """ì „ì²´ ê²½ë¡œ ë°˜í™˜"""
        return self.number

    def get_root_chapter_number(self) -> str:
        """ìµœìƒìœ„ Chapter ë²ˆí˜¸ ë°˜í™˜"""
        return self.number.split('.')[0]

    def get_total_content(self) -> str:
        """ìì‹ ê³¼ ëª¨ë“  í•˜ìœ„ ë…¸ë“œì˜ ë‚´ìš©ì„ í•©ì¹œ í…ìŠ¤íŠ¸ ë°˜í™˜ (ì œëª© í¬í•¨)"""
        content_parts = []

        # ì œëª©ì´ ìˆê³  ë£¨íŠ¸ê°€ ì•„ë‹ˆë©´ ì œëª©ì„ ë¨¼ì € ì¶”ê°€
        if self.title and self.node_type != "document":
            # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì œëª© ì¶”ê°€ (ë ˆë²¨ êµ¬ë¶„)
            if self.node_type == "chapter":
                content_parts.append(f"# {self.title}")      # H1: Chapter
            elif self.node_type == "section":
                content_parts.append(f"## {self.title}")     # H2: Section
            elif self.node_type == "subsection":
                content_parts.append(f"### {self.title}")    # H3: Subsection

        # ë³¸ë¬¸ ë‚´ìš© ì¶”ê°€
        if self.content.strip():
            content_parts.append(self.content)

        # ìì‹ ë…¸ë“œ ë‚´ìš© ì¶”ê°€
        for child in self.children:
            child_content = child.get_total_content()
            if child_content.strip():
                content_parts.append(child_content)

        return "\n\n".join(content_parts)

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ìš©)"""
        return {
            "node_id": self.node_id,
            "node_type": self.node_type,
            "level": self.level,
            "number": self.number,
            "title": self.title,
            "content_preview": self.content[:200] + "..." if len(self.content) > 200 else self.content,
            "content_length": len(self.content),
            "start_pos": self.start_pos,
            "end_pos": self.end_pos,
            "children": [child.to_dict() for child in self.children]
        }


@dataclass
class ChunkGroup:
    """ì²­í‚¹ ê·¸ë£¹"""
    chunk_id: str
    level: str          # "document", "chapter", "section", "subsection"
    nodes: List[DocumentNode]
    parent_context: Optional[str] = None
    boundary_rule: str = "auto"

    def get_total_content(self) -> str:
        """ê·¸ë£¹ ë‚´ ëª¨ë“  ë…¸ë“œì˜ ë‚´ìš©ì„ í•©ì¹œ í…ìŠ¤íŠ¸"""
        content_parts = []
        for node in self.nodes:
            node_content = node.get_total_content()
            if node_content.strip():
                content_parts.append(node_content)
        return "\n\n".join(content_parts)

    def get_content_length(self) -> int:
        """ì´ ë‚´ìš© ê¸¸ì´"""
        return len(self.get_total_content())

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "chunk_id": self.chunk_id,
            "level": self.level,
            "content_length": self.get_content_length(),
            "parent_context": self.parent_context,
            "boundary_rule": self.boundary_rule,
            "nodes": [node.to_dict() for node in self.nodes],
            "content_preview": self.get_total_content()[:300] + "..." if self.get_content_length() > 300 else self.get_total_content()
        }


class DocumentStructureAnalyzer:
    """ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ê¸°"""

    # êµ¬ì¡° ì¸ì‹ íŒ¨í„´
    STRUCTURE_PATTERNS = {
        "chapter": [
            r"^(\d+)\.\s+(.+?)(?:\n|$)",           # "1. ì œëª©"
            r"^ì œ\s*(\d+)\s*ì¥\s*[:\.\s]*(.+?)(?:\n|$)",  # "ì œ 1 ì¥: ì œëª©"
            r"^Chapter\s+(\d+)[\.\:\s]*(.+?)(?:\n|$)",     # "Chapter 1: ì œëª©"
            r"^CHAPTER\s+([IVX]+)[\.\:\s]*(.+?)(?:\n|$)",  # "CHAPTER I: ì œëª©"
            r"^#\s+(\d+)\s+(.+?)(?:\n|$)",         # "# 1 ì œëª©" (ë§ˆí¬ë‹¤ìš´)
            r"^#\s+(\d{2})\s+(.+?)(?:\n|$)",       # "# 01 ì œëª©" (ë§ˆí¬ë‹¤ìš´)
            r"^(\d{2})$",                          # "01" (ë‹¨ë… ìˆ«ì ë¼ì¸)
        ],

        "section": [
            r"^(\d+)\.(\d+)\s+(.+?)(?:\n|$)",      # "1.1 ì œëª©"
            r"^(\d+)-(\d+)[\.\s]+(.+?)(?:\n|$)",   # "1-1. ì œëª©"
            r"^ì œ\s*(\d+)\s*ì ˆ\s*[:\.\s]*(.+?)(?:\n|$)",  # "ì œ 1 ì ˆ: ì œëª©"
            r"^##\s+(\d+)\.(\d+)\s+(.+?)(?:\n|$)", # "## 1.1 ì œëª©" (ë§ˆí¬ë‹¤ìš´)
            r"^##\s+(\d{2})\s+(.+?)(?:\n|$)",      # "## 01 ì œëª©" (ë§ˆí¬ë‹¤ìš´)
            r"^##\s+(.+?)(?:\n|$)",                # "## ì œëª©" (ë§ˆí¬ë‹¤ìš´ ì¼ë°˜)
        ],

        "subsection": [
            r"^(\d+)\.(\d+)\.(\d+)\s+(.+?)(?:\n|$)",  # "1.1.1 ì œëª©"
            r"^(\d+)-(\d+)-(\d+)[\.\s]+(.+?)(?:\n|$)", # "1-1-1. ì œëª©"
            r"^\((\d+)\)\s+(.+?)(?:\n|$)",             # "(1) ì œëª©"
            r"^([ê°€-í£])\.\s+(.+?)(?:\n|$)",           # "ê°€. ì œëª©"
            r"^###\s+(.+?)(?:\n|$)",               # "### ì œëª©" (ë§ˆí¬ë‹¤ìš´)
        ]
    }

    def analyze_structure(self, text: str) -> DocumentNode:
        """ë¬¸ì„œ êµ¬ì¡° ë¶„ì„"""
        import time
        structure_start = time.time()
        logger.info(f"ğŸ“Š ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì‹œì‘ - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")

        # ë£¨íŠ¸ ë¬¸ì„œ ë…¸ë“œ ìƒì„±
        root = DocumentNode(
            node_id="document_root",
            node_type="document",
            level=0,
            number="0",
            title="Document Root",
            content="",
            start_pos=0,
            end_pos=len(text)
        )

        # í…ìŠ¤íŠ¸ë¥¼ ë¼ì¸ë³„ë¡œ ë¶„ì„
        lines = text.split('\n')
        current_pos = 0

        current_chapter = None
        current_section = None

        # ìë™ ë²ˆí˜¸ ì¹´ìš´í„°
        auto_chapter_counter = 1
        auto_section_counter = 1
        auto_subsection_counter = 1

        content_buffer = []

        for i, line in enumerate(lines):
            line_start_pos = current_pos
            line_end_pos = current_pos + len(line) + 1  # +1 for \n
            current_pos = line_end_pos

            # ê° êµ¬ì¡° ë ˆë²¨ í™•ì¸
            structure_match = self._match_structure_pattern(line)

            if structure_match:
                # ì´ì „ content_bufferë¥¼ í˜„ì¬ ë…¸ë“œì— ì €ì¥
                if content_buffer:
                    content_text = '\n'.join(content_buffer)
                    if current_section:
                        current_section.content += content_text + '\n'
                    elif current_chapter:
                        current_chapter.content += content_text + '\n'
                    else:
                        root.content += content_text + '\n'
                    content_buffer = []

                level, number, title = structure_match

                # ìë™ ë²ˆí˜¸ ì²˜ë¦¬
                if number == "auto":
                    if level == "chapter":
                        number = str(auto_chapter_counter)
                        auto_chapter_counter += 1
                        auto_section_counter = 1  # ìƒˆ ì±•í„° ì‹œì‘ì‹œ ì„¹ì…˜ ì¹´ìš´í„° ë¦¬ì…‹
                    elif level == "section":
                        number = str(auto_section_counter)
                        auto_section_counter += 1
                        auto_subsection_counter = 1  # ìƒˆ ì„¹ì…˜ ì‹œì‘ì‹œ ì„œë¸Œì„¹ì…˜ ì¹´ìš´í„° ë¦¬ì…‹
                    elif level == "subsection":
                        number = str(auto_subsection_counter)
                        auto_subsection_counter += 1

                if level == "chapter":
                    current_chapter = DocumentNode(
                        node_id=f"chapter_{number}",
                        node_type="chapter",
                        level=1,
                        number=number,
                        title=title,
                        content="",
                        start_pos=line_start_pos,
                        end_pos=line_end_pos,
                        parent=root
                    )
                    root.children.append(current_chapter)
                    current_section = None
                    logger.debug(f"ğŸ“– Chapter ë°œê²¬: {number} - {title}")

                elif level == "section":
                    # Sectionì€ Chapterê°€ ì—†ì–´ë„ ìƒì„± (ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œì˜ ê²½ìš°)
                    if not current_chapter:
                        # ì•”ì‹œì  Chapter ìƒì„±
                        current_chapter = DocumentNode(
                            node_id=f"chapter_auto",
                            node_type="chapter",
                            level=1,
                            number="1",
                            title="Document Sections",
                            content="",
                            start_pos=line_start_pos,
                            end_pos=line_start_pos,
                            parent=root
                        )
                        root.children.append(current_chapter)
                        logger.debug(f"ğŸ“– ì•”ì‹œì  Chapter ìƒì„±: Document Sections")

                    current_section = DocumentNode(
                        node_id=f"section_{number}",
                        node_type="section",
                        level=2,
                        number=number,
                        title=title,
                        content="",
                        start_pos=line_start_pos,
                        end_pos=line_end_pos,
                        parent=current_chapter
                    )
                    current_chapter.children.append(current_section)
                    logger.debug(f"ğŸ“ Section ë°œê²¬: {number} - {title}")

                elif level == "subsection":
                    if current_section:
                        subsection = DocumentNode(
                            node_id=f"subsection_{number}",
                            node_type="subsection",
                            level=3,
                            number=number,
                            title=title,
                            content="",
                            start_pos=line_start_pos,
                            end_pos=line_end_pos,
                            parent=current_section
                        )
                        current_section.children.append(subsection)
                        logger.debug(f"ğŸ“„ Subsection ë°œê²¬: {number} - {title}")
            else:
                # ì¼ë°˜ ë‚´ìš© ë¼ì¸
                content_buffer.append(line)

        # ë§ˆì§€ë§‰ content_buffer ì²˜ë¦¬
        if content_buffer:
            content_text = '\n'.join(content_buffer)
            if current_section:
                current_section.content += content_text
            elif current_chapter:
                current_chapter.content += content_text
            else:
                root.content += content_text

        # êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ë¡œê·¸
        structure_duration = time.time() - structure_start
        self._log_structure_summary(root)
        logger.info(f"â±ï¸ êµ¬ì¡° ë¶„ì„ ì†Œìš”ì‹œê°„: {structure_duration:.2f}ì´ˆ")

        return root

    def _match_structure_pattern(self, line: str) -> Optional[Tuple[str, str, str]]:
        """ë¼ì¸ì´ êµ¬ì¡° íŒ¨í„´ì— ë§¤ì¹˜ë˜ëŠ”ì§€ í™•ì¸"""
        line = line.strip()
        if not line:
            return None

        # Chapter íŒ¨í„´ í™•ì¸
        for i, pattern in enumerate(self.STRUCTURE_PATTERNS["chapter"]):
            match = re.match(pattern, line)
            if match:
                groups = match.groups()

                # "01" ê°™ì€ ë‹¨ë… ìˆ«ì íŒ¨í„´
                if i == 6 and len(groups) == 1:  # "(\d{2})$" íŒ¨í„´
                    number = groups[0]
                    return ("chapter", number, f"Chapter {number}")

                # ì¼ë°˜ íŒ¨í„´ (ë²ˆí˜¸, ì œëª©)
                elif len(groups) == 2:
                    number, title = groups
                    return ("chapter", number, title.strip())

        # Section íŒ¨í„´ í™•ì¸
        for i, pattern in enumerate(self.STRUCTURE_PATTERNS["section"]):
            match = re.match(pattern, line)
            if match:
                groups = match.groups()

                # "## ì œëª©" ì¼ë°˜ ë§ˆí¬ë‹¤ìš´ íŒ¨í„´
                if i == 5 and len(groups) == 1:  # "^##\s+(.+?)(?:\n|$)" íŒ¨í„´
                    title = groups[0].strip()
                    # ìë™ìœ¼ë¡œ section ë²ˆí˜¸ ìƒì„± (ì„ì‹œ)
                    return ("section", "auto", title)

                # "## 01 ì œëª©" íŒ¨í„´
                elif i == 4 and len(groups) == 2:  # "^##\s+(\d{2})\s+(.+?)(?:\n|$)" íŒ¨í„´
                    number, title = groups
                    return ("section", number, title.strip())

                # "1.1 ì œëª©" íŒ¨í„´
                elif len(groups) == 3:
                    number = f"{groups[0]}.{groups[1]}"
                    title = groups[2].strip()
                    return ("section", number, title)

                # ê¸°íƒ€ íŒ¨í„´
                elif len(groups) == 2:
                    number, title = groups
                    return ("section", number, title.strip())

        # Subsection íŒ¨í„´ í™•ì¸
        for i, pattern in enumerate(self.STRUCTURE_PATTERNS["subsection"]):
            match = re.match(pattern, line)
            if match:
                groups = match.groups()

                # "### ì œëª©" ë§ˆí¬ë‹¤ìš´ íŒ¨í„´
                if i == 4 and len(groups) == 1:  # "^###\s+(.+?)(?:\n|$)" íŒ¨í„´
                    title = groups[0].strip()
                    return ("subsection", "auto", title)

                # ë³µí•© ë²ˆí˜¸ íŒ¨í„´
                elif len(groups) == 4:
                    number = f"{groups[0]}.{groups[1]}.{groups[2]}"
                    title = groups[3].strip()
                    return ("subsection", number, title)

                # ê°„ë‹¨í•œ íŒ¨í„´
                elif len(groups) == 2:
                    number, title = groups
                    return ("subsection", number, title.strip())

        return None

    def _log_structure_summary(self, root: DocumentNode):
        """êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë¡œê·¸"""
        chapter_count = len(root.children)
        section_count = sum(len(chapter.children) for chapter in root.children)
        subsection_count = sum(
            len(section.children)
            for chapter in root.children
            for section in chapter.children
        )

        logger.info(f"ğŸ“Š êµ¬ì¡° ë¶„ì„ ì™„ë£Œ:")
        logger.info(f"  ğŸ“– Chapter: {chapter_count}ê°œ")
        logger.info(f"  ğŸ“ Section: {section_count}ê°œ")
        logger.info(f"  ğŸ“„ Subsection: {subsection_count}ê°œ")

        for chapter in root.children:
            logger.debug(f"  ğŸ“– {chapter.number}: {chapter.title} ({len(chapter.children)}ê°œ section)")


class StructuralChunker:
    """êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.analyzer = DocumentStructureAnalyzer()

    def determine_chunking_level(self, doc_size: int, structure: DocumentNode) -> str:
        """ë¬¸ì„œ í¬ê¸°ì™€ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì²­í‚¹ ë‹¨ìœ„ ê²°ì •"""
        chapter_count = len(structure.children)
        section_count = sum(len(chapter.children) for chapter in structure.children)

        logger.info(f"ğŸ“ ì²­í‚¹ ë ˆë²¨ ê²°ì • - ë¬¸ì„œí¬ê¸°: {doc_size}, Chapter: {chapter_count}, Section: {section_count}")

        # LLM ì²˜ë¦¬ë¥¼ ìœ„í•´ ì²­í¬ í¬ê¸°ë¥¼ ë” ì‘ê²Œ ì¡°ì • (10K ì´í•˜ ê¶Œì¥)
        if doc_size < 8000:     # 8K ì´í•˜: ì „ì²´ ë¬¸ì„œ
            level = "document"
        elif doc_size < 50000:  # 50K ì´í•˜: êµ¬ì¡° ê¸°ë°˜ ë¶„í• 
            if chapter_count >= 3:
                level = "chapter"
            elif section_count >= 3:  # 5 â†’ 3ìœ¼ë¡œ ë‚®ì¶¤
                level = "section"
            elif chapter_count >= 2:  # Chapterê°€ 2ê°œ ì´ìƒì´ë©´ ë¶„í• 
                level = "chapter"
            else:
                level = "document"
        else:                   # 50K ì´ìƒ: ì ê·¹ì  ë¶„í• 
            avg_chapter_size = doc_size / max(chapter_count, 1)
            if avg_chapter_size > 15000:  # 30K â†’ 15Kë¡œ ë‚®ì¶¤
                level = "section" if section_count >= chapter_count * 2 else "chapter"
            else:
                level = "chapter"

        logger.info(f"ğŸ¯ ì„ íƒëœ ì²­í‚¹ ë ˆë²¨: {level} (LLM ìµœì í™”: 8K ì´í•˜ ì„ í˜¸)")
        return level

    def create_chunks(
        self,
        structure: DocumentNode,
        chunk_level: str,
        max_chunk_tokens: int = 16000  # ê¸°ë³¸ê°’ ì¦ê°€: ì²­í¬ ìˆ˜ ê°ì†Œ
    ) -> List[ChunkGroup]:
        """êµ¬ì¡° ê¸°ë°˜ ì²­í¬ ê·¸ë£¹ ìƒì„±

        Args:
            structure: ë¬¸ì„œ êµ¬ì¡° íŠ¸ë¦¬
            chunk_level: ì²­í‚¹ ë ˆë²¨ ("document", "chapter", "section")
            max_chunk_tokens: ì²­í¬ë‹¹ ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 16000)
        """
        import time
        chunking_start = time.time()
        logger.info(f"âœ‚ï¸ ì²­í¬ ìƒì„± ì‹œì‘ - ë ˆë²¨: {chunk_level}, ìµœëŒ€ í† í°: {max_chunk_tokens}")

        chunks = []

        if chunk_level == "document":
            # ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
            chunk = ChunkGroup(
                chunk_id="document_full",
                level="document",
                nodes=[structure],
                boundary_rule="document_boundary"
            )
            chunks.append(chunk)

        elif chunk_level == "chapter":
            # Chapter ë‹¨ìœ„ë¡œ ê·¸ë£¹í•‘
            for i, chapter in enumerate(structure.children):
                chunk = ChunkGroup(
                    chunk_id=f"chapter_{chapter.number}",
                    level="chapter",
                    nodes=[chapter],
                    parent_context=f"Document: {structure.title}",
                    boundary_rule="chapter_boundary"
                )
                chunks.append(chunk)

        elif chunk_level == "section":
            # Section ë‹¨ìœ„ë¡œ ê·¸ë£¹í•‘í•˜ë˜, í† í° í¬ê¸° ê¸°ë°˜ìœ¼ë¡œ ë³‘í•©
            for chapter in structure.children:
                if chapter.children:  # Sectionì´ ìˆëŠ” ê²½ìš°
                    # í† í° í¬ê¸° ê¸°ë°˜ìœ¼ë¡œ ì„¹ì…˜ë“¤ì„ ë³‘í•©
                    merged_chunks = self._merge_sections_by_token_size(
                        chapter.children,
                        max_tokens=max_chunk_tokens,  # íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ë°›ì€ ê°’ ì‚¬ìš©
                        parent_context=f"Chapter {chapter.number}: {chapter.title}"
                    )
                    chunks.extend(merged_chunks)
                else:  # Sectionì´ ì—†ìœ¼ë©´ Chapter ì „ì²´
                    chunk = ChunkGroup(
                        chunk_id=f"chapter_{chapter.number}",
                        level="chapter",
                        nodes=[chapter],
                        parent_context=f"Document: {structure.title}",
                        boundary_rule="chapter_boundary"
                    )
                    chunks.append(chunk)

        # ì²­í¬ ê²½ê³„ ê²€ì¦
        chunks = self._validate_chunk_boundaries(chunks)

        chunking_duration = time.time() - chunking_start
        logger.info(f"âœ… ì²­í¬ ìƒì„± ì™„ë£Œ - ì´ {len(chunks)}ê°œ ì²­í¬ (ì†Œìš”ì‹œê°„: {chunking_duration:.2f}ì´ˆ)")
        for chunk in chunks:
            logger.debug(f"  ğŸ§© {chunk.chunk_id}: {chunk.get_content_length()} ë¬¸ì")

        return chunks

    def _merge_sections_by_token_size(
        self,
        sections: List[DocumentNode],
        max_tokens: int = 8000,
        parent_context: str = ""
    ) -> List[ChunkGroup]:
        """í† í° í¬ê¸° ê¸°ë°˜ìœ¼ë¡œ ì„¹ì…˜ë“¤ì„ ë³‘í•©í•˜ì—¬ ì²­í¬ ìƒì„±

        Args:
            sections: ë³‘í•©í•  ì„¹ì…˜ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸
            max_tokens: ì²­í¬ë‹¹ ìµœëŒ€ í† í° ìˆ˜
            parent_context: ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ (Chapter ì •ë³´ ë“±)

        Returns:
            ë³‘í•©ëœ ì²­í¬ ê·¸ë£¹ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        current_nodes = []
        current_tokens = 0

        for section in sections:
            # ì„¹ì…˜ì˜ í† í° ìˆ˜ ì¶”ì • (ë¬¸ì ìˆ˜ / 4)
            section_content = section.get_total_content()
            section_tokens = len(section_content) // 4

            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í–ˆì„ ë•Œ max_tokensë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
            if current_nodes and (current_tokens + section_tokens > max_tokens):
                # í˜„ì¬ ì²­í¬ ì €ì¥
                chunk_id = f"merged_{sections[0].number}_to_{current_nodes[-1].number}"
                chunk = ChunkGroup(
                    chunk_id=chunk_id,
                    level="section",
                    nodes=current_nodes.copy(),
                    parent_context=parent_context,
                    boundary_rule="token_based_merge"
                )
                chunks.append(chunk)
                logger.debug(f"  ğŸ“¦ ë³‘í•© ì²­í¬ ìƒì„±: {len(current_nodes)}ê°œ ì„¹ì…˜, ~{current_tokens} í† í°")

                # ìƒˆ ì²­í¬ ì‹œì‘
                current_nodes = [section]
                current_tokens = section_tokens
            else:
                # í˜„ì¬ ì²­í¬ì— ì¶”ê°€
                current_nodes.append(section)
                current_tokens += section_tokens

        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_nodes:
            chunk_id = f"merged_{current_nodes[0].number}_to_{current_nodes[-1].number}"
            chunk = ChunkGroup(
                chunk_id=chunk_id,
                level="section",
                nodes=current_nodes,
                parent_context=parent_context,
                boundary_rule="token_based_merge"
            )
            chunks.append(chunk)
            logger.debug(f"  ğŸ“¦ ë³‘í•© ì²­í¬ ìƒì„±: {len(current_nodes)}ê°œ ì„¹ì…˜, ~{current_tokens} í† í°")

        logger.info(f"âœ… ì„¹ì…˜ ë³‘í•© ì™„ë£Œ: {len(sections)}ê°œ ì„¹ì…˜ â†’ {len(chunks)}ê°œ ì²­í¬")
        return chunks

    def _validate_chunk_boundaries(self, chunks: List[ChunkGroup]) -> List[ChunkGroup]:
        """ì²­í¬ ê²½ê³„ ê²€ì¦ ë° ìˆ˜ì •"""
        validated_chunks = []

        for chunk in chunks:
            # í† í° ê¸°ë°˜ ë³‘í•©ëœ ì²­í¬ëŠ” ê²€ì¦ ê±´ë„ˆë›°ê¸°
            if chunk.boundary_rule == "token_based_merge":
                validated_chunks.append(chunk)
                continue

            # ê·œì¹™: ì„œë¡œ ë‹¤ë¥¸ Chapterì˜ ë‚´ìš©ì´ ê°™ì€ ì²­í¬ì— ìˆìœ¼ë©´ ì•ˆë¨
            chapter_numbers = set()
            for node in chunk.nodes:
                if node.node_type == "chapter":
                    chapter_numbers.add(node.number)
                else:
                    chapter_numbers.add(node.get_root_chapter_number())

            if len(chapter_numbers) > 1:
                logger.warning(f"âš ï¸ ì˜ëª»ëœ ì²­í¬ ê²½ê³„ ë°œê²¬: {chunk.chunk_id}")
                # ì˜ëª»ëœ ê²½ê³„ -> Chapterë³„ë¡œ ë¶„í• 
                split_chunks = self._split_by_chapter(chunk)
                validated_chunks.extend(split_chunks)
            else:
                validated_chunks.append(chunk)

        return validated_chunks

    def _split_by_chapter(self, chunk: ChunkGroup) -> List[ChunkGroup]:
        """Chapterë³„ë¡œ ì²­í¬ ë¶„í• """
        chapter_groups = defaultdict(list)

        for node in chunk.nodes:
            if node.node_type == "chapter":
                chapter_num = node.number
            else:
                chapter_num = node.get_root_chapter_number()
            chapter_groups[chapter_num].append(node)

        split_chunks = []
        for chapter_num, nodes in chapter_groups.items():
            split_chunk = ChunkGroup(
                chunk_id=f"chapter_{chapter_num}_split",
                level="chapter",
                nodes=nodes,
                boundary_rule="chapter_boundary_corrected"
            )
            split_chunks.append(split_chunk)

        return split_chunks

    def analyze_and_chunk(self, text: str) -> Tuple[DocumentNode, List[ChunkGroup]]:
        """ë¬¸ì„œ ë¶„ì„ ë° ì²­í‚¹ í†µí•© í”„ë¡œì„¸ìŠ¤"""
        logger.info(f"ğŸš€ ë¬¸ì„œ ë¶„ì„ ë° ì²­í‚¹ ì‹œì‘")

        # 1. êµ¬ì¡° ë¶„ì„
        structure = self.analyzer.analyze_structure(text)

        # 2. ì²­í‚¹ ë ˆë²¨ ê²°ì •
        chunk_level = self.determine_chunking_level(len(text), structure)

        # 3. ì²­í¬ ìƒì„±
        chunks = self.create_chunks(structure, chunk_level)

        logger.info(f"ğŸ‰ ë¶„ì„ ë° ì²­í‚¹ ì™„ë£Œ")
        return structure, chunks


def export_analysis_results(structure: DocumentNode, chunks: List[ChunkGroup],
                          output_path: Path) -> Dict[str, str]:
    """ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
    output_path.mkdir(exist_ok=True)
    exported_files = {}

    # 1. êµ¬ì¡° ë¶„ì„ ê²°ê³¼ JSON
    structure_file = output_path / "document_structure.json"
    with open(structure_file, 'w', encoding='utf-8') as f:
        json.dump(structure.to_dict(), f, ensure_ascii=False, indent=2)
    exported_files["structure"] = str(structure_file)

    # 2. ì²­í¬ ì •ë³´ JSON
    chunks_file = output_path / "chunks_info.json"
    chunks_data = {
        "total_chunks": len(chunks),
        "chunks": [chunk.to_dict() for chunk in chunks]
    }
    with open(chunks_file, 'w', encoding='utf-8') as f:
        json.dump(chunks_data, f, ensure_ascii=False, indent=2)
    exported_files["chunks"] = str(chunks_file)

    # 3. ê° ì²­í¬ë³„ í…ìŠ¤íŠ¸ íŒŒì¼
    chunks_dir = output_path / "chunks_text"
    chunks_dir.mkdir(exist_ok=True)

    for chunk in chunks:
        chunk_file = chunks_dir / f"{chunk.chunk_id}.txt"
        with open(chunk_file, 'w', encoding='utf-8') as f:
            f.write(f"# {chunk.chunk_id}\n")
            f.write(f"Level: {chunk.level}\n")
            f.write(f"Content Length: {chunk.get_content_length()}\n")
            if chunk.parent_context:
                f.write(f"Parent Context: {chunk.parent_context}\n")
            f.write(f"Boundary Rule: {chunk.boundary_rule}\n")
            f.write("=" * 50 + "\n\n")
            f.write(chunk.get_total_content())
        exported_files[chunk.chunk_id] = str(chunk_file)

    logger.info(f"ğŸ“ ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")
    return exported_files


@dataclass
class ChunkInfo:
    """ì²­í‚¹ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    total_chunks: int
    chunks: List[Dict[str, Any]]
    original_structure: Dict[str, Any]
    chunking_metadata: Dict[str, Any] = field(default_factory=dict)


class DocumentChunker:
    """ë¬¸ì„œ ì²­í‚¹ì„ ìœ„í•œ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self):
        self.chunker = StructuralChunker()
        self.logger = logging.getLogger(__name__)

    def chunk_document(
        self,
        file_path: str,
        max_chunk_size: int = 50000,
        output_directory: str = None
    ) -> ChunkInfo:
        """
        ë¬¸ì„œë¥¼ ì²­í‚¹í•©ë‹ˆë‹¤.

        Args:
            file_path: ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ
            max_chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸°
            output_directory: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬

        Returns:
            ChunkInfo: ì²­í‚¹ ì •ë³´
        """
        self.logger.info(f"ğŸ“„ ë¬¸ì„œ ì²­í‚¹ ì‹œì‘: {file_path}")

        # íŒŒì¼ ì½ê¸°
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
        except Exception as e:
            raise ValueError(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")

        # êµ¬ì¡° ë¶„ì„ ë° ì²­í‚¹
        structure, chunk_groups = self.chunker.analyze_and_chunk(text)

        # ChunkInfo í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        chunks_data = []
        for chunk_group in chunk_groups:
            chunk_dict = chunk_group.to_dict()
            chunks_data.append(chunk_dict)

        # ê²°ê³¼ ì €ì¥ (ì„ íƒì )
        if output_directory:
            output_path = Path(output_directory)
            export_analysis_results(structure, chunk_groups, output_path)

        chunk_info = ChunkInfo(
            total_chunks=len(chunks_data),
            chunks=chunks_data,
            original_structure=structure.to_dict(),
            chunking_metadata={
                'file_path': file_path,
                'original_size': len(text),
                'max_chunk_size': max_chunk_size,
                'chunking_level': chunk_groups[0].level if chunk_groups else 'unknown'
            }
        )

        self.logger.info(f"âœ… ë¬¸ì„œ ì²­í‚¹ ì™„ë£Œ: {len(chunks_data)}ê°œ ì²­í¬ ìƒì„±")
        return chunk_info