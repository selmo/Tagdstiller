"""
ë¡œì»¬ íŒŒì¼ ë¶„ì„ ì„œë¹„ìŠ¤
"""
import os
import json
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from sqlalchemy.orm import Session

from services.config_service import ConfigService
from services.parser.auto_parser import AutoParser
from utils.llm_logger import log_prompt_and_response
from services.parser_file_manager import save_parser_results, file_manager

from langchain_ollama import OllamaLLM
LANGCHAIN_AVAILABLE = True

class LocalFileAnalyzer:
    """ë¡œì»¬ íŒŒì¼ ë¶„ì„ì„ ìœ„í•œ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, db: Session, initialize_extractors: bool = True):
        self.db = db
        self.extractor_manager = None
        if initialize_extractors:
            self._ensure_extractor_manager()

    def _ensure_extractor_manager(self):
        """í•„ìš” ì‹œ ì¶”ì¶œê¸° ë§¤ë‹ˆì €ë¥¼ ì§€ì—° ë¡œë”©í•œë‹¤."""
        if self.extractor_manager is None:
            from routers.extraction import ExtractorManager  # ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì´ˆê¸°í™” í”¼í•˜ê¸°
            self.extractor_manager = ExtractorManager(self.db)
        return self.extractor_manager
        
    def get_file_root(self) -> str:
        """ì„¤ì •ì—ì„œ íŒŒì¼ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ê°€ì ¸ì˜¤ê³  ì—†ìœ¼ë©´ ìƒì„±"""
        root_path = ConfigService.get_config_value(self.db, "LOCAL_FILE_ROOT", "./data/uploads")
        root_dir = Path(root_path)
        
        # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
        if not root_dir.exists():
            try:
                root_dir.mkdir(parents=True, exist_ok=True)
                print(f"ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {root_dir.resolve()}")
            except Exception as e:
                print(f"ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
                # ìƒì„± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                return "."
        
        return root_path
    
    def get_absolute_path(self, file_path: str) -> Path:
        """ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜"""
        # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©
        # (change-directory ì—”ë“œí¬ì¸íŠ¸ê°€ os.chdir()ë¡œ ë³€ê²½í•œ ë””ë ‰í† ë¦¬)
        current_dir = Path.cwd()
        target_path = Path(file_path)
        
        if target_path.is_absolute():
            # ì ˆëŒ€ ê²½ë¡œì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            return target_path.resolve()
        else:
            # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ í•´ì„
            return (current_dir / target_path).resolve()
    
    def get_result_file_path(self, file_path: str) -> Path:
        """ë¶„ì„ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œë¥¼ ìƒì„± - parsing ê²°ê³¼ì™€ ê°™ì€ ë””ë ‰í† ë¦¬ì— ì €ì¥"""
        from services.document_parser_service import DocumentParserService
        
        absolute_path = self.get_absolute_path(file_path)
        parser_service = DocumentParserService()
        output_dir = parser_service.get_output_directory(absolute_path)
        result_path = output_dir / "keyword_analysis.json"
        return result_path
    
    def file_exists(self, file_path: str) -> bool:
        """íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            absolute_path = self.get_absolute_path(file_path)
            return absolute_path.exists() and absolute_path.is_file()
        except (ValueError, OSError):
            return False
    
    def is_supported_file(self, file_path: str) -> bool:
        """ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ì¸ì§€ í™•ì¸"""
        allowed_extensions = ConfigService.get_json_config(
            self.db, "ALLOWED_EXTENSIONS", [".txt", ".pdf", ".docx", ".html", ".md"]
        )
        
        file_extension = Path(file_path).suffix.lower()
        return file_extension in allowed_extensions
    
    def load_existing_result(self, file_path: str) -> Optional[Dict[str, Any]]:
        """ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""
        try:
            result_file = self.get_result_file_path(file_path)
            if result_file.exists():
                with open(result_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    def save_result(self, file_path: str, result: Dict[str, Any]) -> str:
        """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        result_file = self.get_result_file_path(file_path)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        result_file.parent.mkdir(parents=True, exist_ok=True)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        enhanced_result = {
            "file_path": file_path,
            "analysis_timestamp": datetime.now().isoformat(),
            "analyzer_version": "1.0.0",
            **result
        }
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(enhanced_result, f, ensure_ascii=False, indent=2)
        
        return str(result_file)
    
    def backup_existing_result(self, file_path: str) -> Optional[str]:
        """ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ì„ ë°±ì—…"""
        result_file = self.get_result_file_path(file_path)
        if not result_file.exists():
            return None
        
        # ë°±ì—… íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = result_file.with_suffix(f'.backup_{timestamp}.json')
        
        try:
            shutil.copy2(result_file, backup_file)
            return str(backup_file)
        except Exception as e:
            print(f"ë°±ì—… ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def parse_file_content(self, file_path: str, use_docling: bool = False) -> str:
        """íŒŒì¼ ë‚´ìš©ì„ íŒŒì‹±í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        
        Args:
            file_path: íŒŒì‹±í•  íŒŒì¼ ê²½ë¡œ
            use_docling: Docling íŒŒì„œ ì‚¬ìš© ì—¬ë¶€ (PDF íŒŒì¼ì—ë§Œ ì ìš©)
        """
        absolute_path = self.get_absolute_path(file_path)
        
        # PDF íŒŒì¼ì´ê³  use_doclingì´ Trueì¸ ê²½ìš° Docling íŒŒì„œ ì‚¬ìš©
        if use_docling and absolute_path.suffix.lower() == '.pdf':
            from services.parser.docling_parser import DoclingParser
            parser = DoclingParser()
            parse_result = parser.parse(absolute_path)
        else:
            # ê¸°ë³¸ AutoParser ì‚¬ìš©
            parser = AutoParser()
            parse_result = parser.parse(absolute_path)
        
        if not parse_result.success:
            raise ValueError(f"íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {parse_result.error_message}")
        
        return parse_result.text
    
    def extract_metadata_with_all_parsers(self, file_path: str, use_llm: bool = True, save_to_file: bool = True) -> Dict[str, Any]:
        """ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì„œë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œë„
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            use_llm: LLM ì‚¬ìš© ì—¬ë¶€
            save_to_file: íŒŒì¼ë¡œ ì €ì¥ ì—¬ë¶€
        
        Returns:
            ê° íŒŒì„œì˜ ê²°ê³¼ë¥¼ í¬í•¨í•œ í†µí•© ë©”íƒ€ë°ì´í„°
        """
        import hashlib
        from datetime import datetime
        
        absolute_path = self.get_absolute_path(file_path)
        file_stats = absolute_path.stat()
        file_size = file_stats.st_size
        
        # ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
        all_results = {
            "file_path": file_path,
            "absolute_path": str(absolute_path),
            "extraction_timestamp": datetime.now().isoformat(),
            "parsers_attempted": [],
            "parsers_results": {},
            "best_result": None,
            "file_info": {
                "size": file_size,
                "modified": datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
                "extension": absolute_path.suffix.lower()
            }
        }
        
        # PDF íŒŒì¼ì¸ ê²½ìš° ëª¨ë“  ê°œë³„ íŒŒì„œ ì‹œë„
        if absolute_path.suffix.lower() == '.pdf':
            parsers_to_try = []
            
            # ê° PDF ì—”ì§„ ì¶”ê°€
            parsers_to_try.extend([
                ("pymupdf4llm", None),
                ("pdfplumber", None),
                ("pymupdf_advanced", None),
                ("pymupdf_basic", None),
                ("pypdf2", None),
                ("docling", "docling")  # Doclingì€ íŠ¹ë³„ ì²˜ë¦¬
            ])
        else:
            parsers_to_try = [("default", None)]  # ê¸°ë³¸ íŒŒì„œë§Œ
        
        best_score = 0
        best_parser = None
        
        for parser_name, parser_type in parsers_to_try:
            try:
                print(f"ğŸ” {parser_name} íŒŒì„œë¡œ ì¶”ì¶œ ì‹œë„...")
                all_results["parsers_attempted"].append(parser_name)
                
                # Docling íŒŒì„œì¸ ê²½ìš°
                if parser_type == "docling":
                    result = self.extract_file_metadata(
                        file_path=file_path,
                        use_llm=False,
                        save_to_file=False,
                        use_docling=True
                    )
                # ê°œë³„ PDF ì—”ì§„ì¸ ê²½ìš°
                elif parser_name in ["pymupdf4llm", "pdfplumber", "pymupdf_advanced", "pymupdf_basic", "pypdf2"]:
                    result = self.extract_file_metadata_with_specific_engine(
                        file_path=file_path,
                        engine_name=parser_name,
                        use_llm=use_llm and (parser_name == "pymupdf4llm"),  # LLMì€ pymupdf4llmì—ì„œë§Œ ì‚¬ìš©
                        save_to_file=False
                    )
                else:
                    # ê¸°ë³¸ íŒŒì„œ
                    result = self.extract_file_metadata(
                        file_path=file_path,
                        use_llm=use_llm,
                        save_to_file=False,
                        use_docling=False
                    )
                
                # ê²°ê³¼ í‰ê°€ (ì ìˆ˜ ê³„ì‚°)
                score = self._evaluate_parser_result(result)
                
                # ê²°ê³¼ ì €ì¥
                all_results["parsers_results"][parser_name] = {
                    "success": True,
                    "score": score,
                    "metadata": result,
                    "parser_used": result.get("parser_used", parser_name)
                }
                
                # ìµœê³  ì ìˆ˜ ì—…ë°ì´íŠ¸
                if score > best_score:
                    best_score = score
                    best_parser = parser_name
                    all_results["best_result"] = parser_name
                    
            except Exception as e:
                print(f"âŒ {parser_name} íŒŒì„œ ì‹¤íŒ¨: {e}")
                all_results["parsers_results"][parser_name] = {
                    "success": False,
                    "error": str(e),
                    "score": 0
                }
        
        # ìµœìƒì˜ ê²°ê³¼ë¥¼ ê¸°ë³¸ ë©”íƒ€ë°ì´í„°ë¡œ ì‚¬ìš©
        if best_parser and all_results["parsers_results"][best_parser]["success"]:
            best_metadata = all_results["parsers_results"][best_parser]["metadata"]
            # ìµœìƒì˜ ê²°ê³¼ë¥¼ ë£¨íŠ¸ ë ˆë²¨ì— ë³‘í•©
            for key, value in best_metadata.items():
                if key not in ["metadata_file", "markdown_file"]:  # íŒŒì¼ ê²½ë¡œëŠ” ì œì™¸
                    all_results[key] = value
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        if save_to_file:
            # í†µí•© ê²°ê³¼ ì €ì¥
            metadata_file = absolute_path.with_suffix(absolute_path.suffix + '.all_parsers.json')
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
            all_results["metadata_file"] = str(metadata_file)
            
            # ìƒˆë¡œìš´ íŒŒì„œë³„ ê°œë³„ íŒŒì¼ ì €ì¥ ì‹œìŠ¤í…œ ì‚¬ìš©
            try:
                saved_parser_files = save_parser_results(file_path, all_results["parsers_results"])
                all_results["individual_parser_files"] = saved_parser_files
                print(f"ğŸ“ {len(saved_parser_files)}ê°œ íŒŒì„œì˜ ê°œë³„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ê°œë³„ íŒŒì„œ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ê° íŒŒì„œë³„ ê²°ê³¼ë„ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ê°œë³„ ì €ì¥ (í˜¸í™˜ì„± ìœ ì§€)
            for parser_name, result_data in all_results["parsers_results"].items():
                if result_data["success"]:
                    parser_file = absolute_path.with_suffix(f'{absolute_path.suffix}.{parser_name}.json')
                    with open(parser_file, 'w', encoding='utf-8') as f:
                        json.dump(result_data["metadata"], f, ensure_ascii=False, indent=2)

            # PDFì˜ ê²½ìš° Markdown íŒŒì¼ë„ í•¨ê»˜ ì €ì¥ (.md, .docling.md)
            if absolute_path.suffix.lower() == '.pdf':
                markdown_files = {}
                # ê¸°ë³¸ íŒŒì„œ ê¸°ë°˜ Markdown (.md)
                try:
                    default_text = self.parse_file_content(file_path, use_docling=False)
                    if default_text:
                        md_path = absolute_path.with_suffix('.md')
                        md_content = self.convert_to_markdown(default_text)
                        with open(md_path, 'w', encoding='utf-8') as f:
                            f.write(md_content)
                        markdown_files["default"] = str(md_path)
                except Exception as e:
                    print(f"ê¸°ë³¸ Markdown ì €ì¥ ì‹¤íŒ¨: {e}")

                # Docling ê¸°ë°˜ Markdown (.docling.md)
                try:
                    docling_text = self.parse_file_content(file_path, use_docling=True)
                    if docling_text:
                        doc_md_path = absolute_path.with_suffix('.docling.md')
                        with open(doc_md_path, 'w', encoding='utf-8') as f:
                            f.write(docling_text)
                        markdown_files["docling"] = str(doc_md_path)
                except Exception as e:
                    print(f"Docling Markdown ì €ì¥ ì‹¤íŒ¨: {e}")

                if markdown_files:
                    all_results["markdown_files"] = markdown_files

        return all_results
    
    def _evaluate_parser_result(self, result: Dict[str, Any]) -> int:
        """íŒŒì„œ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì ìˆ˜ ë°˜í™˜
        
        ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ë” ì¢‹ì€ ê²°ê³¼
        """
        score = 0
        
        # ê¸°ë³¸ ì ìˆ˜ (ì„±ê³µí•œ ê²½ìš°)
        score += 10
        
        # ë©”íƒ€ë°ì´í„° í•„ë“œ ìˆ˜
        score += len(result.keys())
        
        # êµ¬ì¡° ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ê°€ì‚°ì 
        if "docling_structure" in result:
            score += 20
            structure = result["docling_structure"]
            if isinstance(structure, dict):
                # í…Œì´ë¸”ì´ ìˆìœ¼ë©´ ê°€ì‚°ì 
                if "tables" in structure and len(structure.get("tables", [])) > 0:
                    score += 10 * len(structure["tables"])
                # ì„¹ì…˜ì´ ìˆìœ¼ë©´ ê°€ì‚°ì 
                if "sections" in structure and len(structure.get("sections", [])) > 0:
                    score += 5 * len(structure["sections"])
                # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ê°€ì‚°ì 
                if "images" in structure and len(structure.get("images", [])) > 0:
                    score += 5 * len(structure["images"])
        
        # ë¬¸ì„œ êµ¬ì¡° ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
        if "document_structure" in result:
            score += 10
            
        # í…ìŠ¤íŠ¸ í†µê³„ê°€ ìˆëŠ” ê²½ìš°
        if "text_statistics" in result:
            score += 5
            
        # LLM ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
        if "content_analysis" in result:
            score += 15
        
        return score
    
    def extract_file_metadata_with_specific_engine(self, file_path: str, engine_name: str, use_llm: bool = False, save_to_file: bool = True) -> Dict[str, Any]:
        """íŠ¹ì • PDF ì—”ì§„ì„ ì‚¬ìš©í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            engine_name: ì‚¬ìš©í•  ì—”ì§„ ì´ë¦„ (pymupdf4llm, pdfplumber, pymupdf_advanced, pymupdf_basic, pypdf2)
            use_llm: LLM ì‚¬ìš© ì—¬ë¶€
            save_to_file: íŒŒì¼ë¡œ ì €ì¥ ì—¬ë¶€
        """
        import hashlib
        from services.parser.pdf_parser import PdfParser
        from services.parser.base import DocumentMetadata
        
        absolute_path = self.get_absolute_path(file_path)
        file_stats = absolute_path.stat()
        file_size = file_stats.st_size
        
        # PDF íŒŒì„œ ìƒì„±
        pdf_parser = PdfParser()
        
        # íŠ¹ì • ì—”ì§„ ì„ íƒ
        engine_methods = {
            "pymupdf4llm": pdf_parser._parse_with_pymupdf4llm,
            "pdfplumber": pdf_parser._parse_with_pdfplumber,
            "pymupdf_advanced": pdf_parser._parse_with_pymupdf_advanced,
            "pymupdf_basic": pdf_parser._parse_with_pymupdf_basic,
            "pypdf2": pdf_parser._parse_with_pypdf2
        }
        
        if engine_name not in engine_methods:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—”ì§„: {engine_name}")
        
        parse_method = engine_methods[engine_name]
        
        try:
            # íŠ¹ì • ì—”ì§„ìœ¼ë¡œ íŒŒì‹±
            text, metadata_dict = parse_method(absolute_path)
            
            if not text:
                raise ValueError(f"{engine_name} ì—”ì§„ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
            
            # DocumentMetadata ìƒì„±
            metadata = DocumentMetadata(
                title=metadata_dict.get('title', absolute_path.name),
                page_count=metadata_dict.get('page_count', 1),
                word_count=len(text.split()),
                file_size=file_size,
                mime_type='application/pdf',
                parser_name=f"pdf_parser_{engine_name}",
                parser_version="1.0"
            )
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì„¤ì •
            for key, value in metadata_dict.items():
                if hasattr(metadata, key) and value is not None:
                    setattr(metadata, key, value)
            
            # DocumentMetadataë¥¼ ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            metadata_result = metadata.to_schema_compliant_dict(
                file_id=None,
                project_id=None
            )
            
            metadata_result["parser_used"] = f"pdf_{engine_name}"
            
            # íŒŒì¼ ì •ë³´ ì¶”ê°€
            metadata_result["file_info"] = {
                "absolute_path": str(absolute_path),
                "relative_path": file_path,
                "exists": absolute_path.exists(),
                "size": file_size,
                "modified": datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
                "created": datetime.fromtimestamp(file_stats.st_ctime).isoformat() if hasattr(file_stats, 'st_ctime') else None,
            }
            
            # í…ìŠ¤íŠ¸ í†µê³„
            lines = text.split('\n')
            words = text.split()
            
            metadata_result["text_statistics"] = {
                "total_characters": len(text),
                "total_words": len(words),
                "total_lines": len(lines)
            }
            
            # LLM ë¶„ì„ (ìš”ì²­ëœ ê²½ìš°)
            if use_llm and text:
                try:
                    llm_metadata = self.extract_metadata_with_llm(text[:10000])
                    if llm_metadata:
                        metadata_result["content_analysis"] = llm_metadata
                except Exception as e:
                    print(f"LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # íŒŒì¼ ì €ì¥
            if save_to_file:
                metadata_file = absolute_path.with_suffix(f'{absolute_path.suffix}.{engine_name}.metadata.json')
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata_result, f, ensure_ascii=False, indent=2)
                metadata_result["metadata_file"] = str(metadata_file)
            
            return metadata_result
            
        except Exception as e:
            raise ValueError(f"{engine_name} ì—”ì§„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    def extract_file_metadata(self, file_path: str, use_llm: bool = True, save_to_file: bool = True, use_docling: bool = False) -> Dict[str, Any]:
        """íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ì¶œ (Dublin Core í‘œì¤€ ì¤€ìˆ˜)
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            use_llm: LLM ì‚¬ìš© ì—¬ë¶€
            save_to_file: íŒŒì¼ë¡œ ì €ì¥ ì—¬ë¶€
            use_docling: Docling íŒŒì„œ ì‚¬ìš© ì—¬ë¶€ (PDF íŒŒì¼ì—ë§Œ ì ìš©)
        """
        import hashlib
        
        absolute_path = self.get_absolute_path(file_path)
        
        # PDF íŒŒì¼ì´ê³  use_doclingì´ Trueì¸ ê²½ìš° Docling íŒŒì„œ ì‚¬ìš©
        if use_docling and absolute_path.suffix.lower() == '.pdf':
            from services.parser.docling_parser import DoclingParser
            parser = DoclingParser()
            parse_result = parser.parse(absolute_path)
        else:
            # ê¸°ë³¸ AutoParser ì‚¬ìš©
            parser = AutoParser()
            parse_result = parser.parse(absolute_path)
        
        if not parse_result.success:
            raise ValueError(f"íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {parse_result.error_message}")
        
        # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
        file_stats = absolute_path.stat()
        file_size = file_stats.st_size
        
        # ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ìƒì„±
        if not parse_result.metadata:
            from services.parser.base import DocumentMetadata
            metadata = DocumentMetadata(
                title=absolute_path.name,
                file_size=file_size,
                mime_type=None
            )
        else:
            metadata = parse_result.metadata
        
        # DocumentMetadataë¥¼ ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        metadata_dict = metadata.to_schema_compliant_dict(
            file_id=None,
            project_id=None
        )
        
        # Docling íŒŒì„œë¥¼ ì‚¬ìš©í•œ ê²½ìš° ì¶”ê°€ êµ¬ì¡° ì •ë³´ í¬í•¨
        if use_docling and hasattr(metadata, 'document_structure'):
            metadata_dict["docling_structure"] = metadata.document_structure
            metadata_dict["parser_used"] = "docling"
        else:
            metadata_dict["parser_used"] = parse_result.parser_name if hasattr(parse_result, 'parser_name') else "unknown"
        
        # dc:identifierë¥¼ íŒŒì¼ ë‚´ìš©ì˜ í•´ì‹œê°’ìœ¼ë¡œ ì„¤ì •
        if parse_result.text:
            file_hash = hashlib.sha256(parse_result.text.encode('utf-8')).hexdigest()
        else:
            # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ íŒŒì¼ ê²½ë¡œì™€ í¬ê¸°ë¡œ í•´ì‹œ ìƒì„±
            file_hash = hashlib.sha256(f"{absolute_path}:{file_size}".encode('utf-8')).hexdigest()
        metadata_dict["dc:identifier"] = file_hash
        
        # ì¶”ê°€ íŒŒì¼ ì •ë³´
        metadata_dict["file_info"] = {
            "absolute_path": str(absolute_path),
            "relative_path": file_path,
            "exists": absolute_path.exists(),
            "size": file_size,
            "modified": datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
            "created": datetime.fromtimestamp(file_stats.st_ctime).isoformat() if hasattr(file_stats, 'st_ctime') else None,
        }
        
        # í…ìŠ¤íŠ¸ í†µê³„ ë° ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
        if parse_result.text:
            text = parse_result.text
            
            # PDFì˜ ê²½ìš° ì¢…ì¢… ëª¨ë“  í…ìŠ¤íŠ¸ê°€ í•œ ì¤„ë¡œ íŒŒì‹±ë¨
            # ì´ ê²½ìš° ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì¬êµ¬ì„±
            if len(text.split('\n')) <= 2 and len(text) > 1000:
                # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
                import re
                sentence_pattern = re.compile(r'([.!?ã€‚ï¼ï¼Ÿ]\s+|\n\n)')
                sentences_list = sentence_pattern.split(text)
                # ì¬êµ¬ì„±ëœ í…ìŠ¤íŠ¸ (ë¬¸ì¥ë§ˆë‹¤ ì¤„ë°”ê¿ˆ)
                reconstructed_lines = []
                for sent in sentences_list:
                    if sent.strip() and not sentence_pattern.match(sent):
                        reconstructed_lines.append(sent.strip())
                text_for_analysis = '\n'.join(reconstructed_lines)
            else:
                text_for_analysis = text
            
            # ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
            document_structure = self.analyze_document_structure(text_for_analysis, absolute_path.suffix.lower())
            metadata_dict["document_structure"] = document_structure
            
            # í…ìŠ¤íŠ¸ í†µê³„
            lines = text_for_analysis.split('\n')
            words = text.split()
            paragraphs = [p for p in text_for_analysis.split('\n\n') if p.strip()]
            sentences = self.count_sentences(text)
            
            metadata_dict["text_statistics"] = {
                "total_characters": len(text),
                "total_words": len(words),
                "total_lines": len(lines),
                "total_paragraphs": len(paragraphs) if paragraphs else 1,
                "total_sentences": sentences,
                "avg_words_per_sentence": len(words) / sentences if sentences > 0 else 0,
                "avg_sentences_per_paragraph": sentences / len(paragraphs) if paragraphs else sentences,
            }
            
            # LLMì„ ì‚¬ìš©í•œ ê³ ê¸‰ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            if use_llm and parse_result.text:
                try:
                    llm_metadata = self.extract_metadata_with_llm(parse_result.text[:10000])  # ì²˜ìŒ 10000ì ì‚¬ìš©
                    if llm_metadata:
                        metadata_dict["content_analysis"] = llm_metadata
                except Exception as e:
                    print(f"LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # Unknown, Null, ë¹ˆ ê°’ ì œê±°
        metadata_dict = self.filter_empty_values(metadata_dict)
        
        # ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        if save_to_file:
            # Docling ì‚¬ìš© ì‹œ ë³„ë„ íŒŒì¼ëª… ìƒì„±
            if use_docling and metadata_dict.get("parser_used") == "docling":
                metadata_file = absolute_path.with_suffix(absolute_path.suffix + '.docling.metadata.json')
            else:
                metadata_file = absolute_path.with_suffix(absolute_path.suffix + '.metadata.json')
                
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata_dict, f, ensure_ascii=False, indent=2)
            metadata_dict["metadata_file"] = str(metadata_file)
            
            # Markdown í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ (íŒŒì‹±ëœ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°)
            if parse_result.text:
                # Docling ì‚¬ìš© ì‹œ ë³„ë„ Markdown íŒŒì¼ ìƒì„±
                if use_docling and metadata_dict.get("parser_used") == "docling":
                    markdown_file = absolute_path.with_suffix('.docling.md')
                    with open(markdown_file, 'w', encoding='utf-8') as f:
                        f.write(parse_result.text)
                    metadata_dict["markdown_file"] = str(markdown_file)
                else:
                    self.save_as_markdown(absolute_path, parse_result)
        
        return metadata_dict
    
    def filter_empty_values(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Unknown, Null, ë¹ˆ ê°’ì„ ì¬ê·€ì ìœ¼ë¡œ ì œê±°"""
        if not isinstance(data, dict):
            return data
        
        filtered = {}
        for key, value in data.items():
            # ê°’ì´ Noneì´ê±°ë‚˜ "Unknown"ì´ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´/ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì œì™¸
            if value is None:
                continue
            if isinstance(value, str):
                if value.lower() in ['unknown', 'null', ''] or value.strip() == '':
                    continue
            if isinstance(value, list) and len(value) == 0:
                continue
            if isinstance(value, dict):
                # ë”•ì…”ë„ˆë¦¬ëŠ” ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
                filtered_value = self.filter_empty_values(value)
                if filtered_value:  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ í¬í•¨
                    filtered[key] = filtered_value
            else:
                filtered[key] = value
        
        return filtered
    
    def save_as_markdown(self, file_path: Path, parse_result) -> str:
        """íŒŒì‹± ê²°ê³¼ë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        markdown_file = file_path.with_suffix('.md')
        
        # íŒŒì„œê°€ ë°˜í™˜í•œ ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
        content = parse_result.text
        
        # pymupdf4llmì´ ë°˜í™˜í•œ markdownì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if hasattr(parse_result, 'parser_name') and 'pymupdf4llm' in parse_result.parser_name:
            # pymupdf4llmì€ ì´ë¯¸ Markdown í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
            markdown_content = content
        else:
            # ë‹¤ë¥¸ íŒŒì„œì˜ ê²½ìš° ê¸°ë³¸ ë³€í™˜
            markdown_content = self.convert_to_markdown(content)
        
        # ë©”íƒ€ë°ì´í„° í—¤ë” ì¶”ê°€
        if parse_result.metadata:
            header = f"---\ntitle: {parse_result.metadata.title or file_path.name}\n"
            if parse_result.metadata.dc_creator:
                header += f"author: {parse_result.metadata.dc_creator}\n"
            if parse_result.metadata.dc_date:
                header += f"date: {parse_result.metadata.dc_date}\n"
            header += "---\n\n"
            markdown_content = header + markdown_content
        
        # íŒŒì¼ ì €ì¥
        with open(markdown_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        return str(markdown_file)
    
    def convert_to_markdown(self, text: str) -> str:
        """ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜"""
        import re
        
        lines = text.split('\n')
        markdown_lines = []
        
        for line in lines:
            stripped = line.strip()
            
            # ë¹ˆ ì¤„
            if not stripped:
                markdown_lines.append('')
                continue
            
            # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì œëª© íŒ¨í„´
            section_match = re.match(r'^(\d+(?:\.\d+)*)\s*[\.)]?\s+(.+)', stripped)
            if section_match:
                level = len(section_match.group(1).split('.'))
                title = section_match.group(2)
                markdown_lines.append('#' * min(level, 6) + ' ' + title)
                continue
            
            # í…Œì´ë¸” ê°ì§€ (ê°„ë‹¨í•œ íŒ¨í„´)
            if '|' in line and line.count('|') >= 2:
                markdown_lines.append(line)
                continue
            
            # ë¦¬ìŠ¤íŠ¸ í•­ëª©
            if re.match(r'^\s*[-*â€¢]\s+', line):
                markdown_lines.append(line)
                continue
            
            if re.match(r'^\s*\d+[.)]\s+', line):
                markdown_lines.append(line)
                continue
            
            # ì¼ë°˜ ë‹¨ë½
            markdown_lines.append(stripped)
        
        return '\n'.join(markdown_lines)
    
    def extract_metadata_with_llm(self, text: str, file_path: str = None) -> Optional[Dict[str, Any]]:
        """LangChainì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        from services.config_service import ConfigService
        import json
        import logging
        
        # ë¡œê±° ì„¤ì •
        logger = logging.getLogger(__name__)
        
        # LangChain ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if not LANGCHAIN_AVAILABLE:
            logger.error("âŒ LangChainì´ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
            return self._extract_metadata_fallback(text, "LangChain ì‚¬ìš© ë¶ˆê°€")
        
        # LLM ì„¤ì • í™•ì¸
        llm_enabled = ConfigService.get_bool_config(self.db, "ENABLE_LLM_EXTRACTION", False)
        logger.info(f"ğŸ” LLM extraction enabled: {llm_enabled}")
        if not llm_enabled:
            logger.warning("âš ï¸ LLM extraction is disabled in configuration")
            return None
        
        ollama_url = ConfigService.get_config_value(self.db, "OLLAMA_BASE_URL", "http://localhost:11434")
        model_name = ConfigService.get_config_value(self.db, "OLLAMA_MODEL", "llama3.2")
        
        logger.info(f"ğŸ“‹ LLM ì„¤ì •: URL={ollama_url}, Model={model_name}")
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
        # í…ìŠ¤íŠ¸ í¬ê¸° ì œí•œ (ë” ë§ì€ ë‚´ìš© í¬í•¨ì„ ìœ„í•´ ì¦ê°€)
        max_text_length = 10000  # 800 -> 10000ìœ¼ë¡œ ì¦ê°€
        truncated_text = text[:max_text_length]
        
        # ë¬¸ì„œ ì–¸ì–´ ê°ì§€ (í•œê¸€ ë¬¸ìê°€ ë§ìœ¼ë©´ í•œêµ­ì–´ ë¬¸ì„œ)
        import re
        korean_chars = len(re.findall(r'[ê°€-í£]', truncated_text))
        total_chars = len(truncated_text)
        is_korean_doc = (korean_chars / total_chars) > 0.3 if total_chars > 0 else False
        
        # ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë³€ê²½ (íƒ€ì„ì•„ì›ƒ ë°©ì§€)
        prompt = f"""Extract the document title, main language, and a brief summary from the following text.

Requirements:
- Return only valid JSON. No explanations, no markdown, no extra text.
- JSON must contain exactly these keys:
  {{
    "title": string,        // extracted or inferred document title
    "language": "ko" | "en" | "other",  // detected main language
    "summary": string       // concise summary (1â€“2 sentences max)
  }}
- Ensure the JSON is valid and can be parsed without errors.

Text:
{truncated_text}
"""
        
        logger.info(f"ğŸ¤– LangChain Ollama í˜¸ì¶œ ì‹œì‘: {ollama_url}")
        logger.debug(f"ğŸ“ Prompt ê¸¸ì´: {len(prompt)} ë¬¸ì")
        
        try:
            # LangChain Ollama í´ë¼ì´ì–¸íŠ¸ ìƒì„± - ë§¤ìš° ê¸´ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            ollama_client = OllamaLLM(
                base_url=ollama_url,
                model=model_name,
                timeout=360,  # 6ë¶„ìœ¼ë¡œ ì¦ê°€
                temperature=0.3,  # í´ë¼ì´ì–¸íŠ¸ ìƒì„±ì‹œ ì„¤ì •
            )
            
            logger.info(f"ğŸ“¤ LangChain ìš”ì²­ (model={model_name}, timeout=360ì´ˆ(6ë¶„), temperature=0.3)")
            
            # ë¨¼ì € ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ íƒ€ì„ì•„ì›ƒ í™•ì¸
            logger.info("ğŸ§ª ëª¨ë¸ ì‘ë‹µì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
            import time
            test_start = time.time()
            try:
                test_response = ollama_client.invoke("Say hello in JSON: {\"greeting\": \"hello\"}")
                test_duration = time.time() - test_start
                logger.info(f"âœ… ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì„±ê³µ - ì†Œìš”ì‹œê°„: {test_duration:.2f}ì´ˆ, ì‘ë‹µ: {test_response[:100]}")
            except Exception as test_error:
                test_duration = time.time() - test_start
                logger.error(f"âŒ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - ì†Œìš”ì‹œê°„: {test_duration:.2f}ì´ˆ, ì˜¤ë¥˜: {test_error}")
            
            logger.info(f"â±ï¸ ì‹¤ì œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œì‘ (ê¸´ í…ìŠ¤íŠ¸ë¡œ ì¸í•œ ì§€ì—°ì´ ì˜ˆìƒë©ë‹ˆë‹¤...)") 
            
            # ì‹œì‘ ì‹œê°„ ê¸°ë¡
            start_time = time.time()
            
            # LangChainì„ í†µí•´ í˜¸ì¶œ
            response_text = ollama_client.invoke(prompt)
            
            # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
            end_time = time.time()
            duration = end_time - start_time
            
            logger.info(f"ğŸ”§ LangChain í˜¸ì¶œ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ")
            
            logger.info(f"ğŸ“¥ LangChain ì‘ë‹µ ê¸¸ì´: {len(response_text)} ë¬¸ì")
            if response_text:
                logger.debug(f"ğŸ“„ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:200]}...")
            else:
                logger.warning("âš ï¸ LangChainì—ì„œ ë¹ˆ ì‘ë‹µ ë°˜í™˜")
            
            # í”„ë¡¬í”„íŠ¸/ì‘ë‹µ íŒŒì¼ ì €ì¥ (ê²°ê³¼ íŒŒì¼ë“¤ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì—)
            base_dir = "tests/debug_outputs/llm"  # ê¸°ë³¸ê°’
            if file_path:
                try:
                    from services.document_parser_service import DocumentParserService
                    absolute_path = self.get_absolute_path(file_path)
                    parser_service = DocumentParserService()
                    output_dir = parser_service.get_output_directory(absolute_path)
                    base_dir = str(output_dir)
                except Exception:
                    pass  # ê¸°ë³¸ê°’ ì‚¬ìš©
            
            log_prompt_and_response(
                label="local_metadata_langchain",
                provider="ollama",
                model=model_name,
                prompt=prompt,
                response=response_text,
                logger=logger,
                base_dir=base_dir,
                meta={
                    "base_url": ollama_url,
                    "temperature": 0.3,
                    "format": "json",
                    "langchain_version": True,
                },
            )
            
            # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
            if not response_text or response_text.strip() == "":
                logger.error("âŒ LangChainì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
                return self._extract_metadata_fallback(text, "LangChain ë¹ˆ ì‘ë‹µ")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                original_response = response_text
                
                # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json ... ``` ì²˜ë¦¬)
                if "```json" in response_text:
                    json_start = response_text.find("```json") + 7
                    json_end = response_text.find("```", json_start)
                    response_text = response_text[json_start:json_end].strip()
                    logger.debug("ğŸ”§ Extracted JSON from ```json``` blocks")
                elif "```" in response_text:
                    json_start = response_text.find("```") + 3
                    json_end = response_text.find("```", json_start)
                    response_text = response_text[json_start:json_end].strip()
                    logger.debug("ğŸ”§ Extracted JSON from ``` blocks")
                
                # ì²« ë²ˆì§¸ { ì™€ ë§ˆì§€ë§‰ } ì‚¬ì´ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ
                if "{" in response_text and "}" in response_text:
                    json_start = response_text.find("{")
                    json_end = response_text.rfind("}") + 1
                    response_text = response_text[json_start:json_end]
                    logger.debug("ğŸ”§ Extracted JSON between { and }")
                
                def repair_json_like(s: str) -> str:
                    """ê°„ë‹¨í•œ JSON ë³µêµ¬: ì˜ëª»ëœ ì¸ìš©ë¶€í˜¸/ì‰¼í‘œ/ì½”ë“œíœìŠ¤ ì •ë¦¬"""
                    import re
                    # ìŠ¤ë§ˆíŠ¸ ì¸ìš©ë¶€í˜¸ë¥¼ ASCIIë¡œ ë³€í™˜
                    s = s.replace(""", '"').replace(""", '"').replace("'", "'")
                    # í‚¤ì— ì‚¬ìš©ëœ ë‹¨ì¼ ì¸ìš©ë¶€í˜¸ë¥¼ ì´ìŠ¤ì¼€ì´í”„ëœ ìŒë”°ì˜´í‘œë¡œ ë³€ê²½
                    s = re.sub(r"'([A-Za-z0-9_\- ]+)'\s*:", r'"\1":', s)
                    # ê°’ì— ì‚¬ìš©ëœ ë‹¨ì¼ ì¸ìš©ë¶€í˜¸ ë¬¸ìì—´ì„ ìŒë”°ì˜´í‘œë¡œ ë³€ê²½
                    s = re.sub(r":\s*'([^']*)'", r': "\1"', s)
                    # ëì— ë¶™ì€ ì‰¼í‘œ ì œê±°
                    s = re.sub(r",\s*([}\]])", r"\1", s)
                    # BOM/ì œì–´ë¬¸ì ì œê±°
                    s = s.replace("\ufeff", "").strip()
                    return s
                
                try:
                    metadata = json.loads(response_text)
                except json.JSONDecodeError:
                    repaired = repair_json_like(response_text)
                    metadata = json.loads(repaired)
                    logger.info("ğŸ› ï¸ Nonâ€‘strict JSON repaired successfully")
                
                logger.info(f"âœ… LangChain ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì„±ê³µ: {list(metadata.keys())}")
                # ì›ë³¸ ì‘ë‹µë„ í¬í•¨
                metadata["_llm_metadata"] = {
                    "raw_response": original_response,
                    "extraction_status": "langchain_success",
                    "model": model_name,
                    "response_length": len(original_response)
                }
                return metadata
                
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                logger.error(f"ğŸ“„ ë¬¸ì œê°€ ëœ ì‘ë‹µ (ì²˜ìŒ 500ì): {response_text[:500]}")
                logger.warning("âš ï¸ í´ë°± ë©”íƒ€ë°ì´í„° ì¶”ì¶œë¡œ ì „í™˜")
                
                return self._extract_metadata_fallback(text, f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                
        except Exception as e:
            logger.error(f"âŒ LangChain ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            logger.exception("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
            
            # ì˜¤ë¥˜ ì‹œì—ë„ ë¡œê¹…
            try:
                # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
                base_dir = "tests/debug_outputs/llm"  # ê¸°ë³¸ê°’
                if file_path:
                    try:
                        from services.document_parser_service import DocumentParserService
                        absolute_path = self.get_absolute_path(file_path)
                        parser_service = DocumentParserService()
                        output_dir = parser_service.get_output_directory(absolute_path)
                        base_dir = str(output_dir)
                    except Exception:
                        pass  # ê¸°ë³¸ê°’ ì‚¬ìš©
                        
                log_prompt_and_response(
                    label="local_metadata_langchain_error",
                    provider="ollama",
                    model=model_name,
                    prompt=prompt,
                    response="",
                    logger=logger,
                    base_dir=base_dir,
                    meta={"base_url": ollama_url, "error": f"exception: {e}", "langchain_version": True},
                )
            except Exception:
                pass
            
            return self._extract_metadata_fallback(text, f"LangChain ì˜¤ë¥˜: {str(e)}")
    
    def _test_ollama_model(self, ollama_url: str, model_name: str) -> bool:
        """LangChainìœ¼ë¡œ Ollama ëª¨ë¸ ìƒíƒœë¥¼ ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸"""
        import logging
        logger = logging.getLogger(__name__)
        
        if not LANGCHAIN_AVAILABLE:
            logger.warning("âš ï¸ LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ëª¨ë¸ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœ€")
            return False
        
        try:
            logger.debug(f"ğŸ§ª LangChainìœ¼ë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘: {model_name}")
            
            # LangChain Ollama í´ë¼ì´ì–¸íŠ¸ë¡œ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            ollama_client = OllamaLLM(
                base_url=ollama_url,
                model=model_name,
                timeout=15
            )
            
            test_response = ollama_client.invoke("Hello")
            
            if test_response and test_response.strip():
                logger.info(f"âœ… LangChain ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ: '{test_response.strip()[:50]}'")
                return True
            else:
                logger.warning("âš ï¸ LangChain ëª¨ë¸ í…ŒìŠ¤íŠ¸ - ë¹ˆ ì‘ë‹µ ë°˜í™˜")
                return False
                
        except Exception as e:
            logger.warning(f"âš ï¸ LangChain ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _extract_metadata_fallback(self, text: str, error_reason: str) -> Dict[str, Any]:
        """LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info(f"ğŸ“‹ í´ë°± ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œì‘ - ì‚¬ìœ : {error_reason}")
        
        # í…ìŠ¤íŠ¸ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        # ì œëª© ì¶”ì¶œ ì‹œë„ (ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ì¤„)
        title = "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"
        for line in lines[:10]:
            if (len(line) > 5 and len(line) < 200 and
                not line.replace('.', '').replace('-', '').replace('#', '').replace('=', '').isdigit() and
                not line.startswith('http') and
                not '@' in line):
                title = line[:100]  # ì œëª©ì€ 100ìë¡œ ì œí•œ
                break
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)
        import re
        words = re.findall(r'\b[ê°€-í£a-zA-Z]{3,}\b', text)
        word_freq = {}
        for word in words:
            if len(word) >= 3:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # ìƒìœ„ 5ê°œ ë‹¨ì–´ë¥¼ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
        keywords = [word for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]]
        
        # ì–¸ì–´ ê°ì§€
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        language = "ko" if korean_chars > english_chars else "en"
        
        # ë¬¸ì„œ íƒ€ì… ì¶”ì •
        doc_type = "ë¬¸ì„œ"
        if "ë³´ê³ ì„œ" in text or "report" in text.lower():
            doc_type = "ë³´ê³ ì„œ"
        elif "ë…¼ë¬¸" in text or "paper" in text.lower() or "abstract" in text.lower():
            doc_type = "ë…¼ë¬¸"
        elif "ë§¤ë‰´ì–¼" in text or "manual" in text.lower() or "guide" in text.lower():
            doc_type = "ë§¤ë‰´ì–¼"
        
        fallback_metadata = {
            "title": title,
            "document_type": doc_type,
            "language": language,
            "keywords": keywords,
            "summary": f"í´ë°± ëª¨ë“œë¡œ ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„° - ì›ì¸: {error_reason}",
            "main_topics": keywords[:3],  # ìƒìœ„ 3ê°œë¥¼ ì£¼ìš” í† í”½ìœ¼ë¡œ
            "date": None,
            "extraction_status": "fallback",
            "fallback_reason": error_reason,
            "_llm_metadata": {
                "raw_response": "",
                "extraction_status": "fallback",
                "model": "none",
                "response_length": 0,
                "fallback_reason": error_reason
            }
        }
        
        logger.info(f"âœ… í´ë°± ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ - ì œëª©: '{title[:50]}', í‚¤ì›Œë“œ: {len(keywords)}ê°œ")
        return fallback_metadata
    
    def _extract_metadata_with_langchain(self, text: str, ollama_url: str, model_name: str) -> Optional[Dict[str, Any]]:
        """LangChainì„ ì‚¬ìš©í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œë„"""
        if not LANGCHAIN_AVAILABLE:
            return None
            
        import logging
        import json
        logger = logging.getLogger(__name__)
        
        try:
            # LangChain Ollama í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            ollama_client = OllamaLLM(
                base_url=ollama_url,
                model=model_name,
                timeout=60
            )
            
            # ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸ (ë” ì•ˆì •ì ì¸ ì‘ë‹µì„ ìœ„í•´)
            prompt = f"""Analyze this document and extract metadata in JSON format:

{text[:15000]}

Return only a JSON object with these fields:
{{
    "title": "document title or first meaningful line",
    "summary": "1-2 sentence summary",
    "keywords": ["key1", "key2", "key3"],
    "language": "ko or en"
}}

JSON only, no explanations:"""
            
            logger.debug(f"ğŸ”— LangChain í˜¸ì¶œ ì‹œì‘ - ëª¨ë¸: {model_name}")
            
            # LangChainì„ í†µí•´ í˜¸ì¶œ
            response = ollama_client.invoke(prompt)
            
            logger.debug(f"ğŸ“„ LangChain ì‘ë‹µ ê¸¸ì´: {len(response)} ë¬¸ì")
            
            if not response or response.strip() == "":
                logger.warning("âš ï¸ LangChainì—ì„œë„ ë¹ˆ ì‘ë‹µ ë°˜í™˜")
                return None
            
            # í”„ë¡¬í”„íŠ¸/ì‘ë‹µ ë¡œê¹… (ê²°ê³¼ íŒŒì¼ë“¤ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì—)
            base_dir = "tests/debug_outputs/llm"  # ê¸°ë³¸ê°’
            if hasattr(self, '_current_file_path') and self._current_file_path:
                try:
                    from services.document_parser_service import DocumentParserService
                    absolute_path = self.get_absolute_path(self._current_file_path)
                    parser_service = DocumentParserService()
                    output_dir = parser_service.get_output_directory(absolute_path)
                    base_dir = str(output_dir)
                except Exception:
                    pass  # ê¸°ë³¸ê°’ ì‚¬ìš©
            
            log_prompt_and_response(
                label="local_metadata_langchain",
                provider="ollama",
                model=model_name,
                prompt=prompt,
                response=response,
                logger=logger,
                base_dir=base_dir,
                meta={
                    "base_url": ollama_url,
                    "langchain_version": True,
                },
            )
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # JSON ë¶€ë¶„ ì¶”ì¶œ
                json_text = response.strip()
                if "```json" in json_text:
                    start_idx = json_text.find("```json") + 7
                    end_idx = json_text.find("```", start_idx)
                    if end_idx != -1:
                        json_text = json_text[start_idx:end_idx]
                
                # ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€ ì¶”ì¶œ
                start = json_text.find('{')
                end = json_text.rfind('}')
                if start != -1 and end != -1 and end > start:
                    json_text = json_text[start:end+1]
                
                metadata = json.loads(json_text)
                
                # ê¸°ë³¸ í•„ë“œ ë³´ì¥
                result = {
                    "title": metadata.get("title", "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"),
                    "document_type": "ë¬¸ì„œ",
                    "language": metadata.get("language", "ko"),
                    "keywords": metadata.get("keywords", []),
                    "summary": metadata.get("summary", "LangChainìœ¼ë¡œ ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°"),
                    "main_topics": metadata.get("keywords", [])[:3],
                    "date": metadata.get("date"),
                    "extraction_status": "langchain_success",
                    "_llm_metadata": {
                        "raw_response": response,
                        "extraction_status": "langchain_success",
                        "model": model_name,
                        "response_length": len(response)
                    }
                }
                
                logger.info(f"âœ… LangChainìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì„±ê³µ - ì œëª©: '{result['title'][:50]}'")
                return result
                
            except json.JSONDecodeError as e:
                logger.error(f"âŒ LangChain ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                logger.debug(f"ğŸ“„ LangChain ì‘ë‹µ: {response[:500]}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ LangChain ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def analyze_document_structure(self, text: str, file_extension: str) -> Dict[str, Any]:
        """ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ (ì„¹ì…˜, í…Œì´ë¸”, ê·¸ë¦¼ ë“±)"""
        import re
        
        structure = {
            "sections": [],
            "tables_count": 0,
            "figures_count": 0,
            "references_count": 0,
            "footnotes_count": 0,
            "lists_count": 0,
            "headings_hierarchy": []
        }
        
        lines = text.split('\n')
        
        # ì„¹ì…˜/ì œëª© íŒ¨í„´ ê°ì§€ (ë” í¬ê´„ì ì¸ íŒ¨í„´)
        # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì œëª© íŒ¨í„´ (1. , 1.1, 2.3.4 ë“±)
        section_pattern = re.compile(r'^(\d+(?:\.\d+)*)\s*[\.)]?\s+(.+)')
        # ë¡œë§ˆ ìˆ«ì íŒ¨í„´ (I., II., III. ë“±)
        roman_pattern = re.compile(r'^([IVXLCDM]+)\s*[\.)]?\s+(.+)')
        # Markdown ìŠ¤íƒ€ì¼ í—¤ë” (###)
        markdown_pattern = re.compile(r'^(#{1,6})\s+(.+)')
        
        # í•œêµ­ì–´ ì„¹ì…˜ íŒ¨í„´ (ì œ1ì¥, ì œ2ì ˆ, 1ì¥, 2ì ˆ ë“±)
        korean_section_pattern = re.compile(r'^ì œ?\s*(\d+)\s*[ì¥ì ˆí•­]\s*[\.:]?\s*(.+)')
        
        # ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ì œëª© íŒ¨í„´ (ì£¼ë¡œ ì˜ë¬¸ ë¬¸ì„œ)
        # PDFì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” íŒ¨í„´
        uppercase_title_pattern = re.compile(r'^([A-Z][A-Z\s]{2,})\s*$')
        
        # ì½œë¡ ì´ë‚˜ ëŒ€ì‹œë¡œ ëë‚˜ëŠ” ì œëª© íŒ¨í„´
        title_with_separator = re.compile(r'^([ê°€-í£A-Za-z0-9\s]+)\s*[:ï¼š-]\s*$')
        
        # ë“¤ì—¬ì“°ê¸°ê°€ ì—†ê³  ì§§ì€ ë…ë¦½ ë¼ì¸ (ì œëª©ì¼ ê°€ëŠ¥ì„±)
        potential_title_pattern = re.compile(r'^[^\s](.{5,50})$')
        
        # í…Œì´ë¸” ê°ì§€ íŒ¨í„´ (í™•ì¥)
        table_patterns = [
            re.compile(r'\|.*\|'),  # Markdown í…Œì´ë¸”
            re.compile(r'[<\[]?\s*í‘œ\s*\d+', re.IGNORECASE),  # "í‘œ 1", "<í‘œ 1>", "[í‘œ 1]"
            re.compile(r'Table\s*\d+', re.IGNORECASE),
            re.compile(r'<table', re.IGNORECASE),  # HTML í…Œì´ë¸”
            re.compile(r'â”Œ|â”œ|â””|â”€|â”‚'),  # Box drawing ë¬¸ì
            re.compile(r'[í‘œè¡¨]\s*[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+'),  # í•œì ìˆ«ì í¬í•¨
        ]
        
        # ê·¸ë¦¼/ì°¨íŠ¸ ê°ì§€ íŒ¨í„´ (í™•ì¥)
        figure_patterns = [
            re.compile(r'[<\[]?\s*ê·¸ë¦¼\s*\d+', re.IGNORECASE),  # "ê·¸ë¦¼ 1", "<ê·¸ë¦¼ 1>"
            re.compile(r'Figure\s*\d+', re.IGNORECASE),
            re.compile(r'Fig\.\s*\d+', re.IGNORECASE),
            re.compile(r'[<\[]?\s*ì°¨íŠ¸\s*\d+', re.IGNORECASE),
            re.compile(r'Chart\s*\d+', re.IGNORECASE),
            re.compile(r'[<\[]?\s*ë„í‘œ\s*\d+', re.IGNORECASE),  # ë„í‘œ
            re.compile(r'[<\[]?\s*ì‚¬ì§„\s*\d+', re.IGNORECASE),  # ì‚¬ì§„
            re.compile(r'[<\[]?\s*ì´ë¯¸ì§€\s*\d+', re.IGNORECASE),  # ì´ë¯¸ì§€
            re.compile(r'!\[.*\]\(.*\)'),  # Markdown ì´ë¯¸ì§€
        ]
        
        # ì°¸ê³ ë¬¸í—Œ ê°ì§€
        reference_patterns = [
            re.compile(r'^\[\d+\]'),  # [1] ìŠ¤íƒ€ì¼
            re.compile(r'ì°¸ê³ ë¬¸í—Œ', re.IGNORECASE),
            re.compile(r'References', re.IGNORECASE),
            re.compile(r'Bibliography', re.IGNORECASE),
        ]
        
        # ê°ì£¼ ê°ì§€
        footnote_patterns = [
            re.compile(r'\[\^\d+\]'),  # Markdown ê°ì£¼
            re.compile(r'ì£¼\s*\d+[:\)]'),  # "ì£¼1:", "ì£¼ 1)"
        ]
        
        # ë¦¬ìŠ¤íŠ¸ ê°ì§€
        list_patterns = [
            re.compile(r'^\s*[-*â€¢]\s+'),  # ë¶ˆë¦¿ ë¦¬ìŠ¤íŠ¸
            re.compile(r'^\s*\d+[.)]\s+'),  # ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸
            re.compile(r'^\s*[a-z][.)]\s+', re.IGNORECASE),  # ì•ŒíŒŒë²³ ë¦¬ìŠ¤íŠ¸
        ]
        
        # ë¼ì¸ë³„ ë¶„ì„
        prev_line = ""
        next_line = ""
        
        for i, line in enumerate(lines):
            # ì´ì „ ë¼ì¸ê³¼ ë‹¤ìŒ ë¼ì¸ ì°¸ì¡° (ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ìš©)
            prev_line = lines[i-1] if i > 0 else ""
            next_line = lines[i+1] if i < len(lines)-1 else ""
            
            stripped_line = line.strip()
            
            # ë¹ˆ ì¤„ ê±´ë„ˆë›°ê¸°
            if not stripped_line:
                continue
            
            # ì„¹ì…˜/ì œëª© ê°ì§€
            section_match = section_pattern.match(stripped_line)
            if section_match:
                section_num = section_match.group(1)
                section_title = section_match.group(2)
                level = len(section_num.split('.'))
                structure["sections"].append({
                    "number": section_num,
                    "title": section_title,
                    "level": level,
                    "line": i + 1
                })
                structure["headings_hierarchy"].append(level)
                continue
            
            # í•œêµ­ì–´ ì„¹ì…˜ íŒ¨í„´
            korean_match = korean_section_pattern.match(stripped_line)
            if korean_match:
                structure["sections"].append({
                    "number": korean_match.group(1),
                    "title": korean_match.group(2) if korean_match.group(2) else stripped_line,
                    "level": 1,
                    "line": i + 1,
                    "style": "korean"
                })
                structure["headings_hierarchy"].append(1)
                continue
            
            # ë¡œë§ˆ ìˆ«ì ì„¹ì…˜
            roman_match = roman_pattern.match(stripped_line)
            if roman_match and len(roman_match.group(1)) <= 4:  # ë„ˆë¬´ ê¸´ ë¡œë§ˆ ìˆ«ìëŠ” ì œì™¸
                structure["sections"].append({
                    "number": roman_match.group(1),
                    "title": roman_match.group(2),
                    "level": 1,
                    "line": i + 1
                })
                structure["headings_hierarchy"].append(1)
                continue
            
            # Markdown í—¤ë”
            markdown_match = markdown_pattern.match(stripped_line)
            if markdown_match:
                level = len(markdown_match.group(1))
                structure["sections"].append({
                    "title": markdown_match.group(2),
                    "level": level,
                    "line": i + 1,
                    "style": "markdown"
                })
                structure["headings_hierarchy"].append(level)
                continue
            
            # ëŒ€ë¬¸ì ì œëª© (ì˜ë¬¸ ë¬¸ì„œ)
            if uppercase_title_pattern.match(stripped_line) and len(stripped_line) < 50:
                # ì•ë’¤ê°€ ë¹ˆ ì¤„ì¸ ê²½ìš° ì œëª©ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
                if (not prev_line.strip() or not next_line.strip()):
                    structure["sections"].append({
                        "title": stripped_line,
                        "level": 1,
                        "line": i + 1,
                        "style": "uppercase"
                    })
                    structure["headings_hierarchy"].append(1)
                    continue
            
            # ì½œë¡ ì´ë‚˜ ëŒ€ì‹œë¡œ ëë‚˜ëŠ” ì œëª©
            if title_with_separator.match(stripped_line):
                structure["sections"].append({
                    "title": title_with_separator.match(stripped_line).group(1),
                    "level": 2,
                    "line": i + 1,
                    "style": "separator"
                })
                structure["headings_hierarchy"].append(2)
                continue
            
            # ì§§ì€ ë…ë¦½ ë¼ì¸ (ì „í›„ ë¹ˆ ì¤„ì´ ìˆê³  ê¸¸ì´ê°€ ì ì ˆí•œ ê²½ìš°)
            if (len(stripped_line) > 5 and len(stripped_line) < 50 and 
                not prev_line.strip() and not next_line.strip() and 
                not stripped_line.endswith(('.', 'ã€‚', '!', '?', 'ï¼', 'ï¼Ÿ'))):
                # ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ìë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ê²½ìš°
                if re.match(r'^[ê°€-í£A-Za-z]', stripped_line):
                    structure["sections"].append({
                        "title": stripped_line,
                        "level": 3,
                        "line": i + 1,
                        "style": "isolated"
                    })
                    structure["headings_hierarchy"].append(3)
            
            # í…Œì´ë¸” ê°ì§€
            for pattern in table_patterns:
                if pattern.search(line):
                    structure["tables_count"] += 1
                    break
            
            # ê·¸ë¦¼/ì°¨íŠ¸ ê°ì§€
            for pattern in figure_patterns:
                if pattern.search(line):
                    structure["figures_count"] += 1
                    break
            
            # ì°¸ê³ ë¬¸í—Œ ê°ì§€
            for pattern in reference_patterns:
                if pattern.search(line):
                    structure["references_count"] += 1
                    break
            
            # ê°ì£¼ ê°ì§€
            for pattern in footnote_patterns:
                if pattern.search(line):
                    structure["footnotes_count"] += 1
                    break
            
            # ë¦¬ìŠ¤íŠ¸ ê°ì§€
            for pattern in list_patterns:
                if pattern.match(line):
                    structure["lists_count"] += 1
                    break
        
        # ì„¹ì…˜ ì •ë¦¬ ë° ìš”ì•½
        if structure["sections"]:
            # ì¤‘ë³µ ì œê±° (ê°™ì€ ë¼ì¸ì˜ ì„¹ì…˜ ì œê±°)
            seen_lines = set()
            unique_sections = []
            for section in structure["sections"]:
                if section["line"] not in seen_lines:
                    seen_lines.add(section["line"])
                    unique_sections.append(section)
            
            structure["sections"] = unique_sections
            structure["total_sections"] = len(unique_sections)
            structure["max_heading_level"] = max(structure["headings_hierarchy"]) if structure["headings_hierarchy"] else 0
            
            # ì„¹ì…˜ì„ ë ˆë²¨ë³„ë¡œ ë¶„ë¥˜
            sections_by_level = {}
            for section in unique_sections:
                level = section.get("level", 1)
                if level not in sections_by_level:
                    sections_by_level[level] = []
                sections_by_level[level].append(section["title"])
            
            structure["sections_by_level"] = sections_by_level
            
            # ì£¼ìš” ì„¹ì…˜ë§Œ ì¶”ì¶œ (ìƒìœ„ 20ê°œ)
            structure["main_sections"] = [s["title"] for s in unique_sections[:20]]
        else:
            # ì„¹ì…˜ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
            structure["total_sections"] = 0
            structure["max_heading_level"] = 0
            structure["sections_by_level"] = {}
            structure["main_sections"] = []
        
        # ë¬¸ì„œ êµ¬ì¡° ìš”ì•½
        structure["document_outline"] = {
            "has_table_of_contents": any("ëª©ì°¨" in s.get("title", "") or "Contents" in s.get("title", "") for s in structure["sections"]),
            "has_introduction": any("ì„œë¡ " in s.get("title", "") or "Introduction" in s.get("title", "") or "ê°œìš”" in s.get("title", "") for s in structure["sections"]),
            "has_conclusion": any("ê²°ë¡ " in s.get("title", "") or "Conclusion" in s.get("title", "") or "ë§ºìŒ" in s.get("title", "") for s in structure["sections"]),
            "has_references": structure["references_count"] > 0,
            "has_appendix": any("ë¶€ë¡" in s.get("title", "") or "Appendix" in s.get("title", "") for s in structure["sections"]),
            "structure_quality": "good" if structure.get("total_sections", 0) > 3 else "poor"
        }
        
        return structure
    
    def count_sentences(self, text: str) -> int:
        """ë¬¸ì¥ ê°œìˆ˜ ê³„ì‚°"""
        import re
        # í•œêµ­ì–´ì™€ ì˜ì–´ ë¬¸ì¥ ì¢…ê²° ë¶€í˜¸ ê³ ë ¤
        sentence_endings = re.compile(r'[.!?ã€‚ï¼ï¼Ÿ]+[\s\n]')
        sentences = sentence_endings.split(text)
        # ë¹ˆ ë¬¸ì¥ ì œì™¸
        return len([s for s in sentences if s.strip()])
    
    def analyze_document_structure_with_llm(self, text: str, file_path: str, file_extension: str, overrides: Dict[str, Any] = None) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•œ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ (Ollama/OpenAI/Gemini)
        
        overrides: ìš”ì²­ ë‹¨ìœ„ë¡œ LLM êµ¬ì„±ì„ ë®ì–´ì“°ëŠ” ì˜µì…˜(dict)
        ì˜ˆ) {"enabled": true, "provider": "gemini", "model": "models/gemini-2.0-flash", "api_key": "...", "base_url": "...", "max_tokens": 1000, "temperature": 0.2}
        """
        import logging
        import json
        from services.config_service import ConfigService
        from prompts.templates import DocumentStructurePrompts
        from utils.llm_logger import log_prompt_and_response
        
        logger = logging.getLogger(__name__)
        
        # LLM ì„¤ì • í™•ì¸
        overrides = overrides or {}
        llm_enabled = overrides.get("enabled") if "enabled" in overrides else ConfigService.get_bool_config(self.db, "ENABLE_LLM_EXTRACTION", False)
        if not llm_enabled:
            logger.warning("âš ï¸ LLM extraction is disabled in configuration")
            return self._fallback_structure_analysis(text, file_extension)
        
        provider = overrides.get("provider") or ConfigService.get_config_value(self.db, "LLM_PROVIDER", "ollama")
        timeout_override = overrides.get("timeout")
        ollama_timeout_default = ConfigService.get_int_config(self.db, "OLLAMA_TIMEOUT", 120)
        ollama_timeout = timeout_override or ollama_timeout_default
        logger.info(f"ğŸ” LLM ê¸°ë°˜ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì‹œì‘ - provider={provider}")

        try:
            # Providerë³„ ëª¨ë¸/ì—”ë“œí¬ì¸íŠ¸ êµ¬ì„±
            logger.info(f"ğŸ” Provider ì„¤ì • ì‹œì‘: {provider}")
            if provider == "ollama":
                ollama_url = overrides.get("base_url") or ConfigService.get_config_value(self.db, "OLLAMA_BASE_URL", "http://localhost:11434")
                model_name = overrides.get("model") or ConfigService.get_config_value(self.db, "OLLAMA_MODEL", "llama3.2")
                openai_conf = None
                gemini_conf = None
            elif provider == "openai":
                openai_conf = {**ConfigService.get_openai_config(self.db), **overrides}
                if timeout_override is not None:
                    openai_conf["timeout"] = timeout_override
                openai_conf.setdefault("timeout", 120)
                gemini_conf = None
                model_name = openai_conf.get("model")
            elif provider == "gemini":
                gemini_conf = {**ConfigService.get_gemini_config(self.db), **overrides}
                if timeout_override is not None:
                    gemini_conf["timeout"] = timeout_override
                gemini_conf.setdefault("timeout", 120)
                openai_conf = None
                model_name = gemini_conf.get("model")
            else:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” LLM provider '{provider}', ollamaë¡œ í´ë°±")
                provider = "ollama"
                ollama_url = overrides.get("base_url") or ConfigService.get_config_value(self.db, "OLLAMA_BASE_URL", "http://localhost:11434")
                model_name = overrides.get("model") or ConfigService.get_config_value(self.db, "OLLAMA_MODEL", "llama3.2")
                openai_conf = None
                gemini_conf = None
                ollama_timeout = timeout_override or ollama_timeout_default
        except Exception as e:
            logger.error(f"âŒ LLM provider ì„¤ì • ì‹¤íŒ¨: {e}")
            return self._fallback_structure_analysis(text, file_extension)

        is_gemini_flash_25 = bool(provider == "gemini" and isinstance(model_name, str) and "2.5" in model_name)
        if is_gemini_flash_25 and gemini_conf is not None:
            gemini_conf.setdefault("response_mime_type", "application/json")

        logger.info(f"ğŸ” LLM ëª¨ë¸: {model_name}")
        if is_gemini_flash_25:
            logger.info("âœ¨ Gemini 2.5 Flash ì „ìš© í”„ë¡¬í”„íŠ¸ ë° ì‘ë‹µ ì„¤ì • ì ìš©")
        
        try:
            # LangChain Ollama í´ë¼ì´ì–¸íŠ¸ ë˜ëŠ” HTTP í˜¸ì¶œ ì¤€ë¹„
            ollama_client = None
            if provider == "ollama":
                try:
                    from langchain_ollama import OllamaLLM
                    ollama_client = OllamaLLM(
                        base_url=ollama_url,
                        model=model_name,
                        timeout=ollama_timeout,
                        temperature=0.2,
                    )
                except Exception as e:
                    logger.error(f"âŒ Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    return self._fallback_structure_analysis_with_llm_attempt(text, file_extension, str(e))
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë” ë§ì€ ë‚´ìš© í¬í•¨ì„ ìœ„í•´ ì¦ê°€)
            max_text_length = 15000  # 3000 -> 15000ìœ¼ë¡œ ì¦ê°€
            truncated_text = text[:max_text_length] if len(text) > max_text_length else text
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
            prompt_template = (
                DocumentStructurePrompts.STRUCTURE_ANALYSIS_LLM_GEMINI_FLASH25
                if is_gemini_flash_25
                else DocumentStructurePrompts.STRUCTURE_ANALYSIS_LLM
            )
            
            # íŒŒì¼ ì •ë³´ ì¤€ë¹„
            from pathlib import Path
            file_path_obj = Path(file_path) if isinstance(file_path, str) else file_path
            file_info = {
                "filename": file_path_obj.name,
                "extension": file_extension,
                "size": len(text),
                "truncated_size": len(truncated_text)
            }
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = prompt_template.format(
                file_info=json.dumps(file_info, ensure_ascii=False, indent=2),
                text=truncated_text
            )
            
            logger.info(f"ğŸ“¤ LLM êµ¬ì¡° ë¶„ì„ ìš”ì²­ ì¤‘... (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(truncated_text)} ë¬¸ì)")
            
            # LLM í˜¸ì¶œ
            if provider == "ollama":
                try:
                    logger.info("ğŸ” Ollama í˜¸ì¶œ ì‹œì‘...")
                    logger.info(f"ğŸ” í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
                    response = ollama_client.invoke(prompt)
                    logger.info(f"ğŸ” Ollama ì‘ë‹µ ì„±ê³µ - ê¸¸ì´: {len(response)}ì")
                    logger.info(f"ğŸ” Ollama ì‘ë‹µ ì‹œì‘ë¶€ (300ì): {response[:300]!r}")
                    logger.info(f"ğŸ” Ollama ì‘ë‹µ ëë¶€ (300ì): {response[-300:]!r}")
                except Exception as e:
                    logger.error(f"âŒ Ollama í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e}")
                    import traceback
                    logger.error(f"âŒ ì˜ˆì™¸ ìƒì„¸: {traceback.format_exc()}")
                    raise e
                base_dir_provider = "ollama"
                base_url_meta = ollama_url
            elif provider == "openai":
                response = self._call_openai_chat(prompt, openai_conf)
                base_dir_provider = "openai"
                base_url_meta = openai_conf.get("base_url", "https://api.openai.com/v1")
            elif provider == "gemini":
                response = self._call_gemini_generate(prompt, gemini_conf)
                base_dir_provider = "gemini"
                base_url_meta = gemini_conf.get("base_url", "https://generativelanguage.googleapis.com")
            else:
                return self._fallback_structure_analysis_with_llm_attempt(text, file_extension, f"Unsupported provider: {provider}")
            
            logger.info(f"ğŸ“¥ LLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ (ê¸¸ì´: {len(response)} ë¬¸ì)")
            
            # í”„ë¡¬í”„íŠ¸/ì‘ë‹µ ë¡œê¹… (ê²°ê³¼ íŒŒì¼ë“¤ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì—)
            base_dir = "tests/debug_outputs/llm"  # ê¸°ë³¸ê°’
            try:
                from services.document_parser_service import DocumentParserService
                absolute_path = self.get_absolute_path(file_path) if isinstance(file_path, str) else file_path
                parser_service = DocumentParserService()
                output_dir = parser_service.get_output_directory(absolute_path)
                base_dir = str(output_dir)
            except Exception:
                pass  # ê¸°ë³¸ê°’ ì‚¬ìš©
                
            log_prompt_and_response(
                label="document_structure_analysis",
                provider=base_dir_provider,
                model=model_name,
                prompt=prompt,
                response=response,
                logger=logger,
                base_dir=base_dir,
                meta={
                    "base_url": base_url_meta,
                    "temperature": 0.2,
                    "file_extension": file_extension,
                    "text_length": len(text),
                    "truncated_length": len(truncated_text)
                }
            )
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # JSON ì‘ë‹µ ì¶”ì¶œ
                json_response = self._extract_json_from_response(response)
                if json_response:
                    # ê¸°ë³¸ êµ¬ì¡° ë¶„ì„ê³¼ ë³‘í•©
                    basic_structure = self.analyze_document_structure(text, file_extension)
                    
                    # LLM ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                    enhanced_structure = {
                        **basic_structure,
                        "llm_analysis": json_response,
                        "analysis_method": "llm_enhanced",
                        "llm_model": model_name,
                        "llm_success": True
                    }
                    
                    # LLMì—ì„œ ì¶”ì¶œí•œ êµ¬ì¡° ì •ë³´ë¡œ ê¸°ë³¸ ë¶„ì„ ë³´ì™„
                    if "sections" in json_response:
                        enhanced_structure["llm_detected_sections"] = json_response["sections"]
                    
                    if "document_type" in json_response:
                        enhanced_structure["document_type"] = json_response["document_type"]
                    
                    if "main_topics" in json_response:
                        enhanced_structure["main_topics"] = json_response["main_topics"]
                    
                    logger.info("âœ… LLM ê¸°ë°˜ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì„±ê³µ")
                    return enhanced_structure
                else:
                    raise ValueError("JSON ì‘ë‹µì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ")
                    
            except Exception as parse_error:
                logger.error(f"âŒ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {parse_error}")
                logger.debug(f"ğŸ“„ ë¬¸ì œê°€ ëœ ì‘ë‹µ: {response[:500]}")
                return self._fallback_structure_analysis_with_llm_attempt(text, file_extension, str(parse_error))
                
        except Exception as e:
            logger.error(f"âŒ LLM êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._fallback_structure_analysis_with_llm_attempt(text, file_extension, str(e))

    def _call_openai_chat(self, prompt: str, conf: Dict[str, Any]) -> str:
        """OpenAI Chat Completions í˜¸ì¶œ (ë‹¨ìˆœ ë¬¸ìì—´ ì‘ë‹µ)."""
        import requests
        import logging
        logger = logging.getLogger(__name__)
        api_key = conf.get("api_key")
        base_url = conf.get("base_url", "https://api.openai.com/v1")
        model = conf.get("model", "gpt-3.5-turbo")
        max_tokens = conf.get("max_tokens", 8000)
        temperature = conf.get("temperature", 0.2)
        if not api_key:
            raise RuntimeError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        url = f"{base_url}/chat/completions"
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
        timeout = conf.get("timeout", 120)
        r = requests.post(url, headers=headers, json=payload, timeout=timeout)
        r.raise_for_status()
        data = r.json()
        return data["choices"][0]["message"]["content"]

    def _call_gemini_generate(self, prompt: str, conf: Dict[str, Any]) -> str:
        """Google Gemini GenerateContent í˜¸ì¶œ (v1beta REST) with streaming support."""
        import requests
        import logging
        import json
        logger = logging.getLogger(__name__)
        api_key = conf.get("api_key")
        base_url = conf.get("base_url", "https://generativelanguage.googleapis.com")
        model = conf.get("model", "models/gemini-1.5-pro")
        max_tokens = conf.get("max_tokens", 8000)
        temperature = conf.get("temperature", 0.2)
        if not api_key:
            raise RuntimeError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # ìŠ¤íŠ¸ë¦¼ í™œì„±í™”
        url = f"{base_url}/v1beta/{model}:streamGenerateContent?key={api_key}"
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens
            }
        }

        response_mime_type = conf.get("response_mime_type")
        if response_mime_type:
            payload["generationConfig"]["responseMimeType"] = response_mime_type

        timeout = conf.get("timeout", 120)

        # ìŠ¤íŠ¸ë¦¼ vs ë¹„ìŠ¤íŠ¸ë¦¼ ìë™ ì„ íƒ
        try:
            # ë¨¼ì € ìŠ¤íŠ¸ë¦¼ ì‹œë„
            logger.info(f"ğŸ“¡ Gemini ìŠ¤íŠ¸ë¦¼ ìš”ì²­ ì‹œì‘ - ëª¨ë¸: {model}")
            stream_url = f"{base_url}/v1beta/{model}:streamGenerateContent?key={api_key}&alt=sse"

            headers = {
                'Content-Type': 'application/json',
                'Accept': 'text/event-stream'
            }

            collected_response = []
            chunk_count = 0

            with requests.post(stream_url, json=payload, headers=headers, timeout=timeout, stream=True) as r:
                if r.status_code != 200:
                    error_text = r.text if hasattr(r, 'text') else "ì‘ë‹µ ë³¸ë¬¸ ì—†ìŒ"
                    logger.error(f"âŒ ìŠ¤íŠ¸ë¦¼ ìš”ì²­ ì‹¤íŒ¨ ({r.status_code}): {error_text[:500]}")
                    logger.warning(f"âš ï¸ ìŠ¤íŠ¸ë¦¼ ìš”ì²­ ì‹¤íŒ¨ ({r.status_code}), í´ë°± ì‹œë„...")
                    return self._call_gemini_generate_fallback(prompt, conf)

                for line in r.iter_lines():
                    if line:
                        line_str = line.decode('utf-8')
                        logger.debug(f"ğŸ” ìŠ¤íŠ¸ë¦¼ ë¼ì¸: {line_str}")

                        # SSE ë°ì´í„° íŒŒì‹±
                        if line_str.startswith('data: '):
                            data_str = line_str[6:]  # 'data: ' ì œê±°

                            if data_str.strip() == '[DONE]':
                                logger.info("ğŸ Gemini Stream ì™„ë£Œ")
                                break

                            try:
                                chunk_data = json.loads(data_str)
                                candidates = chunk_data.get('candidates', [])
                                if candidates:
                                    content = candidates[0].get('content', {})
                                    parts = content.get('parts', [])
                                    for part in parts:
                                        if 'text' in part:
                                            chunk_text = part['text']
                                            collected_response.append(chunk_text)
                                            chunk_count += 1

                                            # ì¤‘ê°„ ë¡œê¹… (10ê°œ ì²­í¬ë§ˆë‹¤)
                                            if chunk_count % 10 == 0:
                                                total_text = ''.join(collected_response)
                                                logger.info(f"ğŸ“ Gemini Stream ì§„í–‰ ì¤‘: {chunk_count}ê°œ ì²­í¬, {len(total_text)}ì ìˆ˜ì‹ ")
                                                logger.debug(f"í˜„ì¬ê¹Œì§€ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {total_text[:200]}...")

                            except json.JSONDecodeError as e:
                                logger.warning(f"âš ï¸ ìŠ¤íŠ¸ë¦¼ ì²­í¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
                                continue

            final_response = ''.join(collected_response)

            if final_response:
                logger.info(f"âœ… Gemini ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ: {chunk_count}ê°œ ì²­í¬, ì´ {len(final_response)}ì")
                return final_response
            else:
                logger.warning("âš ï¸ ìŠ¤íŠ¸ë¦¼ì—ì„œ ë¹ˆ ì‘ë‹µ, í´ë°± ì‹œë„...")
                return self._call_gemini_generate_fallback(prompt, conf)

        except Exception as e:
            logger.error(f"âŒ Gemini ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            logger.info("ğŸ”„ ê¸°ë³¸ ìƒì„± ë°©ì‹ìœ¼ë¡œ í´ë°±...")
            return self._call_gemini_generate_fallback(prompt, conf)

    def _call_gemini_generate_fallback(self, prompt: str, conf: Dict[str, Any]) -> str:
        """Gemini ê¸°ë³¸ ìƒì„± ë°©ì‹ (ìŠ¤íŠ¸ë¦¼ ì—†ìŒ)"""
        import requests
        import logging
        logger = logging.getLogger(__name__)

        api_key = conf.get("api_key")
        base_url = conf.get("base_url", "https://generativelanguage.googleapis.com")
        model = conf.get("model", "models/gemini-1.5-pro")
        max_tokens = conf.get("max_tokens", 8000)
        temperature = conf.get("temperature", 0.2)

        logger.info(f"ğŸ“¡ Gemini í´ë°± ìš”ì²­ ì‹œì‘ - ëª¨ë¸: {model}")

        # ê¸°ë³¸ ìƒì„± ì—”ë“œí¬ì¸íŠ¸
        url = f"{base_url}/v1beta/{model}:generateContent?key={api_key}"
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens
            }
        }

        response_mime_type = conf.get("response_mime_type")
        if response_mime_type:
            payload["generationConfig"]["responseMimeType"] = response_mime_type

        timeout = conf.get("timeout", 120)
        try:
            r = requests.post(url, json=payload, timeout=timeout)
            logger.info(f"ğŸ“Š í´ë°± ì‘ë‹µ ìƒíƒœ: {r.status_code}")

            if r.status_code != 200:
                error_text = r.text[:500] if r.text else "ì‘ë‹µ ë³¸ë¬¸ ì—†ìŒ"
                logger.error(f"âŒ í´ë°± ìš”ì²­ ì‹¤íŒ¨ ({r.status_code}): {error_text}")
                return ""

            r.raise_for_status()
            data = r.json()
            logger.debug(f"ğŸ“¥ í´ë°± ì‘ë‹µ ë°ì´í„°: {str(data)[:300]}...")

            candidates = data.get("candidates", [])
            if not candidates:
                logger.warning("âš ï¸ í´ë°± ì‘ë‹µì— candidates ì—†ìŒ")
                logger.debug(f"ì „ì²´ ì‘ë‹µ: {data}")
                return ""

            parts = candidates[0].get("content", {}).get("parts", [])
            result = "\n".join(part.get("text", "") for part in parts if isinstance(part, dict))

            logger.info(f"âœ… Gemini í´ë°± ì™„ë£Œ - {len(result)}ì")
            return result

        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ í´ë°± ìš”ì²­ ì˜ˆì™¸: {e}")
            return ""
        except Exception as e:
            logger.error(f"âŒ í´ë°± ì²˜ë¦¬ ì˜ˆì™¸: {e}")
            return ""
    
    def _extract_json_from_response(self, response: str) -> Optional[Dict[str, Any]]:
        """LLM ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ì„ ì¶”ì¶œ (ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ì— íŠ¹í™”)"""
        import json
        import re
        import logging

        logger = logging.getLogger(__name__)

        logger.info(f"ğŸ” JSON ì¶”ì¶œ ì‹œì‘ - ì‘ë‹µ ê¸¸ì´: {len(response)}ì")
        logger.info(f"ğŸ” ì‘ë‹µ ì‹œì‘ë¶€ (200ì): {response[:200]!r}")
        logger.info(f"ğŸ” ì‘ë‹µ ëë¶€ (200ì): {response[-200:]!r}")

        # 1. JSON ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„)
        json_text = None

        # ë°©ë²• 1: ```json ... ``` ë¸”ë¡ (ê°•í™”ëœ íŒ¨í„´)
        json_patterns = [
            r'```json\s*(.*?)\s*```',           # ê¸°ë³¸ json ë¸”ë¡
            r'```JSON\s*(.*?)\s*```',           # ëŒ€ë¬¸ì JSON
            r'```\s*json\s*(.*?)\s*```',        # json ì•ì— ê³µë°±
            r'```\s*{.*?}\s*```',               # ì¤‘ê´„í˜¸ë¡œ ì‹œì‘í•˜ëŠ” ë¸”ë¡
        ]

        for pattern in json_patterns:
            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
            if match:
                json_text = match.group(1).strip()
                logger.debug(f"ğŸ“ JSON ì½”ë“œ ë¸”ë¡ì—ì„œ ì¶”ì¶œ (íŒ¨í„´: {pattern[:20]}...)")
                break

        # ë°©ë²• 2: ``` ... ``` ì¼ë°˜ ë¸”ë¡
        if not json_text and "```" in response:
            pattern = r'```\s*(.*?)\s*```'
            match = re.search(pattern, response, re.DOTALL)
            if match:
                candidate = match.group(1).strip()
                # JSON ê°™ì€ ë‚´ìš©ì¸ì§€ í™•ì¸
                if candidate.startswith('{') and candidate.endswith('}'):
                    json_text = candidate
                    logger.debug("ğŸ“ ì¼ë°˜ ì½”ë“œ ë¸”ë¡ì—ì„œ ì¶”ì¶œ")

        # ë°©ë²• 3: ì¤‘ê´„í˜¸ ë§¤ì¹­ (ë³µì¡í•œ JSON êµ¬ì¡° ì§€ì›)
        if not json_text:
            # documentInfoë‚˜ structureAnalysis í‚¤ë¥¼ ì°¾ì•„ì„œ ì‹œì‘ì  ê²°ì •
            start_patterns = [
                r'\{\s*"documentInfo"',
                r'\{\s*"structureAnalysis"',
                r'\{\s*"coreContent"',
                r'\{\s*"metaInfo"'
            ]

            start_pos = -1
            needs_opening_brace = False

            for pattern in start_patterns:
                match = re.search(pattern, response)
                if match:
                    start_pos = match.start()
                    break

            # ì¤‘ê´„í˜¸ê°€ ì—†ëŠ” ê²½ìš° ì£¼ìš” í•„ë“œë¥¼ ì°¾ì•„ì„œ ì‹œì‘ì  ê²°ì •
            if start_pos == -1:
                field_patterns = [
                    r'"documentInfo"',
                    r'"structureAnalysis"',
                    r'"coreContent"',
                    r'"metaInfo"'
                ]
                for pattern in field_patterns:
                    match = re.search(pattern, response)
                    if match:
                        # í•„ë“œ ì•ì—ì„œ ê°œí–‰/ê³µë°±ì„ ì°¾ì•„ ê·¸ ì§€ì ì„ ì‹œì‘ì ìœ¼ë¡œ ì„¤ì •
                        field_start = match.start()
                        # í•„ë“œ ì•ì˜ ê³µë°±/ê°œí–‰ì„ ì°¾ì•„ì„œ ì‹œì‘ì  ì„¤ì •
                        line_start = response.rfind('\n', 0, field_start)
                        if line_start != -1:
                            # ê°œí–‰ í›„ ê³µë°±ì„ ë¬´ì‹œí•˜ê³  ì‹œì‘ì  ì„¤ì •
                            while line_start + 1 < len(response) and response[line_start + 1] in ' \t':
                                line_start += 1
                            start_pos = line_start + 1
                        else:
                            start_pos = field_start
                        needs_opening_brace = True
                        logger.info(f"ğŸ”§ ì¤‘ê´„í˜¸ ì—†ëŠ” JSON ê°ì§€: '{pattern}' í•„ë“œë¶€í„° ì‹œì‘, start_pos={start_pos}")
                        break

            if start_pos == -1:
                # ì²« ë²ˆì§¸ { ì°¾ê¸°
                start_pos = response.find('{')

            if start_pos != -1:
                # ì¤‘ê´„í˜¸ ê· í˜• ë§ì¶”ê¸°ë¡œ ëì  ì°¾ê¸°
                # needs_opening_braceê°€ Trueë©´ ì‹œì‘ ì¤‘ê´„í˜¸ê°€ ì—†ìœ¼ë¯€ë¡œ ì¹´ìš´íŠ¸ë¥¼ 1ë¡œ ì‹œì‘
                brace_count = 1 if needs_opening_brace else 0
                end_pos = -1
                in_string = False
                escape_next = False

                for i in range(start_pos, len(response)):
                    char = response[i]

                    if escape_next:
                        escape_next = False
                        continue

                    if char == '\\':
                        escape_next = True
                        continue

                    if char == '"' and not escape_next:
                        in_string = not in_string
                        continue

                    if not in_string:
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                end_pos = i + 1
                                break

                if end_pos > start_pos:
                    json_text = response[start_pos:end_pos]
                    # ì¤‘ê´„í˜¸ê°€ ëˆ„ë½ëœ ê²½ìš° ì¶”ê°€
                    if needs_opening_brace and not json_text.strip().startswith('{'):
                        json_text = '{' + json_text
                        logger.info(f"ğŸ”§ ëˆ„ë½ëœ ì‹œì‘ ì¤‘ê´„í˜¸ ì¶”ê°€: {json_text[:100]}...")
                    logger.debug("ğŸ“ ì¤‘ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ ì¶”ì¶œ")
                else:
                    # ë‹«ëŠ” ì¤‘ê´„í˜¸ë¥¼ ì°¾ì§€ ëª»í–ˆì§€ë§Œ ì£¼ìš” í•„ë“œì—ì„œ ì‹œì‘í•œ ê²½ìš°
                    # ë‚¨ì€ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ê³  ëˆ„ë½ëœ ì¤‘ê´„í˜¸ë¥¼ ë³´ì •
                    if needs_opening_brace:
                        candidate = response[start_pos:].strip()
                        if not candidate.startswith('{'):
                            candidate = '{' + candidate
                            logger.info("ğŸ”§ ëˆ„ë½ëœ ì‹œì‘ ì¤‘ê´„í˜¸ ë³´ì • ì¶”ê°€")
                        # ê´„í˜¸ ê· í˜• ë§ì¶”ê¸°
                        open_braces = candidate.count('{')
                        close_braces = candidate.count('}')
                        if close_braces < open_braces:
                            candidate = candidate + ('}' * (open_braces - close_braces))
                            logger.info(f"ğŸ”§ ëˆ„ë½ëœ ë‹«ëŠ” ì¤‘ê´„í˜¸ {open_braces - close_braces}ê°œ ì¶”ê°€")
                        json_text = candidate
                        logger.debug("ğŸ“ ì¤‘ê´„í˜¸ ì¢…ê²° ë³´ì •ìœ¼ë¡œ ì¶”ì¶œ")

        # ë°©ë²• 4: ìµœí›„ì˜ ìˆ˜ë‹¨ - ì „ì²´ í…ìŠ¤íŠ¸
        if not json_text:
            json_text = response.strip()
            logger.debug("ğŸ“ ì „ì²´ ì‘ë‹µ ì‚¬ìš©")

        # JSON ì¶”ì¶œ ê²°ê³¼ ë¡œê¹…
        if json_text:
            logger.debug(f"ğŸ“ JSON ì¶”ì¶œ ì„±ê³µ - ê¸¸ì´: {len(json_text)}ì, ë°©ë²•: {'ì½”ë“œë¸”ë¡' if '```' in response else 'ì¤‘ê´„í˜¸ë§¤ì¹­' if json_text != response.strip() else 'ì „ì²´ì‘ë‹µ'}")
            logger.debug(f"ğŸ“ ì¶”ì¶œëœ JSON ì‹œì‘: {json_text[:200]}")
        else:
            logger.error("âŒ JSON ì¶”ì¶œ ì‹¤íŒ¨ - ëª¨ë“  ë°©ë²•ìœ¼ë¡œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            logger.debug(f"ğŸ“ ì›ë³¸ ì‘ë‹µ ì‹œì‘ 200ì: {response[:200]}")
            logger.debug(f"ğŸ“ ì›ë³¸ ì‘ë‹µ ë 200ì: {response[-200:]}")
            return None

        # JSON ì •ë¦¬
        if json_text:
            # ì•ë’¤ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
            json_text = json_text.strip()

            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œíœìŠ¤ ì œê±° (í˜¹ì‹œ ë‚¨ì•„ìˆì„ ê²½ìš°)
            json_text = re.sub(r'^```json\s*', '', json_text, flags=re.IGNORECASE)
            json_text = re.sub(r'^```JSON\s*', '', json_text, flags=re.IGNORECASE)
            json_text = re.sub(r'\s*```$', '', json_text)
            json_text = re.sub(r'^```\s*', '', json_text)

            # ë¬¸ì„œ ì‹œì‘/ëì˜ BOM ë° ê°œí–‰ ì œì–´ ë¬¸ì ì •ê·œí™”
            if json_text.startswith('\ufeff'):
                json_text = json_text.lstrip('\ufeff')
            json_text = json_text.replace('\u2028', '\\u2028').replace('\u2029', '\\u2029')

            # Gemini íŠ¹í™”: ì„¤ëª… í…ìŠ¤íŠ¸ ì œê±°ëŠ”
            # JSONì´ '{'ë¡œ ë°”ë¡œ ì‹œì‘í•˜ì§€ ì•Šì„ ë•Œë§Œ ì ìš©í•˜ì—¬
            # ì •ìƒ JSON ë‚´ë¶€ ë¬¸ìì—´ì„ ì˜ëª» ì˜ë¼ë‚´ì§€ ì•Šë„ë¡ í•¨
            if not json_text.lstrip().startswith('{'):
                explanation_patterns = [
                    r'^[^{]*?(ë‹¤ìŒì€|ê²°ê³¼ëŠ”|ë¶„ì„|êµ¬ì¡°|JSON)\s*:?\s*\n*\s*{',
                    r'^[^{]*?(Here is|The result|Analysis|Structure|JSON)\s*:?\s*\n*\s*{',
                    r'^.*?ë¶„ì„.*?ê²°ê³¼.*?\n*\s*{',
                    r'^.*?êµ¬ì¡°.*?ë¶„ì„.*?\n*\s*{'
                ]

                for pattern in explanation_patterns:
                    match = re.search(pattern, json_text, re.IGNORECASE | re.DOTALL)
                    if match:
                        # ì„¤ëª… ë¶€ë¶„ì„ ì œê±°í•˜ê³  { ë¶€í„° ì‹œì‘
                        start_pos = match.end() - 1  # { ë¬¸ì í¬í•¨
                        json_text = json_text[start_pos:]
                        logger.debug("ğŸ“ ì„¤ëª… í…ìŠ¤íŠ¸ ì œê±°")
                        break

            # ë¶ˆì™„ì „í•œ JSON ìˆ˜ì • ì‹œë„
            if not json_text.endswith('}') and json_text.count('{') > json_text.count('}'):
                missing_braces = json_text.count('{') - json_text.count('}')
                json_text += '}' * missing_braces
                logger.debug(f"ğŸ“ ëˆ„ë½ëœ ì¤‘ê´„í˜¸ {missing_braces}ê°œ ì¶”ê°€")

        try:
            # ê¸°ë³¸ JSON íŒŒì‹± ì‹œë„
            logger.debug(f"ğŸ“ JSON íŒŒì‹± ì‹œë„ - ê¸¸ì´: {len(json_text)}ì")
            logger.info(f"ğŸ” íŒŒì‹±í•  JSON ë‚´ìš© (ì²« 200ì): {json_text[:200]!r}")
            return json.loads(json_text)
        except json.JSONDecodeError as e:
            logger.error(f"âŒ ê¸°ë³¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            logger.debug(f"ğŸ“ ë¬¸ì œê°€ ëœ JSON ì•ë¶€ë¶„(500ì): {json_text[:500]}")
            logger.debug(f"ğŸ“ ë¬¸ì œê°€ ëœ JSON ë’·ë¶€ë¶„(500ì): {json_text[-500:]}")

            try:
                # ëŒ€ì•ˆ 1: JSON5 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹œë„ (ë” ê´€ëŒ€í•œ íŒŒì‹±)
                try:
                    import json5
                    logger.debug("ğŸ”§ JSON5 íŒŒì‹± ì‹œë„")
                    result = json5.loads(json_text)
                    logger.debug("âœ… JSON5 íŒŒì‹± ì„±ê³µ")
                    return result
                except ImportError:
                    logger.debug("âš ï¸ json5 ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                except Exception as e_json5:
                    logger.debug(f"âŒ JSON5 íŒŒì‹± ì‹¤íŒ¨: {e_json5}")

                # ëŒ€ì•ˆ 2: ê°•í™”ëœ ìì²´ ìˆ˜ì • ì‹œë„
                try:
                    logger.debug("ğŸ”§ ê°•í™”ëœ JSON ìˆ˜ì • ì‹œë„")
                    fixed_json = self._aggressive_json_repair(json_text)
                    if fixed_json != json_text:
                        logger.debug("âœ… JSON êµ¬ì¡° ìˆ˜ì • ì™„ë£Œ")
                        result = json.loads(fixed_json)
                        logger.debug("âœ… ìˆ˜ì •ëœ JSON íŒŒì‹± ì„±ê³µ")
                        return result
                except json.JSONDecodeError as e_aggressive:
                    logger.debug(f"âŒ ê°•í™”ëœ ìˆ˜ì •ë„ ì‹¤íŒ¨: {e_aggressive}")
                except Exception as e_repair:
                    logger.debug(f"âŒ JSON ìˆ˜ì • ì¤‘ ì˜¤ë¥˜: {e_repair}")

                # ëŒ€ì•ˆ 3: ê¸°ì¡´ ìˆ˜ì • ë°©ì‹
                logger.debug("ğŸ”§ ê¸°ì¡´ JSON ìˆ˜ì • ì‹œë„")
                cleaned_json = self._repair_json(json_text)
                logger.debug(f"ğŸ“ ìˆ˜ì •ëœ JSON ì•ë¶€ë¶„(300ì): {cleaned_json[:300]}")
                return json.loads(cleaned_json)

            except json.JSONDecodeError as e2:
                logger.error(f"âŒ ìˆ˜ì • í›„ì—ë„ JSON íŒŒì‹± ì‹¤íŒ¨: {e2}")
                logger.debug(f"ğŸ“ ìµœì¢… ì‹¤íŒ¨í•œ JSON ì•ë¶€ë¶„(200ì): {cleaned_json[:200] if 'cleaned_json' in locals() else 'N/A'}")
                return None
            except Exception as e3:
                logger.error(f"âŒ JSON ìˆ˜ì • ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e3}")
                return None
    
    def _repair_json(self, json_text: str) -> str:
        """JSON ìˆ˜ì • (ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ì— íŠ¹í™”)"""
        import re
        import logging

        logger = logging.getLogger(__name__)

        # 1. ê¸°ë³¸ì ì¸ ë¬¸ì ìˆ˜ì •
        # ìŠ¤ë§ˆíŠ¸ ì¸ìš©ë¶€í˜¸ë¥¼ ASCIIë¡œ ë³€í™˜
        json_text = json_text.replace(""", '"').replace(""", '"').replace("'", "'")
        json_text = json_text.replace("'", "'").replace("`", "'")

        # 2. ì¸ìš©ë¶€í˜¸ ìˆ˜ì •
        # ë‹¨ì¼ ì¸ìš©ë¶€í˜¸ë¥¼ ì´ì¤‘ ì¸ìš©ë¶€í˜¸ë¡œ ë³€í™˜ (í‚¤)
        json_text = re.sub(r"'([^']*)':", r'"\1":', json_text)
        # ë‹¨ì¼ ì¸ìš©ë¶€í˜¸ë¥¼ ì´ì¤‘ ì¸ìš©ë¶€í˜¸ë¡œ ë³€í™˜ (ê°’)
        json_text = re.sub(r":\s*'([^']*)'", r': "\1"', json_text)

        # 3. ë°°ì—´ ë‚´ ë‹¨ì¼ ì¸ìš©ë¶€í˜¸ ìˆ˜ì •
        json_text = re.sub(r'\[\s*\'([^\']*)\'\s*\]', r'["\1"]', json_text)
        json_text = re.sub(r',\s*\'([^\']*)\'\s*(?=[,\]])', r', "\1"', json_text)

        # 4. í‚¤-ê°’ ìŒì—ì„œ í‚¤ê°€ ì¸ìš©ë¶€í˜¸ ì—†ì´ ìˆëŠ” ê²½ìš° ìˆ˜ì •
        json_text = re.sub(r'([{,]\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', json_text)

        # 5. ëì— ë¶™ì€ ì‰¼í‘œ ì œê±°
        json_text = re.sub(r",\s*([}\]])", r"\1", json_text)

        # 6. ì¤„ë°”ê¿ˆ/íƒ­ ë¬¸ì ì²˜ë¦¬ (ë¬¸ìì—´ ë‚´ë¶€ë§Œ ì•ˆì „í•˜ê²Œ ì¹˜í™˜)
        def _escape_in_strings(s: str) -> str:
            result = []
            in_string = False
            escape_next = False
            for ch in s:
                if escape_next:
                    result.append(ch)
                    escape_next = False
                    continue
                if ch == '\\':
                    result.append(ch)
                    escape_next = True
                    continue
                if ch == '"':
                    result.append(ch)
                    in_string = not in_string
                    continue
                if in_string:
                    if ch == '\n':
                        result.append('\\n')
                        continue
                    if ch == '\r':
                        result.append('\\r')
                        continue
                    if ch == '\t':
                        result.append('\\t')
                        continue
                result.append(ch)
            return ''.join(result)

        json_text = _escape_in_strings(json_text)

        # 7. ì œì–´ ë¬¸ì ì œê±° ë° ì¹˜í™˜ (ê°•í™”)
        # Invalid control character ì˜¤ë¥˜ í•´ê²°
        def clean_control_chars(text: str) -> str:
            result = []
            in_string = False
            escape_next = False

            for ch in text:
                if escape_next:
                    result.append(ch)
                    escape_next = False
                    continue

                if ch == '\\':
                    result.append(ch)
                    escape_next = True
                    continue

                if ch == '"':
                    result.append(ch)
                    in_string = not in_string
                    continue

                # ì œì–´ ë¬¸ì ì²˜ë¦¬
                if ord(ch) < 32:  # ì œì–´ ë¬¸ì
                    if in_string:
                        # ë¬¸ìì—´ ë‚´ë¶€ì˜ ì œì–´ ë¬¸ìëŠ” ì´ìŠ¤ì¼€ì´í”„
                        if ch == '\n':
                            result.append('\\n')
                        elif ch == '\r':
                            result.append('\\r')
                        elif ch == '\t':
                            result.append('\\t')
                        else:
                            # ê¸°íƒ€ ì œì–´ ë¬¸ìëŠ” ì œê±°
                            continue
                    else:
                        # ë¬¸ìì—´ ì™¸ë¶€ì˜ ì œì–´ ë¬¸ìëŠ” ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
                        if ch in '\n\r\t':
                            result.append(ch)  # ê°œí–‰/íƒ­ì€ ìœ ì§€
                        else:
                            result.append(' ')  # ê¸°íƒ€ ì œì–´ ë¬¸ìëŠ” ê³µë°±ìœ¼ë¡œ
                else:
                    result.append(ch)

            return ''.join(result)

        json_text = clean_control_chars(json_text)

        # 8. Unterminated string ìˆ˜ì • - ê³ ê¸‰ ë¬¸ìì—´ ê· í˜• ê²€ì‚¬
        def fix_unterminated_strings(text: str) -> str:
            lines = text.split('\n')
            fixed_lines = []

            for i, line in enumerate(lines):
                # ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ ë”°ì˜´í‘œ ê°œìˆ˜ ê³„ì‚°
                quote_positions = []
                escape_next = False

                for j, ch in enumerate(line):
                    if escape_next:
                        escape_next = False
                        continue
                    if ch == '\\':
                        escape_next = True
                        continue
                    if ch == '"':
                        quote_positions.append(j)

                # í™€ìˆ˜ê°œì˜ ë”°ì˜´í‘œê°€ ìˆìœ¼ë©´ unterminated string
                if len(quote_positions) % 2 == 1:
                    # ë§ˆì§€ë§‰ ë”°ì˜´í‘œ ìœ„ì¹˜ ì°¾ê¸°
                    last_quote_pos = quote_positions[-1]

                    # ì¤„ ëê¹Œì§€ì˜ ë‚´ìš© í™•ì¸
                    remaining = line[last_quote_pos + 1:].strip()

                    # ë‹¤ìŒ ì¤„ì´ ìƒˆë¡œìš´ í‚¤/ê°’ìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
                    is_end_of_value = False
                    if i < len(lines) - 1:
                        next_line = lines[i + 1].strip()
                        # ë‹¤ìŒ ì¤„ì´ ìƒˆë¡œìš´ í‚¤, ë°°ì—´ ë, ê°ì²´ ëìœ¼ë¡œ ì‹œì‘í•˜ë©´ ë¬¸ìì—´ ì¢…ë£Œ
                        if (next_line.startswith('"') and ':' in next_line) or \
                           next_line.startswith('}') or next_line.startswith(']') or \
                           next_line.startswith(','):
                            is_end_of_value = True
                    else:
                        # ë§ˆì§€ë§‰ ì¤„ì´ë©´ ì¢…ë£Œ
                        is_end_of_value = True

                    if is_end_of_value:
                        # ë¬¸ìì—´ ë‚´ìš©ì— ë¶ˆì™„ì „í•œ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ì œê±°í•˜ê³  ë”°ì˜´í‘œ ì¶”ê°€
                        if remaining and not remaining.endswith((',', '}', ']')):
                            # ë¶ˆì™„ì „í•œ ëë¶€ë¶„ ì œê±° (ë§ˆì§€ë§‰ ë‹¨ì–´/ë¬¸ì ì œê±°)
                            content = line[:last_quote_pos + 1]
                            if remaining:
                                # ë§ˆì§€ë§‰ ì™„ì „í•œ ë‹¨ì–´ê¹Œì§€ë§Œ ìœ ì§€
                                words = remaining.split()
                                if len(words) > 1:
                                    clean_content = ' '.join(words[:-1])
                                    content += clean_content
                            line = content + '"'
                        else:
                            line = line.rstrip() + '"'

                        logger.debug(f"ğŸ“ Unterminated string ìˆ˜ì •: ì¤„ {i+1}")

                fixed_lines.append(line)

            return '\n'.join(fixed_lines)

        json_text = fix_unterminated_strings(json_text)

        # 8. Extra data ì˜¤ë¥˜ í•´ê²° - JSON ë’¤ì˜ ì¶”ê°€ ë°ì´í„° ì œê±°
        # ì²« ë²ˆì§¸ ì™„ì „í•œ JSON ê°ì²´ë§Œ ì¶”ì¶œ
        brace_count = 0
        json_end = -1
        in_string = False
        escape_next = False

        for i, char in enumerate(json_text):
            if escape_next:
                escape_next = False
                continue

            if char == '\\':
                escape_next = True
                continue

            if char == '"' and not escape_next:
                in_string = not in_string
                continue

            if not in_string:
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        json_end = i + 1
                        break

        if json_end > 0 and json_end < len(json_text):
            original_length = len(json_text)
            json_text = json_text[:json_end]
            logger.debug(f"ğŸ“ Extra data ì œê±°: {original_length - len(json_text)}ì ì‚­ì œ")

        # 9. ë¹ˆ í‚¤ì›Œë“œ/ë¶„ë¥˜ ë°°ì—´ ìˆ˜ì •
        json_text = re.sub(r'"keywords"\s*:\s*\[\s*\]', '"keywords": []', json_text)
        json_text = re.sub(r'"classificationTags"\s*:\s*\[\s*\]', '"classificationTags": []', json_text)

        # 10. ë¶ˆì™„ì „í•œ ê°ì²´ ìˆ˜ì •
        # í‚¤ì›Œë“œ/ë¶„ë¥˜ ê°ì²´ì— í•„ìˆ˜ í•„ë“œê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì¶”ê°€
        def fix_keyword_objects(match):
            obj = match.group(1)
            if '"name"' not in obj:
                return match.group(0)  # name í•„ë“œê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ 
            if '"desc"' not in obj:
                obj = obj.rstrip('} ') + ', "desc": ""}'
            if '"readme"' not in obj:
                obj = obj.rstrip('} ') + ', "readme": ""}'
            return '{"' + obj

        json_text = re.sub(r'\{([^{}]*"name"[^{}]*)\}', fix_keyword_objects, json_text)

        # 11. ë¶ˆì™„ì „í•œ JSON ë§ˆë¬´ë¦¬
        if json_text.count('{') > json_text.count('}'):
            missing_braces = json_text.count('{') - json_text.count('}')
            json_text += '}' * missing_braces
            logger.debug(f"ğŸ“ JSON ìˆ˜ì •: ëˆ„ë½ëœ ì¤‘ê´„í˜¸ {missing_braces}ê°œ ì¶”ê°€")

        if json_text.count('[') > json_text.count(']'):
            missing_brackets = json_text.count('[') - json_text.count(']')
            json_text += ']' * missing_brackets
            logger.debug(f"ğŸ“ JSON ìˆ˜ì •: ëˆ„ë½ëœ ëŒ€ê´„í˜¸ {missing_brackets}ê°œ ì¶”ê°€")

        return json_text.strip()

    def _aggressive_json_repair(self, json_text: str) -> str:
        """ë§¤ìš° ì ê·¹ì ì¸ JSON ìˆ˜ì • (demjson ëŒ€ì²´)"""
        import re
        import logging

        logger = logging.getLogger(__name__)
        original = json_text

        # 1. ì¸ìš©ë¶€í˜¸ ì—†ëŠ” í‚¤ ìˆ˜ì • (JavaScript ìŠ¤íƒ€ì¼)
        json_text = re.sub(r'([{,]\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":', json_text)

        # 2. ë‹¨ì¼ ì¸ìš©ë¶€í˜¸ë¥¼ ì´ì¤‘ ì¸ìš©ë¶€í˜¸ë¡œ ë³€í™˜ (ì „ì²´)
        json_text = re.sub(r"'([^'\\]*(\\.[^'\\]*)*)'", r'"\1"', json_text)

        # 3. ì˜ë¦° ë¬¸ìì—´ ë³µêµ¬ ì‹œë„ (ê°œì„ ëœ ë²„ì „)
        lines = json_text.split('\n')
        fixed_lines = []
        in_string = False
        escape_next = False

        for i, line in enumerate(lines):
            temp_in_string = in_string
            temp_escape_next = escape_next

            # ì¤„ë³„ë¡œ ë¬¸ìì—´ ìƒíƒœ ì¶”ì 
            for char in line:
                if temp_escape_next:
                    temp_escape_next = False
                    continue
                if char == '\\':
                    temp_escape_next = True
                    continue
                if char == '"':
                    temp_in_string = not temp_in_string

            # ë¬¸ìì—´ì´ ì—´ë ¤ìˆê³  ë‹¤ìŒ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¥¼ ë§Œì¡±í•˜ë©´ ë‹«ê¸°
            if temp_in_string:
                # ë§ˆì§€ë§‰ ì¤„ì´ê±°ë‚˜
                # ë‹¤ìŒ ì¤„ì´ ìƒˆë¡œìš´ í‚¤ë¡œ ì‹œì‘í•˜ê±°ë‚˜
                # ë‹¤ìŒ ì¤„ì´ ë‹«ëŠ” ê´„í˜¸ë¡œ ì‹œì‘í•˜ë©´
                if (i == len(lines) - 1 or
                    (i < len(lines) - 1 and re.match(r'^\s*["}]', lines[i+1]))):
                    line = line.rstrip() + '"'
                    temp_in_string = False
                    logger.debug(f"ğŸ“ Unterminated string ìˆ˜ì •: ì¤„ {i+1}")

            fixed_lines.append(line)
            in_string = temp_in_string
            escape_next = temp_escape_next

        json_text = '\n'.join(fixed_lines)

        # 4. ë§ˆì§€ë§‰ ì›ì†Œ ë’¤ ì‰¼í‘œ ì œê±°
        json_text = re.sub(r',(\s*[}\]])', r'\1', json_text)

        # 5. ì¤‘ë³µ ì‰¼í‘œ ì œê±°
        json_text = re.sub(r',\s*,', ',', json_text)

        # 6. JavaScript ì£¼ì„ ì œê±°
        json_text = re.sub(r'//.*$', '', json_text, flags=re.MULTILINE)
        json_text = re.sub(r'/\*.*?\*/', '', json_text, flags=re.DOTALL)

        # 7. ë¶ˆì™„ì „í•œ ë°°ì—´/ê°ì²´ ë‹«ê¸°
        open_braces = json_text.count('{') - json_text.count('}')
        open_brackets = json_text.count('[') - json_text.count(']')

        if open_braces > 0:
            json_text += '}' * open_braces
        if open_brackets > 0:
            json_text += ']' * open_brackets

        # 8. ì—°ì†ëœ ê³µë°± ì •ë¦¬
        json_text = re.sub(r'\s+', ' ', json_text)
        json_text = json_text.strip()

        if json_text != original:
            logger.debug(f"ğŸ“ ì ê·¹ì  ìˆ˜ì • ì™„ë£Œ: {len(original)}ì â†’ {len(json_text)}ì")

        return json_text

    def _fallback_structure_analysis(self, text: str, file_extension: str) -> Dict[str, Any]:
        """LLMì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ê¸°ë³¸ êµ¬ì¡° ë¶„ì„ìœ¼ë¡œ í´ë°±"""
        basic_structure = self.analyze_document_structure(text, file_extension)
        basic_structure.update({
            "analysis_method": "basic_only",
            "llm_success": False,
            "llm_error": "LLM extraction disabled"
        })
        return basic_structure
    
    def _fallback_structure_analysis_with_llm_attempt(self, text: str, file_extension: str, error_msg: str) -> Dict[str, Any]:
        """LLM ì‹¤íŒ¨ ì‹œ ì‹¤íŒ¨ ìƒíƒœë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë°˜í™˜ (ë” ì´ìƒ ê²°ê³¼ ìƒì„±í•˜ì§€ ì•ŠìŒ)"""
        import logging
        logger = logging.getLogger(__name__)

        logger.error(f"âŒ LLM êµ¬ì¡° ë¶„ì„ ì™„ì „ ì‹¤íŒ¨: {error_msg}")

        # ì‹¤íŒ¨ ìƒíƒœë§Œ ë°˜í™˜ (llm_analysis ì—†ìŒ)
        return {
            "analysis_method": "llm_failed",
            "llm_success": False,
            "llm_error": error_msg,
            "file_info": {},
            "analysis_timestamp": "",
            "source_parser": ""
            # llm_analysisëŠ” ì˜ë„ì ìœ¼ë¡œ ëˆ„ë½ - ì´ë¡œ ì¸í•´ HTTPException ë°œìƒ
        }
    
    def extract_keywords(self, content: str, extractors: Optional[List[str]] = None, filename: str = "local_analysis.txt") -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ ìˆ˜í–‰"""
        if extractors is None:
            extractors = ConfigService.get_json_config(
                self.db, "DEFAULT_EXTRACTORS", ["llm"]
            )
        
        # ExtractorManagerë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._ensure_extractor_manager().extract_keywords(content, extractors, filename)
        
        # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
        result = []
        for keyword in keywords:
            result.append({
                "keyword": keyword.keyword,
                "extractor_name": keyword.extractor_name,
                "score": keyword.score,
                "category": keyword.category,
                "start_position": keyword.start_position,
                "end_position": keyword.end_position,
                "context_snippet": keyword.context_snippet,
                "page_number": keyword.page_number,
                "line_number": keyword.line_number,
                "column_number": keyword.column_number
            })
        
        return result
    
    def analyze_file(self, file_path: str, extractors: Optional[List[str]] = None, force_reanalyze: bool = False, use_docling: bool = False) -> Dict[str, Any]:
        """íŒŒì¼ ë¶„ì„ ìˆ˜í–‰
        
        Args:
            file_path: ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ
            extractors: ì‚¬ìš©í•  ì¶”ì¶œê¸° ëª©ë¡
            force_reanalyze: ì¬ë¶„ì„ ì—¬ë¶€
            use_docling: Docling íŒŒì„œ ì‚¬ìš© ì—¬ë¶€ (PDF íŒŒì¼ì—ë§Œ ì ìš©)
        """
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë° í˜•ì‹ í™•ì¸
        if not self.file_exists(file_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        if not self.is_supported_file(file_path):
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_path}")
        
        # ê¸°ì¡´ ê²°ê³¼ í™•ì¸
        if not force_reanalyze:
            existing_result = self.load_existing_result(file_path)
            if existing_result:
                return existing_result
        
        # ì¬ë¶„ì„ì˜ ê²½ìš° ê¸°ì¡´ ê²°ê³¼ ë°±ì—…
        if force_reanalyze:
            backup_path = self.backup_existing_result(file_path)
            if backup_path:
                print(f"ê¸°ì¡´ ê²°ê³¼ë¥¼ ë°±ì—…í–ˆìŠµë‹ˆë‹¤: {backup_path}")
        
        try:
            # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
            absolute_path = self.get_absolute_path(file_path)
            
            # DocumentParserServiceë¥¼ í†µí•´ ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ í™•ì¸
            from services.document_parser_service import DocumentParserService
            parser_service = DocumentParserService()
            
            content = None
            parsing_used = "new_parsing"
            
            # ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ í™•ì¸
            if parser_service.has_parsing_results(absolute_path):
                try:
                    # ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    existing_results = parser_service.load_existing_parsing_results(absolute_path)
                    best_parser = existing_results.get("summary", {}).get("best_parser")
                    
                    if best_parser and best_parser in existing_results.get("parsing_results", {}):
                        parser_dir = parser_service.get_output_directory(absolute_path) / best_parser
                        text_file = parser_dir / f"{best_parser}_text.txt"
                        if text_file.exists():
                            with open(text_file, 'r', encoding='utf-8') as f:
                                content = f.read()
                                parsing_used = f"existing_{best_parser}"
                                print(f"âœ… ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ ì¬ì‚¬ìš©: {best_parser} ({len(content)} ë¬¸ì)")
                except Exception as e:
                    print(f"âš ï¸ ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ íŒŒì‹±: {e}")
            
            # íŒŒì‹± ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ìƒˆë¡œ íŒŒì‹±
            if not content:
                content = self.parse_file_content(file_path, use_docling=use_docling)
                parsing_used = "new_parsing"
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self.extract_keywords(content, extractors, filename=absolute_path.name)
            
            # íŒŒì¼ í†µê³„
            file_stats = absolute_path.stat()
            
            # í‚¤ì›Œë“œë¥¼ ì¶”ì¶œê¸°ë³„ë¡œ ê·¸ë£¹í™”
            grouped_keywords = {}
            for keyword in keywords:
                extractor_name = keyword["extractor_name"]
                if extractor_name not in grouped_keywords:
                    grouped_keywords[extractor_name] = []
                grouped_keywords[extractor_name].append(keyword)
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                "file_info": {
                    "path": file_path,
                    "absolute_path": str(absolute_path),
                    "size": file_stats.st_size,
                    "modified": datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
                    "extension": absolute_path.suffix.lower()
                },
                "content_info": {
                    "length": len(content),
                    "word_count": len(content.split()),
                    "line_count": len(content.splitlines())
                },
                "extraction_info": {
                    "extractors_used": extractors if extractors is not None else ConfigService.get_json_config(
                        self.db, "DEFAULT_EXTRACTORS", ["llm"]
                    ),
                    "total_keywords": len(keywords),
                    "parsing_method": parsing_used
                },
                "keywords": grouped_keywords,
                "analysis_status": "completed"
            }
            
            # ê²°ê³¼ ì €ì¥
            result_file_path = self.save_result(file_path, result)
            result["result_file"] = result_file_path
            
            return result
            
        except Exception as e:
            error_result = {
                "file_info": {
                    "path": file_path,
                    "error": str(e)
                },
                "analysis_status": "failed",
                "error_message": str(e)
            }
            
            # ì˜¤ë¥˜ë„ ì €ì¥
            try:
                self.save_result(file_path, error_result)
            except:
                pass
            
            raise e
