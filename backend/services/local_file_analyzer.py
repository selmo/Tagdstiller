"""
ë¡œì»¬ íŒŒì¼ ë¶„ì„ ì„œë¹„ìŠ¤
"""
import os
import json
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from sqlalchemy.orm import Session

from services.config_service import ConfigService
from routers.extraction import ExtractorManager
from services.parser.auto_parser import AutoParser
from utils.llm_logger import log_prompt_and_response
from services.parser_file_manager import save_parser_results, file_manager

from langchain_ollama import OllamaLLM
LANGCHAIN_AVAILABLE = True

class LocalFileAnalyzer:
    """ë¡œì»¬ íŒŒì¼ ë¶„ì„ì„ ìœ„í•œ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, db: Session):
        self.db = db
        self.extractor_manager = ExtractorManager(db)
        
    def get_file_root(self) -> str:
        """ì„¤ì •ì—ì„œ íŒŒì¼ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ê°€ì ¸ì˜¤ê³  ì—†ìœ¼ë©´ ìƒì„±"""
        root_path = ConfigService.get_config_value(self.db, "LOCAL_FILE_ROOT", "./data/uploads")
        root_dir = Path(root_path)
        
        # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
        if not root_dir.exists():
            try:
                root_dir.mkdir(parents=True, exist_ok=True)
                print(f"ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {root_dir.resolve()}")
            except Exception as e:
                print(f"ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
                # ìƒì„± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                return "."
        
        return root_path
    
    def get_absolute_path(self, file_path: str) -> Path:
        """ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜"""
        # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©
        # (change-directory ì—”ë“œí¬ì¸íŠ¸ê°€ os.chdir()ë¡œ ë³€ê²½í•œ ë””ë ‰í† ë¦¬)
        current_dir = Path.cwd()
        target_path = Path(file_path)
        
        if target_path.is_absolute():
            # ì ˆëŒ€ ê²½ë¡œì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            return target_path.resolve()
        else:
            # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ í•´ì„
            return (current_dir / target_path).resolve()
    
    def get_result_file_path(self, file_path: str) -> Path:
        """ë¶„ì„ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œë¥¼ ìƒì„± - parsing ê²°ê³¼ì™€ ê°™ì€ ë””ë ‰í† ë¦¬ì— ì €ì¥"""
        from services.document_parser_service import DocumentParserService
        
        absolute_path = self.get_absolute_path(file_path)
        parser_service = DocumentParserService()
        output_dir = parser_service.get_output_directory(absolute_path)
        result_path = output_dir / "keyword_analysis.json"
        return result_path
    
    def file_exists(self, file_path: str) -> bool:
        """íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            absolute_path = self.get_absolute_path(file_path)
            return absolute_path.exists() and absolute_path.is_file()
        except (ValueError, OSError):
            return False
    
    def is_supported_file(self, file_path: str) -> bool:
        """ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ì¸ì§€ í™•ì¸"""
        allowed_extensions = ConfigService.get_json_config(
            self.db, "ALLOWED_EXTENSIONS", [".txt", ".pdf", ".docx", ".html", ".md"]
        )
        
        file_extension = Path(file_path).suffix.lower()
        return file_extension in allowed_extensions
    
    def load_existing_result(self, file_path: str) -> Optional[Dict[str, Any]]:
        """ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""
        try:
            result_file = self.get_result_file_path(file_path)
            if result_file.exists():
                with open(result_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    def save_result(self, file_path: str, result: Dict[str, Any]) -> str:
        """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        result_file = self.get_result_file_path(file_path)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        result_file.parent.mkdir(parents=True, exist_ok=True)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        enhanced_result = {
            "file_path": file_path,
            "analysis_timestamp": datetime.now().isoformat(),
            "analyzer_version": "1.0.0",
            **result
        }
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(enhanced_result, f, ensure_ascii=False, indent=2)
        
        return str(result_file)
    
    def backup_existing_result(self, file_path: str) -> Optional[str]:
        """ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ì„ ë°±ì—…"""
        result_file = self.get_result_file_path(file_path)
        if not result_file.exists():
            return None
        
        # ë°±ì—… íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = result_file.with_suffix(f'.backup_{timestamp}.json')
        
        try:
            shutil.copy2(result_file, backup_file)
            return str(backup_file)
        except Exception as e:
            print(f"ë°±ì—… ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def parse_file_content(self, file_path: str, use_docling: bool = False) -> str:
        """íŒŒì¼ ë‚´ìš©ì„ íŒŒì‹±í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        
        Args:
            file_path: íŒŒì‹±í•  íŒŒì¼ ê²½ë¡œ
            use_docling: Docling íŒŒì„œ ì‚¬ìš© ì—¬ë¶€ (PDF íŒŒì¼ì—ë§Œ ì ìš©)
        """
        absolute_path = self.get_absolute_path(file_path)
        
        # PDF íŒŒì¼ì´ê³  use_doclingì´ Trueì¸ ê²½ìš° Docling íŒŒì„œ ì‚¬ìš©
        if use_docling and absolute_path.suffix.lower() == '.pdf':
            from services.parser.docling_parser import DoclingParser
            parser = DoclingParser()
            parse_result = parser.parse(absolute_path)
        else:
            # ê¸°ë³¸ AutoParser ì‚¬ìš©
            parser = AutoParser()
            parse_result = parser.parse(absolute_path)
        
        if not parse_result.success:
            raise ValueError(f"íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {parse_result.error_message}")
        
        return parse_result.text
    
    def extract_metadata_with_all_parsers(self, file_path: str, use_llm: bool = True, save_to_file: bool = True) -> Dict[str, Any]:
        """ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì„œë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œë„
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            use_llm: LLM ì‚¬ìš© ì—¬ë¶€
            save_to_file: íŒŒì¼ë¡œ ì €ì¥ ì—¬ë¶€
        
        Returns:
            ê° íŒŒì„œì˜ ê²°ê³¼ë¥¼ í¬í•¨í•œ í†µí•© ë©”íƒ€ë°ì´í„°
        """
        import hashlib
        from datetime import datetime
        
        absolute_path = self.get_absolute_path(file_path)
        file_stats = absolute_path.stat()
        file_size = file_stats.st_size
        
        # ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
        all_results = {
            "file_path": file_path,
            "absolute_path": str(absolute_path),
            "extraction_timestamp": datetime.now().isoformat(),
            "parsers_attempted": [],
            "parsers_results": {},
            "best_result": None,
            "file_info": {
                "size": file_size,
                "modified": datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
                "extension": absolute_path.suffix.lower()
            }
        }
        
        # PDF íŒŒì¼ì¸ ê²½ìš° ëª¨ë“  ê°œë³„ íŒŒì„œ ì‹œë„
        if absolute_path.suffix.lower() == '.pdf':
            parsers_to_try = []
            
            # ê° PDF ì—”ì§„ ì¶”ê°€
            parsers_to_try.extend([
                ("pymupdf4llm", None),
                ("pdfplumber", None),
                ("pymupdf_advanced", None),
                ("pymupdf_basic", None),
                ("pypdf2", None),
                ("docling", "docling")  # Doclingì€ íŠ¹ë³„ ì²˜ë¦¬
            ])
        else:
            parsers_to_try = [("default", None)]  # ê¸°ë³¸ íŒŒì„œë§Œ
        
        best_score = 0
        best_parser = None
        
        for parser_name, parser_type in parsers_to_try:
            try:
                print(f"ğŸ” {parser_name} íŒŒì„œë¡œ ì¶”ì¶œ ì‹œë„...")
                all_results["parsers_attempted"].append(parser_name)
                
                # Docling íŒŒì„œì¸ ê²½ìš°
                if parser_type == "docling":
                    result = self.extract_file_metadata(
                        file_path=file_path,
                        use_llm=False,
                        save_to_file=False,
                        use_docling=True
                    )
                # ê°œë³„ PDF ì—”ì§„ì¸ ê²½ìš°
                elif parser_name in ["pymupdf4llm", "pdfplumber", "pymupdf_advanced", "pymupdf_basic", "pypdf2"]:
                    result = self.extract_file_metadata_with_specific_engine(
                        file_path=file_path,
                        engine_name=parser_name,
                        use_llm=use_llm and (parser_name == "pymupdf4llm"),  # LLMì€ pymupdf4llmì—ì„œë§Œ ì‚¬ìš©
                        save_to_file=False
                    )
                else:
                    # ê¸°ë³¸ íŒŒì„œ
                    result = self.extract_file_metadata(
                        file_path=file_path,
                        use_llm=use_llm,
                        save_to_file=False,
                        use_docling=False
                    )
                
                # ê²°ê³¼ í‰ê°€ (ì ìˆ˜ ê³„ì‚°)
                score = self._evaluate_parser_result(result)
                
                # ê²°ê³¼ ì €ì¥
                all_results["parsers_results"][parser_name] = {
                    "success": True,
                    "score": score,
                    "metadata": result,
                    "parser_used": result.get("parser_used", parser_name)
                }
                
                # ìµœê³  ì ìˆ˜ ì—…ë°ì´íŠ¸
                if score > best_score:
                    best_score = score
                    best_parser = parser_name
                    all_results["best_result"] = parser_name
                    
            except Exception as e:
                print(f"âŒ {parser_name} íŒŒì„œ ì‹¤íŒ¨: {e}")
                all_results["parsers_results"][parser_name] = {
                    "success": False,
                    "error": str(e),
                    "score": 0
                }
        
        # ìµœìƒì˜ ê²°ê³¼ë¥¼ ê¸°ë³¸ ë©”íƒ€ë°ì´í„°ë¡œ ì‚¬ìš©
        if best_parser and all_results["parsers_results"][best_parser]["success"]:
            best_metadata = all_results["parsers_results"][best_parser]["metadata"]
            # ìµœìƒì˜ ê²°ê³¼ë¥¼ ë£¨íŠ¸ ë ˆë²¨ì— ë³‘í•©
            for key, value in best_metadata.items():
                if key not in ["metadata_file", "markdown_file"]:  # íŒŒì¼ ê²½ë¡œëŠ” ì œì™¸
                    all_results[key] = value
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        if save_to_file:
            # í†µí•© ê²°ê³¼ ì €ì¥
            metadata_file = absolute_path.with_suffix(absolute_path.suffix + '.all_parsers.json')
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
            all_results["metadata_file"] = str(metadata_file)
            
            # ìƒˆë¡œìš´ íŒŒì„œë³„ ê°œë³„ íŒŒì¼ ì €ì¥ ì‹œìŠ¤í…œ ì‚¬ìš©
            try:
                saved_parser_files = save_parser_results(file_path, all_results["parsers_results"])
                all_results["individual_parser_files"] = saved_parser_files
                print(f"ğŸ“ {len(saved_parser_files)}ê°œ íŒŒì„œì˜ ê°œë³„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ê°œë³„ íŒŒì„œ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ê° íŒŒì„œë³„ ê²°ê³¼ë„ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ê°œë³„ ì €ì¥ (í˜¸í™˜ì„± ìœ ì§€)
            for parser_name, result_data in all_results["parsers_results"].items():
                if result_data["success"]:
                    parser_file = absolute_path.with_suffix(f'{absolute_path.suffix}.{parser_name}.json')
                    with open(parser_file, 'w', encoding='utf-8') as f:
                        json.dump(result_data["metadata"], f, ensure_ascii=False, indent=2)

            # PDFì˜ ê²½ìš° Markdown íŒŒì¼ë„ í•¨ê»˜ ì €ì¥ (.md, .docling.md)
            if absolute_path.suffix.lower() == '.pdf':
                markdown_files = {}
                # ê¸°ë³¸ íŒŒì„œ ê¸°ë°˜ Markdown (.md)
                try:
                    default_text = self.parse_file_content(file_path, use_docling=False)
                    if default_text:
                        md_path = absolute_path.with_suffix('.md')
                        md_content = self.convert_to_markdown(default_text)
                        with open(md_path, 'w', encoding='utf-8') as f:
                            f.write(md_content)
                        markdown_files["default"] = str(md_path)
                except Exception as e:
                    print(f"ê¸°ë³¸ Markdown ì €ì¥ ì‹¤íŒ¨: {e}")

                # Docling ê¸°ë°˜ Markdown (.docling.md)
                try:
                    docling_text = self.parse_file_content(file_path, use_docling=True)
                    if docling_text:
                        doc_md_path = absolute_path.with_suffix('.docling.md')
                        with open(doc_md_path, 'w', encoding='utf-8') as f:
                            f.write(docling_text)
                        markdown_files["docling"] = str(doc_md_path)
                except Exception as e:
                    print(f"Docling Markdown ì €ì¥ ì‹¤íŒ¨: {e}")

                if markdown_files:
                    all_results["markdown_files"] = markdown_files

        return all_results
    
    def _evaluate_parser_result(self, result: Dict[str, Any]) -> int:
        """íŒŒì„œ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ì ìˆ˜ ë°˜í™˜
        
        ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ë” ì¢‹ì€ ê²°ê³¼
        """
        score = 0
        
        # ê¸°ë³¸ ì ìˆ˜ (ì„±ê³µí•œ ê²½ìš°)
        score += 10
        
        # ë©”íƒ€ë°ì´í„° í•„ë“œ ìˆ˜
        score += len(result.keys())
        
        # êµ¬ì¡° ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ê°€ì‚°ì 
        if "docling_structure" in result:
            score += 20
            structure = result["docling_structure"]
            if isinstance(structure, dict):
                # í…Œì´ë¸”ì´ ìˆìœ¼ë©´ ê°€ì‚°ì 
                if "tables" in structure and len(structure.get("tables", [])) > 0:
                    score += 10 * len(structure["tables"])
                # ì„¹ì…˜ì´ ìˆìœ¼ë©´ ê°€ì‚°ì 
                if "sections" in structure and len(structure.get("sections", [])) > 0:
                    score += 5 * len(structure["sections"])
                # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ê°€ì‚°ì 
                if "images" in structure and len(structure.get("images", [])) > 0:
                    score += 5 * len(structure["images"])
        
        # ë¬¸ì„œ êµ¬ì¡° ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
        if "document_structure" in result:
            score += 10
            
        # í…ìŠ¤íŠ¸ í†µê³„ê°€ ìˆëŠ” ê²½ìš°
        if "text_statistics" in result:
            score += 5
            
        # LLM ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
        if "content_analysis" in result:
            score += 15
        
        return score
    
    def extract_file_metadata_with_specific_engine(self, file_path: str, engine_name: str, use_llm: bool = False, save_to_file: bool = True) -> Dict[str, Any]:
        """íŠ¹ì • PDF ì—”ì§„ì„ ì‚¬ìš©í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            engine_name: ì‚¬ìš©í•  ì—”ì§„ ì´ë¦„ (pymupdf4llm, pdfplumber, pymupdf_advanced, pymupdf_basic, pypdf2)
            use_llm: LLM ì‚¬ìš© ì—¬ë¶€
            save_to_file: íŒŒì¼ë¡œ ì €ì¥ ì—¬ë¶€
        """
        import hashlib
        from services.parser.pdf_parser import PdfParser
        from services.parser.base import DocumentMetadata
        
        absolute_path = self.get_absolute_path(file_path)
        file_stats = absolute_path.stat()
        file_size = file_stats.st_size
        
        # PDF íŒŒì„œ ìƒì„±
        pdf_parser = PdfParser()
        
        # íŠ¹ì • ì—”ì§„ ì„ íƒ
        engine_methods = {
            "pymupdf4llm": pdf_parser._parse_with_pymupdf4llm,
            "pdfplumber": pdf_parser._parse_with_pdfplumber,
            "pymupdf_advanced": pdf_parser._parse_with_pymupdf_advanced,
            "pymupdf_basic": pdf_parser._parse_with_pymupdf_basic,
            "pypdf2": pdf_parser._parse_with_pypdf2
        }
        
        if engine_name not in engine_methods:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—”ì§„: {engine_name}")
        
        parse_method = engine_methods[engine_name]
        
        try:
            # íŠ¹ì • ì—”ì§„ìœ¼ë¡œ íŒŒì‹±
            text, metadata_dict = parse_method(absolute_path)
            
            if not text:
                raise ValueError(f"{engine_name} ì—”ì§„ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
            
            # DocumentMetadata ìƒì„±
            metadata = DocumentMetadata(
                title=metadata_dict.get('title', absolute_path.name),
                page_count=metadata_dict.get('page_count', 1),
                word_count=len(text.split()),
                file_size=file_size,
                mime_type='application/pdf',
                parser_name=f"pdf_parser_{engine_name}",
                parser_version="1.0"
            )
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì„¤ì •
            for key, value in metadata_dict.items():
                if hasattr(metadata, key) and value is not None:
                    setattr(metadata, key, value)
            
            # DocumentMetadataë¥¼ ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            metadata_result = metadata.to_schema_compliant_dict(
                file_id=None,
                project_id=None
            )
            
            metadata_result["parser_used"] = f"pdf_{engine_name}"
            
            # íŒŒì¼ ì •ë³´ ì¶”ê°€
            metadata_result["file_info"] = {
                "absolute_path": str(absolute_path),
                "relative_path": file_path,
                "exists": absolute_path.exists(),
                "size": file_size,
                "modified": datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
                "created": datetime.fromtimestamp(file_stats.st_ctime).isoformat() if hasattr(file_stats, 'st_ctime') else None,
            }
            
            # í…ìŠ¤íŠ¸ í†µê³„
            lines = text.split('\n')
            words = text.split()
            
            metadata_result["text_statistics"] = {
                "total_characters": len(text),
                "total_words": len(words),
                "total_lines": len(lines)
            }
            
            # LLM ë¶„ì„ (ìš”ì²­ëœ ê²½ìš°)
            if use_llm and text:
                try:
                    llm_metadata = self.extract_metadata_with_llm(text[:10000])
                    if llm_metadata:
                        metadata_result["content_analysis"] = llm_metadata
                except Exception as e:
                    print(f"LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # íŒŒì¼ ì €ì¥
            if save_to_file:
                metadata_file = absolute_path.with_suffix(f'{absolute_path.suffix}.{engine_name}.metadata.json')
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata_result, f, ensure_ascii=False, indent=2)
                metadata_result["metadata_file"] = str(metadata_file)
            
            return metadata_result
            
        except Exception as e:
            raise ValueError(f"{engine_name} ì—”ì§„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    def extract_file_metadata(self, file_path: str, use_llm: bool = True, save_to_file: bool = True, use_docling: bool = False) -> Dict[str, Any]:
        """íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ì¶œ (Dublin Core í‘œì¤€ ì¤€ìˆ˜)
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            use_llm: LLM ì‚¬ìš© ì—¬ë¶€
            save_to_file: íŒŒì¼ë¡œ ì €ì¥ ì—¬ë¶€
            use_docling: Docling íŒŒì„œ ì‚¬ìš© ì—¬ë¶€ (PDF íŒŒì¼ì—ë§Œ ì ìš©)
        """
        import hashlib
        
        absolute_path = self.get_absolute_path(file_path)
        
        # PDF íŒŒì¼ì´ê³  use_doclingì´ Trueì¸ ê²½ìš° Docling íŒŒì„œ ì‚¬ìš©
        if use_docling and absolute_path.suffix.lower() == '.pdf':
            from services.parser.docling_parser import DoclingParser
            parser = DoclingParser()
            parse_result = parser.parse(absolute_path)
        else:
            # ê¸°ë³¸ AutoParser ì‚¬ìš©
            parser = AutoParser()
            parse_result = parser.parse(absolute_path)
        
        if not parse_result.success:
            raise ValueError(f"íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {parse_result.error_message}")
        
        # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
        file_stats = absolute_path.stat()
        file_size = file_stats.st_size
        
        # ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ìƒì„±
        if not parse_result.metadata:
            from services.parser.base import DocumentMetadata
            metadata = DocumentMetadata(
                title=absolute_path.name,
                file_size=file_size,
                mime_type=None
            )
        else:
            metadata = parse_result.metadata
        
        # DocumentMetadataë¥¼ ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        metadata_dict = metadata.to_schema_compliant_dict(
            file_id=None,
            project_id=None
        )
        
        # Docling íŒŒì„œë¥¼ ì‚¬ìš©í•œ ê²½ìš° ì¶”ê°€ êµ¬ì¡° ì •ë³´ í¬í•¨
        if use_docling and hasattr(metadata, 'document_structure'):
            metadata_dict["docling_structure"] = metadata.document_structure
            metadata_dict["parser_used"] = "docling"
        else:
            metadata_dict["parser_used"] = parse_result.parser_name if hasattr(parse_result, 'parser_name') else "unknown"
        
        # dc:identifierë¥¼ íŒŒì¼ ë‚´ìš©ì˜ í•´ì‹œê°’ìœ¼ë¡œ ì„¤ì •
        if parse_result.text:
            file_hash = hashlib.sha256(parse_result.text.encode('utf-8')).hexdigest()
        else:
            # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ íŒŒì¼ ê²½ë¡œì™€ í¬ê¸°ë¡œ í•´ì‹œ ìƒì„±
            file_hash = hashlib.sha256(f"{absolute_path}:{file_size}".encode('utf-8')).hexdigest()
        metadata_dict["dc:identifier"] = file_hash
        
        # ì¶”ê°€ íŒŒì¼ ì •ë³´
        metadata_dict["file_info"] = {
            "absolute_path": str(absolute_path),
            "relative_path": file_path,
            "exists": absolute_path.exists(),
            "size": file_size,
            "modified": datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
            "created": datetime.fromtimestamp(file_stats.st_ctime).isoformat() if hasattr(file_stats, 'st_ctime') else None,
        }
        
        # í…ìŠ¤íŠ¸ í†µê³„ ë° ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
        if parse_result.text:
            text = parse_result.text
            
            # PDFì˜ ê²½ìš° ì¢…ì¢… ëª¨ë“  í…ìŠ¤íŠ¸ê°€ í•œ ì¤„ë¡œ íŒŒì‹±ë¨
            # ì´ ê²½ìš° ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì¬êµ¬ì„±
            if len(text.split('\n')) <= 2 and len(text) > 1000:
                # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
                import re
                sentence_pattern = re.compile(r'([.!?ã€‚ï¼ï¼Ÿ]\s+|\n\n)')
                sentences_list = sentence_pattern.split(text)
                # ì¬êµ¬ì„±ëœ í…ìŠ¤íŠ¸ (ë¬¸ì¥ë§ˆë‹¤ ì¤„ë°”ê¿ˆ)
                reconstructed_lines = []
                for sent in sentences_list:
                    if sent.strip() and not sentence_pattern.match(sent):
                        reconstructed_lines.append(sent.strip())
                text_for_analysis = '\n'.join(reconstructed_lines)
            else:
                text_for_analysis = text
            
            # ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
            document_structure = self.analyze_document_structure(text_for_analysis, absolute_path.suffix.lower())
            metadata_dict["document_structure"] = document_structure
            
            # í…ìŠ¤íŠ¸ í†µê³„
            lines = text_for_analysis.split('\n')
            words = text.split()
            paragraphs = [p for p in text_for_analysis.split('\n\n') if p.strip()]
            sentences = self.count_sentences(text)
            
            metadata_dict["text_statistics"] = {
                "total_characters": len(text),
                "total_words": len(words),
                "total_lines": len(lines),
                "total_paragraphs": len(paragraphs) if paragraphs else 1,
                "total_sentences": sentences,
                "avg_words_per_sentence": len(words) / sentences if sentences > 0 else 0,
                "avg_sentences_per_paragraph": sentences / len(paragraphs) if paragraphs else sentences,
            }
            
            # LLMì„ ì‚¬ìš©í•œ ê³ ê¸‰ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            if use_llm and parse_result.text:
                try:
                    llm_metadata = self.extract_metadata_with_llm(parse_result.text[:10000])  # ì²˜ìŒ 10000ì ì‚¬ìš©
                    if llm_metadata:
                        metadata_dict["content_analysis"] = llm_metadata
                except Exception as e:
                    print(f"LLM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # Unknown, Null, ë¹ˆ ê°’ ì œê±°
        metadata_dict = self.filter_empty_values(metadata_dict)
        
        # ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        if save_to_file:
            # Docling ì‚¬ìš© ì‹œ ë³„ë„ íŒŒì¼ëª… ìƒì„±
            if use_docling and metadata_dict.get("parser_used") == "docling":
                metadata_file = absolute_path.with_suffix(absolute_path.suffix + '.docling.metadata.json')
            else:
                metadata_file = absolute_path.with_suffix(absolute_path.suffix + '.metadata.json')
                
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata_dict, f, ensure_ascii=False, indent=2)
            metadata_dict["metadata_file"] = str(metadata_file)
            
            # Markdown í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ (íŒŒì‹±ëœ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°)
            if parse_result.text:
                # Docling ì‚¬ìš© ì‹œ ë³„ë„ Markdown íŒŒì¼ ìƒì„±
                if use_docling and metadata_dict.get("parser_used") == "docling":
                    markdown_file = absolute_path.with_suffix('.docling.md')
                    with open(markdown_file, 'w', encoding='utf-8') as f:
                        f.write(parse_result.text)
                    metadata_dict["markdown_file"] = str(markdown_file)
                else:
                    self.save_as_markdown(absolute_path, parse_result)
        
        return metadata_dict
    
    def filter_empty_values(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Unknown, Null, ë¹ˆ ê°’ì„ ì¬ê·€ì ìœ¼ë¡œ ì œê±°"""
        if not isinstance(data, dict):
            return data
        
        filtered = {}
        for key, value in data.items():
            # ê°’ì´ Noneì´ê±°ë‚˜ "Unknown"ì´ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´/ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì œì™¸
            if value is None:
                continue
            if isinstance(value, str):
                if value.lower() in ['unknown', 'null', ''] or value.strip() == '':
                    continue
            if isinstance(value, list) and len(value) == 0:
                continue
            if isinstance(value, dict):
                # ë”•ì…”ë„ˆë¦¬ëŠ” ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
                filtered_value = self.filter_empty_values(value)
                if filtered_value:  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ í¬í•¨
                    filtered[key] = filtered_value
            else:
                filtered[key] = value
        
        return filtered
    
    def save_as_markdown(self, file_path: Path, parse_result) -> str:
        """íŒŒì‹± ê²°ê³¼ë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        markdown_file = file_path.with_suffix('.md')
        
        # íŒŒì„œê°€ ë°˜í™˜í•œ ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
        content = parse_result.text
        
        # pymupdf4llmì´ ë°˜í™˜í•œ markdownì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if hasattr(parse_result, 'parser_name') and 'pymupdf4llm' in parse_result.parser_name:
            # pymupdf4llmì€ ì´ë¯¸ Markdown í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
            markdown_content = content
        else:
            # ë‹¤ë¥¸ íŒŒì„œì˜ ê²½ìš° ê¸°ë³¸ ë³€í™˜
            markdown_content = self.convert_to_markdown(content)
        
        # ë©”íƒ€ë°ì´í„° í—¤ë” ì¶”ê°€
        if parse_result.metadata:
            header = f"---\ntitle: {parse_result.metadata.title or file_path.name}\n"
            if parse_result.metadata.dc_creator:
                header += f"author: {parse_result.metadata.dc_creator}\n"
            if parse_result.metadata.dc_date:
                header += f"date: {parse_result.metadata.dc_date}\n"
            header += "---\n\n"
            markdown_content = header + markdown_content
        
        # íŒŒì¼ ì €ì¥
        with open(markdown_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        return str(markdown_file)
    
    def convert_to_markdown(self, text: str) -> str:
        """ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜"""
        import re
        
        lines = text.split('\n')
        markdown_lines = []
        
        for line in lines:
            stripped = line.strip()
            
            # ë¹ˆ ì¤„
            if not stripped:
                markdown_lines.append('')
                continue
            
            # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì œëª© íŒ¨í„´
            section_match = re.match(r'^(\d+(?:\.\d+)*)\s*[\.)]?\s+(.+)', stripped)
            if section_match:
                level = len(section_match.group(1).split('.'))
                title = section_match.group(2)
                markdown_lines.append('#' * min(level, 6) + ' ' + title)
                continue
            
            # í…Œì´ë¸” ê°ì§€ (ê°„ë‹¨í•œ íŒ¨í„´)
            if '|' in line and line.count('|') >= 2:
                markdown_lines.append(line)
                continue
            
            # ë¦¬ìŠ¤íŠ¸ í•­ëª©
            if re.match(r'^\s*[-*â€¢]\s+', line):
                markdown_lines.append(line)
                continue
            
            if re.match(r'^\s*\d+[.)]\s+', line):
                markdown_lines.append(line)
                continue
            
            # ì¼ë°˜ ë‹¨ë½
            markdown_lines.append(stripped)
        
        return '\n'.join(markdown_lines)
    
    def extract_metadata_with_llm(self, text: str, file_path: str = None) -> Optional[Dict[str, Any]]:
        """LangChainì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        from services.config_service import ConfigService
        import json
        import logging
        
        # ë¡œê±° ì„¤ì •
        logger = logging.getLogger(__name__)
        
        # LangChain ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if not LANGCHAIN_AVAILABLE:
            logger.error("âŒ LangChainì´ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
            return self._extract_metadata_fallback(text, "LangChain ì‚¬ìš© ë¶ˆê°€")
        
        # LLM ì„¤ì • í™•ì¸
        llm_enabled = ConfigService.get_bool_config(self.db, "ENABLE_LLM_EXTRACTION", False)
        logger.info(f"ğŸ” LLM extraction enabled: {llm_enabled}")
        if not llm_enabled:
            logger.warning("âš ï¸ LLM extraction is disabled in configuration")
            return None
        
        ollama_url = ConfigService.get_config_value(self.db, "OLLAMA_BASE_URL", "http://localhost:11434")
        model_name = ConfigService.get_config_value(self.db, "OLLAMA_MODEL", "llama3.2")
        
        logger.info(f"ğŸ“‹ LLM ì„¤ì •: URL={ollama_url}, Model={model_name}")
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
        # í…ìŠ¤íŠ¸ í¬ê¸° ì œí•œ (ë” ë§ì€ ë‚´ìš© í¬í•¨ì„ ìœ„í•´ ì¦ê°€)
        max_text_length = 10000  # 800 -> 10000ìœ¼ë¡œ ì¦ê°€
        truncated_text = text[:max_text_length]
        
        # ë¬¸ì„œ ì–¸ì–´ ê°ì§€ (í•œê¸€ ë¬¸ìê°€ ë§ìœ¼ë©´ í•œêµ­ì–´ ë¬¸ì„œ)
        import re
        korean_chars = len(re.findall(r'[ê°€-í£]', truncated_text))
        total_chars = len(truncated_text)
        is_korean_doc = (korean_chars / total_chars) > 0.3 if total_chars > 0 else False
        
        # ë” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë³€ê²½ (íƒ€ì„ì•„ì›ƒ ë°©ì§€)
        prompt = f"""Extract the document title, main language, and a brief summary from the following text.

Requirements:
- Return only valid JSON. No explanations, no markdown, no extra text.
- JSON must contain exactly these keys:
  {{
    "title": string,        // extracted or inferred document title
    "language": "ko" | "en" | "other",  // detected main language
    "summary": string       // concise summary (1â€“2 sentences max)
  }}
- Ensure the JSON is valid and can be parsed without errors.

Text:
{truncated_text}
"""
        
        logger.info(f"ğŸ¤– LangChain Ollama í˜¸ì¶œ ì‹œì‘: {ollama_url}")
        logger.debug(f"ğŸ“ Prompt ê¸¸ì´: {len(prompt)} ë¬¸ì")
        
        try:
            # LangChain Ollama í´ë¼ì´ì–¸íŠ¸ ìƒì„± - ë§¤ìš° ê¸´ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            ollama_client = OllamaLLM(
                base_url=ollama_url,
                model=model_name,
                timeout=360,  # 6ë¶„ìœ¼ë¡œ ì¦ê°€
                temperature=0.3,  # í´ë¼ì´ì–¸íŠ¸ ìƒì„±ì‹œ ì„¤ì •
            )
            
            logger.info(f"ğŸ“¤ LangChain ìš”ì²­ (model={model_name}, timeout=360ì´ˆ(6ë¶„), temperature=0.3)")
            
            # ë¨¼ì € ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ íƒ€ì„ì•„ì›ƒ í™•ì¸
            logger.info("ğŸ§ª ëª¨ë¸ ì‘ë‹µì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
            import time
            test_start = time.time()
            try:
                test_response = ollama_client.invoke("Say hello in JSON: {\"greeting\": \"hello\"}")
                test_duration = time.time() - test_start
                logger.info(f"âœ… ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì„±ê³µ - ì†Œìš”ì‹œê°„: {test_duration:.2f}ì´ˆ, ì‘ë‹µ: {test_response[:100]}")
            except Exception as test_error:
                test_duration = time.time() - test_start
                logger.error(f"âŒ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - ì†Œìš”ì‹œê°„: {test_duration:.2f}ì´ˆ, ì˜¤ë¥˜: {test_error}")
            
            logger.info(f"â±ï¸ ì‹¤ì œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œì‘ (ê¸´ í…ìŠ¤íŠ¸ë¡œ ì¸í•œ ì§€ì—°ì´ ì˜ˆìƒë©ë‹ˆë‹¤...)") 
            
            # ì‹œì‘ ì‹œê°„ ê¸°ë¡
            start_time = time.time()
            
            # LangChainì„ í†µí•´ í˜¸ì¶œ
            response_text = ollama_client.invoke(prompt)
            
            # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
            end_time = time.time()
            duration = end_time - start_time
            
            logger.info(f"ğŸ”§ LangChain í˜¸ì¶œ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ")
            
            logger.info(f"ğŸ“¥ LangChain ì‘ë‹µ ê¸¸ì´: {len(response_text)} ë¬¸ì")
            if response_text:
                logger.debug(f"ğŸ“„ ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:200]}...")
            else:
                logger.warning("âš ï¸ LangChainì—ì„œ ë¹ˆ ì‘ë‹µ ë°˜í™˜")
            
            # í”„ë¡¬í”„íŠ¸/ì‘ë‹µ íŒŒì¼ ì €ì¥ (ê²°ê³¼ íŒŒì¼ë“¤ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì—)
            base_dir = "tests/debug_outputs/llm"  # ê¸°ë³¸ê°’
            if file_path:
                try:
                    from services.document_parser_service import DocumentParserService
                    absolute_path = self.get_absolute_path(file_path)
                    parser_service = DocumentParserService()
                    output_dir = parser_service.get_output_directory(absolute_path)
                    base_dir = str(output_dir)
                except Exception:
                    pass  # ê¸°ë³¸ê°’ ì‚¬ìš©
            
            log_prompt_and_response(
                label="local_metadata_langchain",
                provider="ollama",
                model=model_name,
                prompt=prompt,
                response=response_text,
                logger=logger,
                base_dir=base_dir,
                meta={
                    "base_url": ollama_url,
                    "temperature": 0.3,
                    "format": "json",
                    "langchain_version": True,
                },
            )
            
            # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
            if not response_text or response_text.strip() == "":
                logger.error("âŒ LangChainì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
                return self._extract_metadata_fallback(text, "LangChain ë¹ˆ ì‘ë‹µ")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                original_response = response_text
                
                # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json ... ``` ì²˜ë¦¬)
                if "```json" in response_text:
                    json_start = response_text.find("```json") + 7
                    json_end = response_text.find("```", json_start)
                    response_text = response_text[json_start:json_end].strip()
                    logger.debug("ğŸ”§ Extracted JSON from ```json``` blocks")
                elif "```" in response_text:
                    json_start = response_text.find("```") + 3
                    json_end = response_text.find("```", json_start)
                    response_text = response_text[json_start:json_end].strip()
                    logger.debug("ğŸ”§ Extracted JSON from ``` blocks")
                
                # ì²« ë²ˆì§¸ { ì™€ ë§ˆì§€ë§‰ } ì‚¬ì´ì˜ ë‚´ìš©ë§Œ ì¶”ì¶œ
                if "{" in response_text and "}" in response_text:
                    json_start = response_text.find("{")
                    json_end = response_text.rfind("}") + 1
                    response_text = response_text[json_start:json_end]
                    logger.debug("ğŸ”§ Extracted JSON between { and }")
                
                def repair_json_like(s: str) -> str:
                    """ê°„ë‹¨í•œ JSON ë³µêµ¬: ì˜ëª»ëœ ì¸ìš©ë¶€í˜¸/ì‰¼í‘œ/ì½”ë“œíœìŠ¤ ì •ë¦¬"""
                    import re
                    # ìŠ¤ë§ˆíŠ¸ ì¸ìš©ë¶€í˜¸ë¥¼ ASCIIë¡œ ë³€í™˜
                    s = s.replace(""", '"').replace(""", '"').replace("'", "'")
                    # í‚¤ì— ì‚¬ìš©ëœ ë‹¨ì¼ ì¸ìš©ë¶€í˜¸ë¥¼ ì´ìŠ¤ì¼€ì´í”„ëœ ìŒë”°ì˜´í‘œë¡œ ë³€ê²½
                    s = re.sub(r"'([A-Za-z0-9_\- ]+)'\s*:", r'"\1":', s)
                    # ê°’ì— ì‚¬ìš©ëœ ë‹¨ì¼ ì¸ìš©ë¶€í˜¸ ë¬¸ìì—´ì„ ìŒë”°ì˜´í‘œë¡œ ë³€ê²½
                    s = re.sub(r":\s*'([^']*)'", r': "\1"', s)
                    # ëì— ë¶™ì€ ì‰¼í‘œ ì œê±°
                    s = re.sub(r",\s*([}\]])", r"\1", s)
                    # BOM/ì œì–´ë¬¸ì ì œê±°
                    s = s.replace("\ufeff", "").strip()
                    return s
                
                try:
                    metadata = json.loads(response_text)
                except json.JSONDecodeError:
                    repaired = repair_json_like(response_text)
                    metadata = json.loads(repaired)
                    logger.info("ğŸ› ï¸ Nonâ€‘strict JSON repaired successfully")
                
                logger.info(f"âœ… LangChain ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì„±ê³µ: {list(metadata.keys())}")
                # ì›ë³¸ ì‘ë‹µë„ í¬í•¨
                metadata["_llm_metadata"] = {
                    "raw_response": original_response,
                    "extraction_status": "langchain_success",
                    "model": model_name,
                    "response_length": len(original_response)
                }
                return metadata
                
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                logger.error(f"ğŸ“„ ë¬¸ì œê°€ ëœ ì‘ë‹µ (ì²˜ìŒ 500ì): {response_text[:500]}")
                logger.warning("âš ï¸ í´ë°± ë©”íƒ€ë°ì´í„° ì¶”ì¶œë¡œ ì „í™˜")
                
                return self._extract_metadata_fallback(text, f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                
        except Exception as e:
            logger.error(f"âŒ LangChain ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            logger.exception("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
            
            # ì˜¤ë¥˜ ì‹œì—ë„ ë¡œê¹…
            try:
                # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
                base_dir = "tests/debug_outputs/llm"  # ê¸°ë³¸ê°’
                if file_path:
                    try:
                        from services.document_parser_service import DocumentParserService
                        absolute_path = self.get_absolute_path(file_path)
                        parser_service = DocumentParserService()
                        output_dir = parser_service.get_output_directory(absolute_path)
                        base_dir = str(output_dir)
                    except Exception:
                        pass  # ê¸°ë³¸ê°’ ì‚¬ìš©
                        
                log_prompt_and_response(
                    label="local_metadata_langchain_error",
                    provider="ollama",
                    model=model_name,
                    prompt=prompt,
                    response="",
                    logger=logger,
                    base_dir=base_dir,
                    meta={"base_url": ollama_url, "error": f"exception: {e}", "langchain_version": True},
                )
            except Exception:
                pass
            
            return self._extract_metadata_fallback(text, f"LangChain ì˜¤ë¥˜: {str(e)}")
    
    def _test_ollama_model(self, ollama_url: str, model_name: str) -> bool:
        """LangChainìœ¼ë¡œ Ollama ëª¨ë¸ ìƒíƒœë¥¼ ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸"""
        import logging
        logger = logging.getLogger(__name__)
        
        if not LANGCHAIN_AVAILABLE:
            logger.warning("âš ï¸ LangChainì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ëª¨ë¸ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœ€")
            return False
        
        try:
            logger.debug(f"ğŸ§ª LangChainìœ¼ë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘: {model_name}")
            
            # LangChain Ollama í´ë¼ì´ì–¸íŠ¸ë¡œ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            ollama_client = OllamaLLM(
                base_url=ollama_url,
                model=model_name,
                timeout=15
            )
            
            test_response = ollama_client.invoke("Hello")
            
            if test_response and test_response.strip():
                logger.info(f"âœ… LangChain ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ: '{test_response.strip()[:50]}'")
                return True
            else:
                logger.warning("âš ï¸ LangChain ëª¨ë¸ í…ŒìŠ¤íŠ¸ - ë¹ˆ ì‘ë‹µ ë°˜í™˜")
                return False
                
        except Exception as e:
            logger.warning(f"âš ï¸ LangChain ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _extract_metadata_fallback(self, text: str, error_reason: str) -> Dict[str, Any]:
        """LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info(f"ğŸ“‹ í´ë°± ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œì‘ - ì‚¬ìœ : {error_reason}")
        
        # í…ìŠ¤íŠ¸ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        # ì œëª© ì¶”ì¶œ ì‹œë„ (ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ì¤„)
        title = "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"
        for line in lines[:10]:
            if (len(line) > 5 and len(line) < 200 and
                not line.replace('.', '').replace('-', '').replace('#', '').replace('=', '').isdigit() and
                not line.startswith('http') and
                not '@' in line):
                title = line[:100]  # ì œëª©ì€ 100ìë¡œ ì œí•œ
                break
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)
        import re
        words = re.findall(r'\b[ê°€-í£a-zA-Z]{3,}\b', text)
        word_freq = {}
        for word in words:
            if len(word) >= 3:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # ìƒìœ„ 5ê°œ ë‹¨ì–´ë¥¼ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
        keywords = [word for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]]
        
        # ì–¸ì–´ ê°ì§€
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        language = "ko" if korean_chars > english_chars else "en"
        
        # ë¬¸ì„œ íƒ€ì… ì¶”ì •
        doc_type = "ë¬¸ì„œ"
        if "ë³´ê³ ì„œ" in text or "report" in text.lower():
            doc_type = "ë³´ê³ ì„œ"
        elif "ë…¼ë¬¸" in text or "paper" in text.lower() or "abstract" in text.lower():
            doc_type = "ë…¼ë¬¸"
        elif "ë§¤ë‰´ì–¼" in text or "manual" in text.lower() or "guide" in text.lower():
            doc_type = "ë§¤ë‰´ì–¼"
        
        fallback_metadata = {
            "title": title,
            "document_type": doc_type,
            "language": language,
            "keywords": keywords,
            "summary": f"í´ë°± ëª¨ë“œë¡œ ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„° - ì›ì¸: {error_reason}",
            "main_topics": keywords[:3],  # ìƒìœ„ 3ê°œë¥¼ ì£¼ìš” í† í”½ìœ¼ë¡œ
            "date": None,
            "extraction_status": "fallback",
            "fallback_reason": error_reason,
            "_llm_metadata": {
                "raw_response": "",
                "extraction_status": "fallback",
                "model": "none",
                "response_length": 0,
                "fallback_reason": error_reason
            }
        }
        
        logger.info(f"âœ… í´ë°± ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ - ì œëª©: '{title[:50]}', í‚¤ì›Œë“œ: {len(keywords)}ê°œ")
        return fallback_metadata
    
    def _extract_metadata_with_langchain(self, text: str, ollama_url: str, model_name: str) -> Optional[Dict[str, Any]]:
        """LangChainì„ ì‚¬ìš©í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œë„"""
        if not LANGCHAIN_AVAILABLE:
            return None
            
        import logging
        import json
        logger = logging.getLogger(__name__)
        
        try:
            # LangChain Ollama í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            ollama_client = OllamaLLM(
                base_url=ollama_url,
                model=model_name,
                timeout=60
            )
            
            # ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸ (ë” ì•ˆì •ì ì¸ ì‘ë‹µì„ ìœ„í•´)
            prompt = f"""Analyze this document and extract metadata in JSON format:

{text[:15000]}

Return only a JSON object with these fields:
{{
    "title": "document title or first meaningful line",
    "summary": "1-2 sentence summary",
    "keywords": ["key1", "key2", "key3"],
    "language": "ko or en"
}}

JSON only, no explanations:"""
            
            logger.debug(f"ğŸ”— LangChain í˜¸ì¶œ ì‹œì‘ - ëª¨ë¸: {model_name}")
            
            # LangChainì„ í†µí•´ í˜¸ì¶œ
            response = ollama_client.invoke(prompt)
            
            logger.debug(f"ğŸ“„ LangChain ì‘ë‹µ ê¸¸ì´: {len(response)} ë¬¸ì")
            
            if not response or response.strip() == "":
                logger.warning("âš ï¸ LangChainì—ì„œë„ ë¹ˆ ì‘ë‹µ ë°˜í™˜")
                return None
            
            # í”„ë¡¬í”„íŠ¸/ì‘ë‹µ ë¡œê¹… (ê²°ê³¼ íŒŒì¼ë“¤ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì—)
            base_dir = "tests/debug_outputs/llm"  # ê¸°ë³¸ê°’
            if hasattr(self, '_current_file_path') and self._current_file_path:
                try:
                    from services.document_parser_service import DocumentParserService
                    absolute_path = self.get_absolute_path(self._current_file_path)
                    parser_service = DocumentParserService()
                    output_dir = parser_service.get_output_directory(absolute_path)
                    base_dir = str(output_dir)
                except Exception:
                    pass  # ê¸°ë³¸ê°’ ì‚¬ìš©
            
            log_prompt_and_response(
                label="local_metadata_langchain",
                provider="ollama",
                model=model_name,
                prompt=prompt,
                response=response,
                logger=logger,
                base_dir=base_dir,
                meta={
                    "base_url": ollama_url,
                    "langchain_version": True,
                },
            )
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # JSON ë¶€ë¶„ ì¶”ì¶œ
                json_text = response.strip()
                if "```json" in json_text:
                    start_idx = json_text.find("```json") + 7
                    end_idx = json_text.find("```", start_idx)
                    if end_idx != -1:
                        json_text = json_text[start_idx:end_idx]
                
                # ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€ ì¶”ì¶œ
                start = json_text.find('{')
                end = json_text.rfind('}')
                if start != -1 and end != -1 and end > start:
                    json_text = json_text[start:end+1]
                
                metadata = json.loads(json_text)
                
                # ê¸°ë³¸ í•„ë“œ ë³´ì¥
                result = {
                    "title": metadata.get("title", "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"),
                    "document_type": "ë¬¸ì„œ",
                    "language": metadata.get("language", "ko"),
                    "keywords": metadata.get("keywords", []),
                    "summary": metadata.get("summary", "LangChainìœ¼ë¡œ ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°"),
                    "main_topics": metadata.get("keywords", [])[:3],
                    "date": metadata.get("date"),
                    "extraction_status": "langchain_success",
                    "_llm_metadata": {
                        "raw_response": response,
                        "extraction_status": "langchain_success",
                        "model": model_name,
                        "response_length": len(response)
                    }
                }
                
                logger.info(f"âœ… LangChainìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì„±ê³µ - ì œëª©: '{result['title'][:50]}'")
                return result
                
            except json.JSONDecodeError as e:
                logger.error(f"âŒ LangChain ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                logger.debug(f"ğŸ“„ LangChain ì‘ë‹µ: {response[:500]}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ LangChain ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def analyze_document_structure(self, text: str, file_extension: str) -> Dict[str, Any]:
        """ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ (ì„¹ì…˜, í…Œì´ë¸”, ê·¸ë¦¼ ë“±)"""
        import re
        
        structure = {
            "sections": [],
            "tables_count": 0,
            "figures_count": 0,
            "references_count": 0,
            "footnotes_count": 0,
            "lists_count": 0,
            "headings_hierarchy": []
        }
        
        lines = text.split('\n')
        
        # ì„¹ì…˜/ì œëª© íŒ¨í„´ ê°ì§€ (ë” í¬ê´„ì ì¸ íŒ¨í„´)
        # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì œëª© íŒ¨í„´ (1. , 1.1, 2.3.4 ë“±)
        section_pattern = re.compile(r'^(\d+(?:\.\d+)*)\s*[\.)]?\s+(.+)')
        # ë¡œë§ˆ ìˆ«ì íŒ¨í„´ (I., II., III. ë“±)
        roman_pattern = re.compile(r'^([IVXLCDM]+)\s*[\.)]?\s+(.+)')
        # Markdown ìŠ¤íƒ€ì¼ í—¤ë” (###)
        markdown_pattern = re.compile(r'^(#{1,6})\s+(.+)')
        
        # í•œêµ­ì–´ ì„¹ì…˜ íŒ¨í„´ (ì œ1ì¥, ì œ2ì ˆ, 1ì¥, 2ì ˆ ë“±)
        korean_section_pattern = re.compile(r'^ì œ?\s*(\d+)\s*[ì¥ì ˆí•­]\s*[\.:]?\s*(.+)')
        
        # ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ì œëª© íŒ¨í„´ (ì£¼ë¡œ ì˜ë¬¸ ë¬¸ì„œ)
        # PDFì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” íŒ¨í„´
        uppercase_title_pattern = re.compile(r'^([A-Z][A-Z\s]{2,})\s*$')
        
        # ì½œë¡ ì´ë‚˜ ëŒ€ì‹œë¡œ ëë‚˜ëŠ” ì œëª© íŒ¨í„´
        title_with_separator = re.compile(r'^([ê°€-í£A-Za-z0-9\s]+)\s*[:ï¼š-]\s*$')
        
        # ë“¤ì—¬ì“°ê¸°ê°€ ì—†ê³  ì§§ì€ ë…ë¦½ ë¼ì¸ (ì œëª©ì¼ ê°€ëŠ¥ì„±)
        potential_title_pattern = re.compile(r'^[^\s](.{5,50})$')
        
        # í…Œì´ë¸” ê°ì§€ íŒ¨í„´ (í™•ì¥)
        table_patterns = [
            re.compile(r'\|.*\|'),  # Markdown í…Œì´ë¸”
            re.compile(r'[<\[]?\s*í‘œ\s*\d+', re.IGNORECASE),  # "í‘œ 1", "<í‘œ 1>", "[í‘œ 1]"
            re.compile(r'Table\s*\d+', re.IGNORECASE),
            re.compile(r'<table', re.IGNORECASE),  # HTML í…Œì´ë¸”
            re.compile(r'â”Œ|â”œ|â””|â”€|â”‚'),  # Box drawing ë¬¸ì
            re.compile(r'[í‘œè¡¨]\s*[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+'),  # í•œì ìˆ«ì í¬í•¨
        ]
        
        # ê·¸ë¦¼/ì°¨íŠ¸ ê°ì§€ íŒ¨í„´ (í™•ì¥)
        figure_patterns = [
            re.compile(r'[<\[]?\s*ê·¸ë¦¼\s*\d+', re.IGNORECASE),  # "ê·¸ë¦¼ 1", "<ê·¸ë¦¼ 1>"
            re.compile(r'Figure\s*\d+', re.IGNORECASE),
            re.compile(r'Fig\.\s*\d+', re.IGNORECASE),
            re.compile(r'[<\[]?\s*ì°¨íŠ¸\s*\d+', re.IGNORECASE),
            re.compile(r'Chart\s*\d+', re.IGNORECASE),
            re.compile(r'[<\[]?\s*ë„í‘œ\s*\d+', re.IGNORECASE),  # ë„í‘œ
            re.compile(r'[<\[]?\s*ì‚¬ì§„\s*\d+', re.IGNORECASE),  # ì‚¬ì§„
            re.compile(r'[<\[]?\s*ì´ë¯¸ì§€\s*\d+', re.IGNORECASE),  # ì´ë¯¸ì§€
            re.compile(r'!\[.*\]\(.*\)'),  # Markdown ì´ë¯¸ì§€
        ]
        
        # ì°¸ê³ ë¬¸í—Œ ê°ì§€
        reference_patterns = [
            re.compile(r'^\[\d+\]'),  # [1] ìŠ¤íƒ€ì¼
            re.compile(r'ì°¸ê³ ë¬¸í—Œ', re.IGNORECASE),
            re.compile(r'References', re.IGNORECASE),
            re.compile(r'Bibliography', re.IGNORECASE),
        ]
        
        # ê°ì£¼ ê°ì§€
        footnote_patterns = [
            re.compile(r'\[\^\d+\]'),  # Markdown ê°ì£¼
            re.compile(r'ì£¼\s*\d+[:\)]'),  # "ì£¼1:", "ì£¼ 1)"
        ]
        
        # ë¦¬ìŠ¤íŠ¸ ê°ì§€
        list_patterns = [
            re.compile(r'^\s*[-*â€¢]\s+'),  # ë¶ˆë¦¿ ë¦¬ìŠ¤íŠ¸
            re.compile(r'^\s*\d+[.)]\s+'),  # ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸
            re.compile(r'^\s*[a-z][.)]\s+', re.IGNORECASE),  # ì•ŒíŒŒë²³ ë¦¬ìŠ¤íŠ¸
        ]
        
        # ë¼ì¸ë³„ ë¶„ì„
        prev_line = ""
        next_line = ""
        
        for i, line in enumerate(lines):
            # ì´ì „ ë¼ì¸ê³¼ ë‹¤ìŒ ë¼ì¸ ì°¸ì¡° (ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ìš©)
            prev_line = lines[i-1] if i > 0 else ""
            next_line = lines[i+1] if i < len(lines)-1 else ""
            
            stripped_line = line.strip()
            
            # ë¹ˆ ì¤„ ê±´ë„ˆë›°ê¸°
            if not stripped_line:
                continue
            
            # ì„¹ì…˜/ì œëª© ê°ì§€
            section_match = section_pattern.match(stripped_line)
            if section_match:
                section_num = section_match.group(1)
                section_title = section_match.group(2)
                level = len(section_num.split('.'))
                structure["sections"].append({
                    "number": section_num,
                    "title": section_title,
                    "level": level,
                    "line": i + 1
                })
                structure["headings_hierarchy"].append(level)
                continue
            
            # í•œêµ­ì–´ ì„¹ì…˜ íŒ¨í„´
            korean_match = korean_section_pattern.match(stripped_line)
            if korean_match:
                structure["sections"].append({
                    "number": korean_match.group(1),
                    "title": korean_match.group(2) if korean_match.group(2) else stripped_line,
                    "level": 1,
                    "line": i + 1,
                    "style": "korean"
                })
                structure["headings_hierarchy"].append(1)
                continue
            
            # ë¡œë§ˆ ìˆ«ì ì„¹ì…˜
            roman_match = roman_pattern.match(stripped_line)
            if roman_match and len(roman_match.group(1)) <= 4:  # ë„ˆë¬´ ê¸´ ë¡œë§ˆ ìˆ«ìëŠ” ì œì™¸
                structure["sections"].append({
                    "number": roman_match.group(1),
                    "title": roman_match.group(2),
                    "level": 1,
                    "line": i + 1
                })
                structure["headings_hierarchy"].append(1)
                continue
            
            # Markdown í—¤ë”
            markdown_match = markdown_pattern.match(stripped_line)
            if markdown_match:
                level = len(markdown_match.group(1))
                structure["sections"].append({
                    "title": markdown_match.group(2),
                    "level": level,
                    "line": i + 1,
                    "style": "markdown"
                })
                structure["headings_hierarchy"].append(level)
                continue
            
            # ëŒ€ë¬¸ì ì œëª© (ì˜ë¬¸ ë¬¸ì„œ)
            if uppercase_title_pattern.match(stripped_line) and len(stripped_line) < 50:
                # ì•ë’¤ê°€ ë¹ˆ ì¤„ì¸ ê²½ìš° ì œëª©ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
                if (not prev_line.strip() or not next_line.strip()):
                    structure["sections"].append({
                        "title": stripped_line,
                        "level": 1,
                        "line": i + 1,
                        "style": "uppercase"
                    })
                    structure["headings_hierarchy"].append(1)
                    continue
            
            # ì½œë¡ ì´ë‚˜ ëŒ€ì‹œë¡œ ëë‚˜ëŠ” ì œëª©
            if title_with_separator.match(stripped_line):
                structure["sections"].append({
                    "title": title_with_separator.match(stripped_line).group(1),
                    "level": 2,
                    "line": i + 1,
                    "style": "separator"
                })
                structure["headings_hierarchy"].append(2)
                continue
            
            # ì§§ì€ ë…ë¦½ ë¼ì¸ (ì „í›„ ë¹ˆ ì¤„ì´ ìˆê³  ê¸¸ì´ê°€ ì ì ˆí•œ ê²½ìš°)
            if (len(stripped_line) > 5 and len(stripped_line) < 50 and 
                not prev_line.strip() and not next_line.strip() and 
                not stripped_line.endswith(('.', 'ã€‚', '!', '?', 'ï¼', 'ï¼Ÿ'))):
                # ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ìë¡œ ì‹œì‘í•˜ì§€ ì•ŠëŠ” ê²½ìš°
                if re.match(r'^[ê°€-í£A-Za-z]', stripped_line):
                    structure["sections"].append({
                        "title": stripped_line,
                        "level": 3,
                        "line": i + 1,
                        "style": "isolated"
                    })
                    structure["headings_hierarchy"].append(3)
            
            # í…Œì´ë¸” ê°ì§€
            for pattern in table_patterns:
                if pattern.search(line):
                    structure["tables_count"] += 1
                    break
            
            # ê·¸ë¦¼/ì°¨íŠ¸ ê°ì§€
            for pattern in figure_patterns:
                if pattern.search(line):
                    structure["figures_count"] += 1
                    break
            
            # ì°¸ê³ ë¬¸í—Œ ê°ì§€
            for pattern in reference_patterns:
                if pattern.search(line):
                    structure["references_count"] += 1
                    break
            
            # ê°ì£¼ ê°ì§€
            for pattern in footnote_patterns:
                if pattern.search(line):
                    structure["footnotes_count"] += 1
                    break
            
            # ë¦¬ìŠ¤íŠ¸ ê°ì§€
            for pattern in list_patterns:
                if pattern.match(line):
                    structure["lists_count"] += 1
                    break
        
        # ì„¹ì…˜ ì •ë¦¬ ë° ìš”ì•½
        if structure["sections"]:
            # ì¤‘ë³µ ì œê±° (ê°™ì€ ë¼ì¸ì˜ ì„¹ì…˜ ì œê±°)
            seen_lines = set()
            unique_sections = []
            for section in structure["sections"]:
                if section["line"] not in seen_lines:
                    seen_lines.add(section["line"])
                    unique_sections.append(section)
            
            structure["sections"] = unique_sections
            structure["total_sections"] = len(unique_sections)
            structure["max_heading_level"] = max(structure["headings_hierarchy"]) if structure["headings_hierarchy"] else 0
            
            # ì„¹ì…˜ì„ ë ˆë²¨ë³„ë¡œ ë¶„ë¥˜
            sections_by_level = {}
            for section in unique_sections:
                level = section.get("level", 1)
                if level not in sections_by_level:
                    sections_by_level[level] = []
                sections_by_level[level].append(section["title"])
            
            structure["sections_by_level"] = sections_by_level
            
            # ì£¼ìš” ì„¹ì…˜ë§Œ ì¶”ì¶œ (ìƒìœ„ 20ê°œ)
            structure["main_sections"] = [s["title"] for s in unique_sections[:20]]
        else:
            # ì„¹ì…˜ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
            structure["total_sections"] = 0
            structure["max_heading_level"] = 0
            structure["sections_by_level"] = {}
            structure["main_sections"] = []
        
        # ë¬¸ì„œ êµ¬ì¡° ìš”ì•½
        structure["document_outline"] = {
            "has_table_of_contents": any("ëª©ì°¨" in s.get("title", "") or "Contents" in s.get("title", "") for s in structure["sections"]),
            "has_introduction": any("ì„œë¡ " in s.get("title", "") or "Introduction" in s.get("title", "") or "ê°œìš”" in s.get("title", "") for s in structure["sections"]),
            "has_conclusion": any("ê²°ë¡ " in s.get("title", "") or "Conclusion" in s.get("title", "") or "ë§ºìŒ" in s.get("title", "") for s in structure["sections"]),
            "has_references": structure["references_count"] > 0,
            "has_appendix": any("ë¶€ë¡" in s.get("title", "") or "Appendix" in s.get("title", "") for s in structure["sections"]),
            "structure_quality": "good" if structure.get("total_sections", 0) > 3 else "poor"
        }
        
        return structure
    
    def count_sentences(self, text: str) -> int:
        """ë¬¸ì¥ ê°œìˆ˜ ê³„ì‚°"""
        import re
        # í•œêµ­ì–´ì™€ ì˜ì–´ ë¬¸ì¥ ì¢…ê²° ë¶€í˜¸ ê³ ë ¤
        sentence_endings = re.compile(r'[.!?ã€‚ï¼ï¼Ÿ]+[\s\n]')
        sentences = sentence_endings.split(text)
        # ë¹ˆ ë¬¸ì¥ ì œì™¸
        return len([s for s in sentences if s.strip()])
    
    def analyze_document_structure_with_llm(self, text: str, file_path: str, file_extension: str, overrides: Dict[str, Any] = None) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•œ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ (Ollama/OpenAI/Gemini)
        
        overrides: ìš”ì²­ ë‹¨ìœ„ë¡œ LLM êµ¬ì„±ì„ ë®ì–´ì“°ëŠ” ì˜µì…˜(dict)
        ì˜ˆ) {"enabled": true, "provider": "gemini", "model": "models/gemini-2.0-flash", "api_key": "...", "base_url": "...", "max_tokens": 1000, "temperature": 0.2}
        """
        import logging
        import json
        from services.config_service import ConfigService
        from prompts.templates import DocumentStructurePrompts
        from utils.llm_logger import log_prompt_and_response
        
        logger = logging.getLogger(__name__)
        
        # LLM ì„¤ì • í™•ì¸
        overrides = overrides or {}
        llm_enabled = overrides.get("enabled") if "enabled" in overrides else ConfigService.get_bool_config(self.db, "ENABLE_LLM_EXTRACTION", False)
        if not llm_enabled:
            logger.warning("âš ï¸ LLM extraction is disabled in configuration")
            return self._fallback_structure_analysis(text, file_extension)
        
        provider = overrides.get("provider") or ConfigService.get_config_value(self.db, "LLM_PROVIDER", "ollama")
        logger.info(f"ğŸ” LLM ê¸°ë°˜ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì‹œì‘ - provider={provider}")
        
        # Providerë³„ ëª¨ë¸/ì—”ë“œí¬ì¸íŠ¸ êµ¬ì„±
        if provider == "ollama":
            ollama_url = overrides.get("base_url") or ConfigService.get_config_value(self.db, "OLLAMA_BASE_URL", "http://localhost:11434")
            model_name = overrides.get("model") or ConfigService.get_config_value(self.db, "OLLAMA_MODEL", "llama3.2")
            openai_conf = None
            gemini_conf = None
        elif provider == "openai":
            openai_conf = {**ConfigService.get_openai_config(self.db), **overrides}
            gemini_conf = None
            model_name = openai_conf.get("model")
        elif provider == "gemini":
            gemini_conf = {**ConfigService.get_gemini_config(self.db), **overrides}
            openai_conf = None
            model_name = gemini_conf.get("model")
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” LLM provider '{provider}', ollamaë¡œ í´ë°±")
            provider = "ollama"
            ollama_url = overrides.get("base_url") or ConfigService.get_config_value(self.db, "OLLAMA_BASE_URL", "http://localhost:11434")
            model_name = overrides.get("model") or ConfigService.get_config_value(self.db, "OLLAMA_MODEL", "llama3.2")
            openai_conf = None
            gemini_conf = None
        
        logger.info(f"ğŸ” LLM ëª¨ë¸: {model_name}")
        
        try:
            # LangChain Ollama í´ë¼ì´ì–¸íŠ¸ ë˜ëŠ” HTTP í˜¸ì¶œ ì¤€ë¹„
            ollama_client = None
            if provider == "ollama":
                try:
                    from langchain_ollama import OllamaLLM
                    ollama_client = OllamaLLM(
                        base_url=ollama_url,
                        model=model_name,
                        timeout=120,
                        temperature=0.2,
                    )
                except Exception as e:
                    logger.error(f"âŒ Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    return self._fallback_structure_analysis_with_llm_attempt(text, file_extension, str(e))
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë” ë§ì€ ë‚´ìš© í¬í•¨ì„ ìœ„í•´ ì¦ê°€)
            max_text_length = 15000  # 3000 -> 15000ìœ¼ë¡œ ì¦ê°€
            truncated_text = text[:max_text_length] if len(text) > max_text_length else text
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
            prompt_template = DocumentStructurePrompts.STRUCTURE_ANALYSIS_LLM
            
            # íŒŒì¼ ì •ë³´ ì¤€ë¹„
            from pathlib import Path
            file_path_obj = Path(file_path) if isinstance(file_path, str) else file_path
            file_info = {
                "filename": file_path_obj.name,
                "extension": file_extension,
                "size": len(text),
                "truncated_size": len(truncated_text)
            }
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = prompt_template.format(
                file_info=json.dumps(file_info, ensure_ascii=False, indent=2),
                text=truncated_text
            )
            
            logger.info(f"ğŸ“¤ LLM êµ¬ì¡° ë¶„ì„ ìš”ì²­ ì¤‘... (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(truncated_text)} ë¬¸ì)")
            
            # LLM í˜¸ì¶œ
            if provider == "ollama":
                response = ollama_client.invoke(prompt)
                base_dir_provider = "ollama"
                base_url_meta = ollama_url
            elif provider == "openai":
                response = self._call_openai_chat(prompt, openai_conf)
                base_dir_provider = "openai"
                base_url_meta = openai_conf.get("base_url", "https://api.openai.com/v1")
            elif provider == "gemini":
                response = self._call_gemini_generate(prompt, gemini_conf)
                base_dir_provider = "gemini"
                base_url_meta = gemini_conf.get("base_url", "https://generativelanguage.googleapis.com")
            else:
                return self._fallback_structure_analysis_with_llm_attempt(text, file_extension, f"Unsupported provider: {provider}")
            
            logger.info(f"ğŸ“¥ LLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ (ê¸¸ì´: {len(response)} ë¬¸ì)")
            
            # í”„ë¡¬í”„íŠ¸/ì‘ë‹µ ë¡œê¹… (ê²°ê³¼ íŒŒì¼ë“¤ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì—)
            base_dir = "tests/debug_outputs/llm"  # ê¸°ë³¸ê°’
            try:
                from services.document_parser_service import DocumentParserService
                absolute_path = self.get_absolute_path(file_path) if isinstance(file_path, str) else file_path
                parser_service = DocumentParserService()
                output_dir = parser_service.get_output_directory(absolute_path)
                base_dir = str(output_dir)
            except Exception:
                pass  # ê¸°ë³¸ê°’ ì‚¬ìš©
                
            log_prompt_and_response(
                label="document_structure_analysis",
                provider=base_dir_provider,
                model=model_name,
                prompt=prompt,
                response=response,
                logger=logger,
                base_dir=base_dir,
                meta={
                    "base_url": base_url_meta,
                    "temperature": 0.2,
                    "file_extension": file_extension,
                    "text_length": len(text),
                    "truncated_length": len(truncated_text)
                }
            )
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # JSON ì‘ë‹µ ì¶”ì¶œ
                json_response = self._extract_json_from_response(response)
                if json_response:
                    # ê¸°ë³¸ êµ¬ì¡° ë¶„ì„ê³¼ ë³‘í•©
                    basic_structure = self.analyze_document_structure(text, file_extension)
                    
                    # LLM ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                    enhanced_structure = {
                        **basic_structure,
                        "llm_analysis": json_response,
                        "analysis_method": "llm_enhanced",
                        "llm_model": model_name,
                        "llm_success": True
                    }
                    
                    # LLMì—ì„œ ì¶”ì¶œí•œ êµ¬ì¡° ì •ë³´ë¡œ ê¸°ë³¸ ë¶„ì„ ë³´ì™„
                    if "sections" in json_response:
                        enhanced_structure["llm_detected_sections"] = json_response["sections"]
                    
                    if "document_type" in json_response:
                        enhanced_structure["document_type"] = json_response["document_type"]
                    
                    if "main_topics" in json_response:
                        enhanced_structure["main_topics"] = json_response["main_topics"]
                    
                    logger.info("âœ… LLM ê¸°ë°˜ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì„±ê³µ")
                    return enhanced_structure
                else:
                    raise ValueError("JSON ì‘ë‹µì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ")
                    
            except Exception as parse_error:
                logger.error(f"âŒ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {parse_error}")
                logger.debug(f"ğŸ“„ ë¬¸ì œê°€ ëœ ì‘ë‹µ: {response[:500]}")
                return self._fallback_structure_analysis_with_llm_attempt(text, file_extension, str(parse_error))
                
        except Exception as e:
            logger.error(f"âŒ LLM êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._fallback_structure_analysis_with_llm_attempt(text, file_extension, str(e))

    def _call_openai_chat(self, prompt: str, conf: Dict[str, Any]) -> str:
        """OpenAI Chat Completions í˜¸ì¶œ (ë‹¨ìˆœ ë¬¸ìì—´ ì‘ë‹µ)."""
        import requests
        import logging
        logger = logging.getLogger(__name__)
        api_key = conf.get("api_key")
        base_url = conf.get("base_url", "https://api.openai.com/v1")
        model = conf.get("model", "gpt-3.5-turbo")
        max_tokens = conf.get("max_tokens", 1000)
        temperature = conf.get("temperature", 0.2)
        if not api_key:
            raise RuntimeError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        url = f"{base_url}/chat/completions"
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
        r = requests.post(url, headers=headers, json=payload, timeout=120)
        r.raise_for_status()
        data = r.json()
        return data["choices"][0]["message"]["content"]

    def _call_gemini_generate(self, prompt: str, conf: Dict[str, Any]) -> str:
        """Google Gemini GenerateContent í˜¸ì¶œ (v1beta REST)."""
        import requests
        import logging
        logger = logging.getLogger(__name__)
        api_key = conf.get("api_key")
        base_url = conf.get("base_url", "https://generativelanguage.googleapis.com")
        model = conf.get("model", "models/gemini-1.5-pro")
        max_tokens = conf.get("max_tokens", 1000)
        temperature = conf.get("temperature", 0.2)
        if not api_key:
            raise RuntimeError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        url = f"{base_url}/v1beta/{model}:generateContent?key={api_key}"
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens
            }
        }
        r = requests.post(url, json=payload, timeout=120)
        r.raise_for_status()
        data = r.json()
        candidates = data.get("candidates", [])
        if not candidates:
            return ""
        parts = candidates[0].get("content", {}).get("parts", [])
        return "\n".join(part.get("text", "") for part in parts if isinstance(part, dict))
    
    def _extract_json_from_response(self, response: str) -> Optional[Dict[str, Any]]:
        """LLM ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ì„ ì¶”ì¶œ"""
        import json
        import re
        
        # JSON ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ
        if "```json" in response:
            json_start = response.find("```json") + 7
            json_end = response.find("```", json_start)
            if json_end != -1:
                json_text = response[json_start:json_end].strip()
            else:
                json_text = response[json_start:].strip()
        elif "```" in response:
            json_start = response.find("```") + 3
            json_end = response.find("```", json_start)
            if json_end != -1:
                json_text = response[json_start:json_end].strip()
            else:
                json_text = response[json_start:].strip()
        else:
            # ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€ ì¶”ì¶œ
            start = response.find('{')
            end = response.rfind('}')
            if start != -1 and end != -1 and end > start:
                json_text = response[start:end+1]
            else:
                json_text = response
        
        try:
            # ê¸°ë³¸ JSON íŒŒì‹± ì‹œë„
            return json.loads(json_text)
        except json.JSONDecodeError:
            try:
                # ê°„ë‹¨í•œ JSON ìˆ˜ì • ì‹œë„
                cleaned_json = self._repair_json(json_text)
                return json.loads(cleaned_json)
            except:
                return None
    
    def _repair_json(self, json_text: str) -> str:
        """ê°„ë‹¨í•œ JSON ìˆ˜ì •"""
        import re
        
        # ìŠ¤ë§ˆíŠ¸ ì¸ìš©ë¶€í˜¸ë¥¼ ASCIIë¡œ ë³€í™˜
        json_text = json_text.replace(""", '"').replace(""", '"').replace("'", "'")
        
        # ë‹¨ì¼ ì¸ìš©ë¶€í˜¸ë¥¼ ì´ì¤‘ ì¸ìš©ë¶€í˜¸ë¡œ ë³€í™˜
        json_text = re.sub(r"'([^']*)':", r'"\1":', json_text)  # í‚¤
        json_text = re.sub(r":\s*'([^']*)'", r': "\1"', json_text)  # ê°’
        
        # ëì— ë¶™ì€ ì‰¼í‘œ ì œê±°
        json_text = re.sub(r",\s*([}\]])", r"\1", json_text)
        
        # ì œì–´ ë¬¸ì ì œê±°
        json_text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', json_text)
        
        return json_text.strip()
    
    def _fallback_structure_analysis(self, text: str, file_extension: str) -> Dict[str, Any]:
        """LLMì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ê¸°ë³¸ êµ¬ì¡° ë¶„ì„ìœ¼ë¡œ í´ë°±"""
        basic_structure = self.analyze_document_structure(text, file_extension)
        basic_structure.update({
            "analysis_method": "basic_only",
            "llm_success": False,
            "llm_error": "LLM extraction disabled"
        })
        return basic_structure
    
    def _fallback_structure_analysis_with_llm_attempt(self, text: str, file_extension: str, error_msg: str) -> Dict[str, Any]:
        """LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡° ë¶„ì„ìœ¼ë¡œ í´ë°±"""
        basic_structure = self.analyze_document_structure(text, file_extension)
        basic_structure.update({
            "analysis_method": "basic_fallback",
            "llm_success": False,
            "llm_error": error_msg
        })
        return basic_structure
    
    def extract_keywords(self, content: str, extractors: Optional[List[str]] = None, filename: str = "local_analysis.txt") -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ ìˆ˜í–‰"""
        if extractors is None:
            extractors = ConfigService.get_json_config(
                self.db, "DEFAULT_EXTRACTORS", ["llm"]
            )
        
        # ExtractorManagerë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.extractor_manager.extract_keywords(content, extractors, filename)
        
        # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
        result = []
        for keyword in keywords:
            result.append({
                "keyword": keyword.keyword,
                "extractor_name": keyword.extractor_name,
                "score": keyword.score,
                "category": keyword.category,
                "start_position": keyword.start_position,
                "end_position": keyword.end_position,
                "context_snippet": keyword.context_snippet,
                "page_number": keyword.page_number,
                "line_number": keyword.line_number,
                "column_number": keyword.column_number
            })
        
        return result
    
    def analyze_file(self, file_path: str, extractors: Optional[List[str]] = None, force_reanalyze: bool = False, use_docling: bool = False) -> Dict[str, Any]:
        """íŒŒì¼ ë¶„ì„ ìˆ˜í–‰
        
        Args:
            file_path: ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ
            extractors: ì‚¬ìš©í•  ì¶”ì¶œê¸° ëª©ë¡
            force_reanalyze: ì¬ë¶„ì„ ì—¬ë¶€
            use_docling: Docling íŒŒì„œ ì‚¬ìš© ì—¬ë¶€ (PDF íŒŒì¼ì—ë§Œ ì ìš©)
        """
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë° í˜•ì‹ í™•ì¸
        if not self.file_exists(file_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        if not self.is_supported_file(file_path):
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_path}")
        
        # ê¸°ì¡´ ê²°ê³¼ í™•ì¸
        if not force_reanalyze:
            existing_result = self.load_existing_result(file_path)
            if existing_result:
                return existing_result
        
        # ì¬ë¶„ì„ì˜ ê²½ìš° ê¸°ì¡´ ê²°ê³¼ ë°±ì—…
        if force_reanalyze:
            backup_path = self.backup_existing_result(file_path)
            if backup_path:
                print(f"ê¸°ì¡´ ê²°ê³¼ë¥¼ ë°±ì—…í–ˆìŠµë‹ˆë‹¤: {backup_path}")
        
        try:
            # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
            absolute_path = self.get_absolute_path(file_path)
            
            # DocumentParserServiceë¥¼ í†µí•´ ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ í™•ì¸
            from services.document_parser_service import DocumentParserService
            parser_service = DocumentParserService()
            
            content = None
            parsing_used = "new_parsing"
            
            # ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ í™•ì¸
            if parser_service.has_parsing_results(absolute_path):
                try:
                    # ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    existing_results = parser_service.load_existing_parsing_results(absolute_path)
                    best_parser = existing_results.get("summary", {}).get("best_parser")
                    
                    if best_parser and best_parser in existing_results.get("parsing_results", {}):
                        parser_dir = parser_service.get_output_directory(absolute_path) / best_parser
                        text_file = parser_dir / f"{best_parser}_text.txt"
                        if text_file.exists():
                            with open(text_file, 'r', encoding='utf-8') as f:
                                content = f.read()
                                parsing_used = f"existing_{best_parser}"
                                print(f"âœ… ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ ì¬ì‚¬ìš©: {best_parser} ({len(content)} ë¬¸ì)")
                except Exception as e:
                    print(f"âš ï¸ ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ íŒŒì‹±: {e}")
            
            # íŒŒì‹± ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ìƒˆë¡œ íŒŒì‹±
            if not content:
                content = self.parse_file_content(file_path, use_docling=use_docling)
                parsing_used = "new_parsing"
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self.extract_keywords(content, extractors, filename=absolute_path.name)
            
            # íŒŒì¼ í†µê³„
            file_stats = absolute_path.stat()
            
            # í‚¤ì›Œë“œë¥¼ ì¶”ì¶œê¸°ë³„ë¡œ ê·¸ë£¹í™”
            grouped_keywords = {}
            for keyword in keywords:
                extractor_name = keyword["extractor_name"]
                if extractor_name not in grouped_keywords:
                    grouped_keywords[extractor_name] = []
                grouped_keywords[extractor_name].append(keyword)
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                "file_info": {
                    "path": file_path,
                    "absolute_path": str(absolute_path),
                    "size": file_stats.st_size,
                    "modified": datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
                    "extension": absolute_path.suffix.lower()
                },
                "content_info": {
                    "length": len(content),
                    "word_count": len(content.split()),
                    "line_count": len(content.splitlines())
                },
                "extraction_info": {
                    "extractors_used": extractors if extractors is not None else ConfigService.get_json_config(
                        self.db, "DEFAULT_EXTRACTORS", ["llm"]
                    ),
                    "total_keywords": len(keywords),
                    "parsing_method": parsing_used
                },
                "keywords": grouped_keywords,
                "analysis_status": "completed"
            }
            
            # ê²°ê³¼ ì €ì¥
            result_file_path = self.save_result(file_path, result)
            result["result_file"] = result_file_path
            
            return result
            
        except Exception as e:
            error_result = {
                "file_info": {
                    "path": file_path,
                    "error": str(e)
                },
                "analysis_status": "failed",
                "error_message": str(e)
            }
            
            # ì˜¤ë¥˜ë„ ì €ì¥
            try:
                self.save_result(file_path, error_result)
            except:
                pass
            
            raise e
