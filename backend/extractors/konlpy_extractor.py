from typing import List, Dict, Any, Optional
from pathlib import Path
from collections import Counter
import time
from .base import KeywordExtractor, Keyword
from utils.text_cleaner import TextCleaner
from utils.position_mapper import PositionMapper
from utils.debug_logger import get_debug_logger

class KoNLPyExtractor(KeywordExtractor):
    """KoNLPy ê¸°ë°˜ í•œêµ­ì–´ ëª…ì‚¬ ì¶”ì¶œê¸°"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("konlpy", config)
        self.tagger = None
        self.tagger_type = config.get('tagger', 'Okt') if config else 'Okt'  # Okt, Komoran, Hannanum ë“±
    
    def load_model(self) -> bool:
        """KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            import logging
            logger = logging.getLogger(__name__)
            
            logger.info(f"ğŸ“¦ KoNLPy '{self.tagger_type}' í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì‹œì‘...")
            
            # KoNLPy ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
            from konlpy.tag import Okt, Komoran, Hannanum, Kkma
            
            # ì§€ì›ë˜ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸° ëª©ë¡
            available_taggers = {
                'Okt': Okt,
                'Komoran': Komoran, 
                'Hannanum': Hannanum,
                'Kkma': Kkma
            }
            
            # ìš”ì²­ëœ í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸
            if self.tagger_type not in available_taggers:
                logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°: '{self.tagger_type}', Oktë¡œ ëŒ€ì²´ë¨")
                self.tagger_type = 'Okt'
            
            # í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì‹œë„
            tagger_class = available_taggers[self.tagger_type]
            logger.info(f"ğŸ”„ '{self.tagger_type}' í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
            
            self.tagger = tagger_class()
            self.actual_tagger_type = self.tagger_type
            self.is_loaded = True
            
            logger.info(f"âœ… KoNLPy '{self.tagger_type}' í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì„±ê³µ")
            return True
            
        except ImportError as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"âŒ KoNLPy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {e}")
            logger.info(f"ğŸ’¡ KoNLPy ì„¤ì¹˜ ëª…ë ¹ì–´: pip install konlpy")
            self.is_loaded = False
            return False
            
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"âŒ KoNLPy '{self.tagger_type}' í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            return False
    
    def extract(self, text: str, file_path: Optional[Path] = None) -> List[Keyword]:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ì—¬ í‚¤ì›Œë“œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
        import logging
        logger = logging.getLogger(__name__)
        debug_logger = get_debug_logger()
        start_time = time.time()
        
        logger.info(f"ğŸ” KoNLPy í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘ - ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        # ë””ë²„ê·¸ ë¡œê¹…: ì¶”ì¶œ ì‹œì‘
        debug_logger.start_extraction(
            extractor_name="konlpy",
            file_info={"filename": str(file_path) if file_path else "unknown", "id": None},
            text=text,
            config=self.config
        )
        
        # ìœ„ì¹˜ ë§¤í•‘ ìƒì„±
        position_mapper = PositionMapper()
        position_map = position_mapper.create_position_map(text, file_path)
        logger.info(f"ğŸ“ ìœ„ì¹˜ ë§¤í•‘ ìƒì„± ì™„ë£Œ - ì´ {position_map['total_pages']}í˜ì´ì§€, {position_map['total_lines']}ì¤„")
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        original_text_copy = text  # ì›ë³¸ ë³´ê´€
        cleaned_text = TextCleaner.clean_text(text)
        logger.info(f"ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ - ì •ì œëœ ê¸¸ì´: {len(cleaned_text)} ë¬¸ì")
        
        # ë””ë²„ê·¸ ë¡œê¹…: ì „ì²˜ë¦¬ ê²°ê³¼
        debug_logger.log_preprocessing(
            extractor_name="konlpy",
            original_text=original_text_copy,
            preprocessed_text=cleaned_text,
            preprocessing_steps=["clean_text", "normalize_korean", "konlpy_preprocessing"]
        )
        
        if not self.is_loaded:
            logger.info("ğŸ“¦ KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì‹œë„...")
            self.load_model()
        
        if not self.is_loaded:
            logger.warning("âš ï¸ KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì‹¤íŒ¨, íŒ¨í„´ ê¸°ë°˜ ì¶”ì¶œë¡œ fallback")
            return self._extract_korean_nouns_simple(cleaned_text, text, position_mapper, position_map)
        
        try:
            logger.info(f"ğŸ¯ KoNLPy '{getattr(self, 'actual_tagger_type', self.tagger_type)}'ë¡œ ëª…ì‚¬ ì¶”ì¶œ ì¤‘...")
            # ì‹¤ì œ KoNLPy ì‚¬ìš©
            keywords = self._extract_korean_nouns_konlpy(cleaned_text, text, position_mapper, position_map)
            
            # ìµœì¢… ê²°ê³¼ ë¡œê¹…
            extraction_time = time.time() - start_time
            debug_logger.log_final_results(
                extractor_name="konlpy",
                final_keywords=[{
                    "keyword": kw.text,
                    "score": kw.score,
                    "category": kw.category,
                    "start_position": kw.start_position,
                    "end_position": kw.end_position,
                    "page_number": kw.page_number,
                    "line_number": kw.line_number,
                    "context": kw.context_snippet
                } for kw in keywords],
                extraction_time=extraction_time,
                total_processing_time=time.time() - start_time
            )
            
            return keywords
        except Exception as e:
            logger.error(f"âŒ KoNLPy ëª…ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨, íŒ¨í„´ ê¸°ë°˜ ì¶”ì¶œë¡œ fallback: {e}")
            fallback_keywords = self._extract_korean_nouns_simple(cleaned_text, text, position_mapper, position_map)
            
            # í´ë°± ê²°ê³¼ ë¡œê¹…
            extraction_time = time.time() - start_time
            debug_logger.log_final_results(
                extractor_name="konlpy",
                final_keywords=[{
                    "keyword": kw.text,
                    "score": kw.score,
                    "category": kw.category,
                    "start_position": kw.start_position,
                    "end_position": kw.end_position,
                    "context": kw.context_snippet
                } for kw in fallback_keywords],
                extraction_time=extraction_time,
                total_processing_time=time.time() - start_time
            )
            
            return fallback_keywords
        finally:
            # ë””ë²„ê·¸ ì„¸ì…˜ ì €ì¥
            debug_logger.save_debug_session()
    
    def _extract_korean_nouns_konlpy(self, text: str, original_text: str, position_mapper: PositionMapper, position_map: Dict[str, any]) -> List[Keyword]:
        """KoNLPyë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ í•œêµ­ì–´ ëª…ì‚¬ ì¶”ì¶œ"""
        import logging
        import time
        logger = logging.getLogger(__name__)
        debug_logger = get_debug_logger()
        
        try:
            # KoNLPy ì„¤ì • ë¡œê¹…
            max_keywords = self.config.get('max_keywords', 15) if self.config else 15
            min_length = self.config.get('min_length', 2) if self.config else 2
            min_frequency = self.config.get('min_frequency', 1) if self.config else 1
            
            actual_tagger_name = getattr(self, 'actual_tagger_type', self.tagger_type)
            logger.info(f"âš™ï¸ KoNLPy ì„¤ì • - í˜•íƒœì†Œë¶„ì„ê¸°: {actual_tagger_name}, ìµœëŒ€ëª…ì‚¬: {max_keywords}, ìµœì†Œê¸¸ì´: {min_length}, ìµœì†Œë¹ˆë„: {min_frequency}")
            
            # ë””ë²„ê·¸ ë¡œê¹…: ëª¨ë¸ ì •ë³´
            debug_logger.log_embeddings(
                extractor_name="konlpy",
                model_name=f"konlpy_{actual_tagger_name}"
            )
            
            # KoNLPy í˜•íƒœì†Œ ë¶„ì„ ì‹œì‘
            start_time = time.time()
            logger.info(f"ğŸ” KoNLPy í˜•íƒœì†Œ ë¶„ì„ ì‹œì‘ (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì)...")
            
            # ëª…ì‚¬ë§Œ ì¶”ì¶œ (í’ˆì‚¬ê°€ Nìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒë“¤)
            nouns = self.tagger.nouns(text)
            processing_time = time.time() - start_time
            logger.info(f"âš¡ KoNLPy í˜•íƒœì†Œ ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {processing_time:.3f}ì´ˆ)")
            
            # ë””ë²„ê·¸ ë¡œê¹…: í›„ë³´ ìƒì„±
            debug_logger.log_candidate_generation(
                extractor_name="konlpy",
                candidates=nouns,
                generation_method=f"konlpy_{actual_tagger_name}_nouns",
                params={
                    "tagger": actual_tagger_name,
                    "processing_time": processing_time,
                    "total_nouns_found": len(nouns),
                    "min_length": min_length,
                    "min_frequency": min_frequency
                }
            )
            
            # ëª…ì‚¬ í•„í„°ë§ ë° ì „ì²˜ë¦¬
            logger.info(f"ğŸ”„ ëª…ì‚¬ í•„í„°ë§ ì‹œì‘ ({len(nouns)}ê°œ ì²˜ë¦¬ ì˜ˆì •)...")
            
            valid_nouns = []
            filtered_count = 0
            
            for noun in nouns:
                # ê¸¸ì´ ì²´í¬
                if len(noun) < min_length:
                    filtered_count += 1
                    continue
                
                # ìœ íš¨ì„± ì²´í¬ (TextCleaner ì‚¬ìš©)
                if not TextCleaner.is_meaningful_keyword(noun):
                    filtered_count += 1
                    continue
                
                # ì •ê·œí™”
                normalized_noun = TextCleaner.normalize_keyword(noun)
                if not normalized_noun:
                    filtered_count += 1
                    continue
                
                valid_nouns.append(normalized_noun)
            
            logger.info(f"ğŸ§¹ ëª…ì‚¬ í•„í„°ë§ ì™„ë£Œ - ìœ íš¨: {len(valid_nouns)}ê°œ, ì œì™¸: {filtered_count}ê°œ")
            
            if not valid_nouns:
                logger.warning("âš ï¸ KoNLPyì—ì„œ ìœ íš¨í•œ ëª…ì‚¬ë¥¼ ì°¾ì§€ ëª»í•¨")
                return []
            
            # ë¹ˆë„ìˆ˜ ê³„ì‚°
            noun_counts = Counter(valid_nouns)
            
            # ìµœì†Œ ë¹ˆë„ í•„í„°ë§
            filtered_nouns = {
                noun: count for noun, count in noun_counts.items()
                if count >= min_frequency
            }
            
            if not filtered_nouns:
                logger.warning(f"âš ï¸ ìµœì†Œ ë¹ˆë„({min_frequency}) ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ëª…ì‚¬ê°€ ì—†ìŒ")
                return []
            
            # ë¹ˆë„ìˆ˜ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
            max_count = max(filtered_nouns.values())
            logger.info(f"ğŸ“Š ëª…ì‚¬ ë¹ˆë„ ë¶„ì„ - ì´ ê³ ìœ ëª…ì‚¬: {len(filtered_nouns)}ê°œ, ìµœëŒ€ë¹ˆë„: {max_count}")
            
            # ë””ë²„ê·¸ ë¡œê¹…: ìœ ì‚¬ë„ ê³„ì‚° (ë¹ˆë„ìˆ˜ ê¸°ë°˜)
            frequencies = list(filtered_nouns.values())
            candidates_only = list(filtered_nouns.keys())
            import numpy as np
            frequency_array = np.array(frequencies, dtype=float)
            normalized_frequencies = frequency_array / max_count  # ì •ê·œí™”
            
            debug_logger.log_similarity_calculation(
                extractor_name="konlpy",
                similarities=normalized_frequencies,
                candidates=candidates_only,
                method="frequency_based"
            )
            
            results = []
            processed_nouns = 0
            
            # ëª…ì‚¬ë¥¼ í‚¤ì›Œë“œë¡œ ë³€í™˜
            for noun, count in filtered_nouns.items():
                processed_nouns += 1
                
                # ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚° (ë¹ˆë„ ê¸°ë°˜)
                frequency_score = count / max_count
                # ê¸¸ì´ ë³´ë„ˆìŠ¤ (ê¸´ ëª…ì‚¬ì¼ìˆ˜ë¡ ì¤‘ìš”í•  ê°€ëŠ¥ì„± ë†’ìŒ)
                length_bonus = min(0.2, (len(noun) - 2) * 0.05)
                final_score = min(1.0, frequency_score + length_bonus)
                
                # í…ìŠ¤íŠ¸ì—ì„œ ìœ„ì¹˜ ì°¾ê¸°
                positions = self._find_keyword_positions(text, noun)
                
                # ê° ìœ„ì¹˜ë§ˆë‹¤ í‚¤ì›Œë“œ ê°ì²´ ìƒì„±
                for start_pos, end_pos in positions:
                    context = self._extract_context(text, start_pos, end_pos)
                    page_number, line_number, column_number = position_mapper.get_position_info(start_pos, position_map)
                    results.append(Keyword(
                        text=noun,
                        score=final_score,
                        extractor=self.name,
                        category="noun",
                        start_position=start_pos,
                        end_position=end_pos,
                        context_snippet=context,
                        page_number=page_number,
                        line_number=line_number,
                        column_number=column_number
                    ))
                
                # ì§„í–‰ë¥  ë¡œê¹… (ìƒìœ„ 5ê°œë§Œ)
                if processed_nouns <= 5:
                    logger.info(f"  ğŸ“ [{processed_nouns}/{len(filtered_nouns)}] '{noun}' - ë¹ˆë„: {count}, ì ìˆ˜: {final_score:.3f}, ìœ„ì¹˜: {len(positions)}ê³³")
            
            # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ìˆœ ì •ë ¬
            logger.info(f"ğŸ”§ í›„ì²˜ë¦¬ ì‹œì‘ - ì¤‘ë³µ ì œê±° ë° ì •ë ¬ ({len(results)}ê°œ â†’ ìµœëŒ€ {max_keywords}ê°œ)")
            
            # ì¤‘ë³µ ì œê±° (ê°™ì€ í…ìŠ¤íŠ¸, ê°™ì€ ìœ„ì¹˜)
            unique_results = {}
            duplicates_removed = 0
            
            for result in results:
                key = (result.text, result.start_position, result.end_position)
                if key not in unique_results or unique_results[key].score < result.score:
                    if key in unique_results:
                        duplicates_removed += 1
                    unique_results[key] = result
            
            if duplicates_removed > 0:
                logger.info(f"ğŸ§¹ ì¤‘ë³µ ì œê±°: {duplicates_removed}ê°œ ëª…ì‚¬ ì œê±°")
            
            final_results = list(unique_results.values())
            final_results.sort(key=lambda x: x.score, reverse=True)
            
            # ìƒìœ„ Nê°œë§Œ ë°˜í™˜
            before_limit = len(final_results)
            final_results = final_results[:max_keywords]
            
            if before_limit > max_keywords:
                logger.info(f"ğŸ“Š ìƒìœ„ í‚¤ì›Œë“œ ì œí•œ: {before_limit}ê°œ â†’ {len(final_results)}ê°œ")
            
            # ë””ë²„ê·¸ ë¡œê¹…: ìœ„ì¹˜ ë¶„ì„ ê²°ê³¼
            keywords_with_positions = []
            for kw in final_results:
                kw_data = {
                    "keyword": kw.text,
                    "score": kw.score,
                    "positions": []
                }
                if kw.start_position is not None:
                    kw_data["positions"].append({
                        "start": kw.start_position,
                        "end": kw.end_position,
                        "page": kw.page_number,
                        "line": kw.line_number,
                        "context": kw.context_snippet
                    })
                keywords_with_positions.append(kw_data)
            
            debug_logger.log_position_analysis(
                extractor_name="konlpy",
                keywords_with_positions=keywords_with_positions,
                text=text,
                analysis_method="simple_text_search"
            )
            
            # ìµœì¢… ê²°ê³¼ ë¡œê¹…
            if final_results:
                top_nouns = [f"{kw.text}({kw.score:.3f})" for kw in final_results[:5]]
                logger.info(f"âœ… KoNLPy ëª…ì‚¬ ì¶”ì¶œ ì™„ë£Œ - ì´ {len(final_results)}ê°œ ëª…ì‚¬ (ì²˜ë¦¬: {processed_nouns}, ìœ íš¨: {len(valid_nouns)})")
                logger.info(f"ğŸ† ìƒìœ„ ëª…ì‚¬: {', '.join(top_nouns)}")
            else:
                logger.warning("âš ï¸ KoNLPy ì²˜ë¦¬ í›„ ìœ íš¨í•œ ëª…ì‚¬ê°€ ì—†ìŒ")
            
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ KoNLPy ëª…ì‚¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    def _extract_korean_nouns_simple(self, text: str, original_text: str, position_mapper: PositionMapper, position_map: Dict[str, any]) -> List[Keyword]:
        """ê°„ë‹¨í•œ í•œêµ­ì–´ ëª…ì‚¬ ì¶”ì¶œ (KoNLPy ì—†ì´)"""
        import re
        
        # ìœ íš¨í•œ ë‹¨ì–´ ì¶”ì¶œ (TextCleaner ì‚¬ìš©)
        min_length = self.config.get('min_length', 2) if self.config else 2
        valid_words = TextCleaner.extract_valid_words(text, min_length=min_length)
        
        # í•œêµ­ì–´ ëª…ì‚¬ í•„í„°ë§ (í•œê¸€ë¡œë§Œ êµ¬ì„±ëœ ë‹¨ì–´)
        korean_nouns = []
        for word in valid_words:
            if re.match(r'^[ê°€-í£]+$', word) and self._is_likely_noun(word):
                korean_nouns.append(word)
        
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        noun_counts = Counter(korean_nouns)
        
        # ìµœì†Œ ë¹ˆë„ í•„í„°ë§
        min_frequency = self.config.get('min_frequency', 1) if self.config else 1
        filtered_nouns = {
            noun: count for noun, count in noun_counts.items()
            if count >= min_frequency
        }
        
        if not filtered_nouns:
            return []
        
        # ë¹ˆë„ìˆ˜ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
        max_count = max(filtered_nouns.values())
        
        results = []
        for noun, count in filtered_nouns.items():
            score = min(1.0, count / max(1, max_count))  # ì •ê·œí™”ëœ ì ìˆ˜
            
            # í…ìŠ¤íŠ¸ì—ì„œ ìœ„ì¹˜ ì°¾ê¸°
            positions = self._find_keyword_positions(text, noun)
            
            for start_pos, end_pos in positions:
                context = self._extract_context(text, start_pos, end_pos)
                page_number, line_number, column_number = position_mapper.get_position_info(start_pos, position_map)
                results.append(Keyword(
                    text=noun,
                    score=score,
                    extractor=self.name,
                    category="noun",
                    start_position=start_pos,
                    end_position=end_pos,
                    context_snippet=context,
                    page_number=page_number,
                    line_number=line_number,
                    column_number=column_number
                ))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ í‚¤ì›Œë“œë§Œ ë°˜í™˜
        results.sort(key=lambda x: x.score, reverse=True)
        max_keywords = self.config.get('max_keywords', 10) if self.config else 10
        return results[:max_keywords]
    
    def _is_likely_noun(self, word: str) -> bool:
        """ë‹¨ì–´ê°€ ëª…ì‚¬ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ì§€ ê°„ë‹¨íˆ íŒë‹¨"""
        # ê¸°ìˆ  ê´€ë ¨ ëª…ì‚¬ íŒ¨í„´
        tech_patterns = [
            'ê¸°ìˆ ', 'ì‹œìŠ¤í…œ', 'ë°ì´í„°', 'ì •ë³´', 'ë¶„ì„', 'ê°œë°œ', 'ì—°êµ¬', 'ì„œë¹„ìŠ¤', 'í”„ë¡œê·¸ë¨',
            'ì†Œí”„íŠ¸ì›¨ì–´', 'í•˜ë“œì›¨ì–´', 'ë„¤íŠ¸ì›Œí¬', 'ë³´ì•ˆ', 'ì¸í„°ë„·', 'ì›¹ì‚¬ì´íŠ¸', 'ì• í”Œë¦¬ì¼€ì´ì…˜',
            'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì•Œê³ ë¦¬ì¦˜', 'ë¹…ë°ì´í„°', 'í´ë¼ìš°ë“œ', 'ëª¨ë°”ì¼',
            'ìë™í™”', 'ìµœì í™”', 'íš¨ìœ¨í™”', 'ë””ì§€í„¸', 'ì˜¨ë¼ì¸', 'í”Œë«í¼', 'ì†”ë£¨ì…˜'
        ]
        
        # ê¸°ìˆ  ê´€ë ¨ ì–´ê·¼ì´ í¬í•¨ëœ ê²½ìš°
        for pattern in tech_patterns:
            if pattern in word:
                return True
        
        # ëª…ì‚¬ ì–´ë¯¸ íŒ¨í„´
        noun_endings = ['ê¸°ìˆ ', 'ì‹œìŠ¤í…œ', 'ì •ë³´', 'ë°©ë²•', 'ê³¼ì •', 'ê²°ê³¼', 'ëª©ì ', 'ìˆ˜ë‹¨', 'ë„êµ¬', 'ì¥ì¹˜', 'ì„¤ë¹„']
        for ending in noun_endings:
            if word.endswith(ending):
                return True
        
        # ê¸°ë³¸ì ìœ¼ë¡œ 2ê¸€ì ì´ìƒì´ë©´ ëª…ì‚¬ë¡œ ê°„ì£¼
        return len(word) >= 2
    
    def _find_keyword_positions(self, text: str, keyword: str) -> List[tuple]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        positions = []
        start = 0
        
        while True:
            pos = text.find(keyword, start)
            if pos == -1:
                break
            positions.append((pos, pos + len(keyword)))
            start = pos + 1
        return positions
    
    def _extract_context(self, text: str, start_pos: int, end_pos: int, context_size: int = 50) -> str:
        """í‚¤ì›Œë“œ ì£¼ë³€ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        context_start = max(0, start_pos - context_size)
        context_end = min(len(text), end_pos + context_size)
        
        context = text[context_start:context_end]
        
        # ì•ë’¤ì— ìƒëµ í‘œì‹œ ì¶”ê°€
        if context_start > 0:
            context = "..." + context
        if context_end < len(text):
            context = context + "..."
            
        return context