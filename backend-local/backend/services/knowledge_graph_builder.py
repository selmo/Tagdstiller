"""
Knowledge Graph Builder Service

ë¬¸ì„œ ì „ì²´ë¥¼ Knowledge Graphë¡œ ë³€í™˜í•˜ëŠ” ì „ìš© ì„œë¹„ìŠ¤
LLMì„ ì‚¬ìš©í•˜ì—¬ ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ê³  ê·¸ë˜í”„ êµ¬ì¡°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import json
import logging
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from sqlalchemy.orm import Session

from prompts.templates import KnowledgeGraphPrompts, PromptTemplate
from services.local_file_analyzer import LocalFileAnalyzer

logger = logging.getLogger(__name__)


class KnowledgeGraphBuilder:
    """ë¬¸ì„œë¥¼ Knowledge Graphë¡œ ë³€í™˜í•˜ëŠ” ë¹Œë” í´ë˜ìŠ¤"""

    def __init__(self, db: Session):
        self.db = db
        self.analyzer = LocalFileAnalyzer(db)
        self.logger = logging.getLogger(__name__)

    def build_knowledge_graph(
        self,
        text: str,
        file_path: str,
        domain: str = "general",
        structure_info: Optional[Dict[str, Any]] = None,
        llm_config: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œ í…ìŠ¤íŠ¸ì—ì„œ Knowledge Graph ìƒì„±

        Args:
            text: ë¬¸ì„œ í…ìŠ¤íŠ¸
            file_path: íŒŒì¼ ê²½ë¡œ
            domain: ë¬¸ì„œ ë„ë©”ì¸ (general, technical, academic, business, legal)
            structure_info: ë¬¸ì„œ êµ¬ì¡° ì •ë³´ (ì„ íƒ)
            llm_config: LLM ì„¤ì • (ì„ íƒ)

        Returns:
            Knowledge Graph JSON êµ¬ì¡°
        """
        try:
            self.logger.info(f"ğŸ” Knowledge Graph ìƒì„± ì‹œì‘: {Path(file_path).name} (ë„ë©”ì¸: {domain})")

            # 1. ë„ë©”ì¸ë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
            prompt_template = self._get_kg_prompt_template(domain)

            # 2. êµ¬ì¡° ì •ë³´ ê°„ëµí™”
            structure_summary = self._summarize_structure(structure_info) if structure_info else "êµ¬ì¡° ì •ë³´ ì—†ìŒ"

            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = prompt_template.format(
                text=text[:100000],  # ìµœëŒ€ 100K ë¬¸ì (ì•½ 50K í† í°)
                domain=domain,
                structure_info=structure_summary
            )

            # 4. LLM í˜¸ì¶œ (LocalFileAnalyzerì˜ LLM ê¸°ëŠ¥ í™œìš©)
            llm_response = self._call_llm_for_kg(prompt, llm_config or {})

            if not llm_response.get("success"):
                return self._create_error_result(llm_response.get("error", "LLM í˜¸ì¶œ ì‹¤íŒ¨"))

            # 5. LLM ì‘ë‹µ íŒŒì‹±
            raw_response = llm_response.get("response", "")
            self.logger.debug(f"ğŸ” LLM ì›ì‹œ ì‘ë‹µ (ì²˜ìŒ 500ì): {raw_response[:500]}")
            kg_data = self._parse_kg_response(raw_response)

            # 6. ë©”íƒ€ë°ì´í„° ì¶”ê°€
            kg_result = self._enrich_kg_with_metadata(
                kg_data,
                file_path,
                domain,
                structure_info
            )

            self.logger.info(
                f"âœ… Knowledge Graph ìƒì„± ì™„ë£Œ: "
                f"{kg_result['stats']['entity_count']}ê°œ ì—”í‹°í‹°, "
                f"{kg_result['stats']['relationship_count']}ê°œ ê´€ê³„"
            )

            return kg_result

        except Exception as e:
            self.logger.error(f"âŒ Knowledge Graph ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return self._create_error_result(str(e))

    def _get_kg_prompt_template(self, domain: str) -> PromptTemplate:
        """ë„ë©”ì¸ë³„ KG ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ì„ íƒ"""
        domain_prompts = {
            "general": KnowledgeGraphPrompts.GENERAL_KG_EXTRAION,
            "technical": KnowledgeGraphPrompts.TECHNICAL_KG_EXTRACTION,
            "academic": KnowledgeGraphPrompts.ACADEMIC_KG_EXTRACTION,
            "business": KnowledgeGraphPrompts.BUSINESS_KG_EXTRACTION,
            "legal": KnowledgeGraphPrompts.LEGAL_KG_EXTRACTION,
        }
        return domain_prompts.get(domain, KnowledgeGraphPrompts.GENERAL_KG_EXTRAION)

    def _summarize_structure(self, structure_info: Dict[str, Any]) -> str:
        """ë¬¸ì„œ êµ¬ì¡° ì •ë³´ë¥¼ ê°„ëµí•œ í…ìŠ¤íŠ¸ë¡œ ìš”ì•½"""
        if not structure_info:
            return "êµ¬ì¡° ì •ë³´ ì—†ìŒ"

        try:
            summary_parts = []

            # ë¬¸ì„œ ê¸°ë³¸ ì •ë³´
            doc_info = structure_info.get("documentInfo", {})
            if doc_info:
                title = doc_info.get("title", "ì œëª© ì—†ìŒ")
                doc_type = doc_info.get("documentType", "ë¯¸ë¶„ë¥˜")
                summary_parts.append(f"ë¬¸ì„œ: {title} ({doc_type})")

            # êµ¬ì¡° ë¶„ì„ ìš”ì•½
            structure_analysis = structure_info.get("structureAnalysis", [])
            if structure_analysis:
                section_count = len(structure_analysis)
                summary_parts.append(f"{section_count}ê°œ ì£¼ìš” ì„¹ì…˜")

            # í•µì‹¬ ë‚´ìš© ìš”ì•½
            core_content = structure_info.get("coreContent", {})
            if core_content:
                main_topic = core_content.get("mainTopic", "")
                if main_topic:
                    summary_parts.append(f"ì£¼ì œ: {main_topic}")

            return " | ".join(summary_parts) if summary_parts else "êµ¬ì¡° ì •ë³´ ì—†ìŒ"

        except Exception as e:
            self.logger.warning(f"êµ¬ì¡° ì •ë³´ ìš”ì•½ ì‹¤íŒ¨: {e}")
            return "êµ¬ì¡° ì •ë³´ ì²˜ë¦¬ ì˜¤ë¥˜"

    def _call_llm_for_kg(self, prompt: str, llm_config: Dict[str, Any]) -> Dict[str, Any]:
        """LLM í˜¸ì¶œí•˜ì—¬ Knowledge Graph ì¶”ì¶œ"""
        try:
            # LLM ì„¤ì • ì¶”ì¶œ
            provider = llm_config.get("provider", "gemini")

            if provider == "gemini":
                return self._call_gemini_for_kg(prompt, llm_config)
            elif provider == "openai":
                return self._call_openai_for_kg(prompt, llm_config)
            elif provider == "ollama":
                return self._call_ollama_for_kg(prompt, llm_config)
            else:
                return {"success": False, "error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM í”„ë¡œë°”ì´ë”: {provider}"}

        except Exception as e:
            self.logger.error(f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    def _call_gemini_for_kg(self, prompt: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """Gemini API í˜¸ì¶œ (ìë™ ì¬ì‹œë„ í¬í•¨)"""
        import requests
        import time

        api_key = config.get("api_key")
        model = config.get("model", "models/gemini-2.0-flash")
        base_url = config.get("base_url", "https://generativelanguage.googleapis.com")
        timeout = config.get("timeout", 600)
        max_retries = config.get("max_retries", 5)  # 3 â†’ 5ë¡œ ì¦ê°€
        base_delay = config.get("base_delay", 15)  # ê¸°ë³¸ 15ì´ˆ: ëª¨ë“  ìš”ì²­ ì „ ëŒ€ê¸°

        if not api_key:
            return {"success": False, "error": "Gemini API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤"}

        # Gemini API ì—”ë“œí¬ì¸íŠ¸
        url = f"{base_url}/v1beta/{model}:generateContent?key={api_key}"

        # ìš”ì²­ ë³¸ë¬¸
        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }],
            "generationConfig": {
                "temperature": config.get("temperature", 0.1),
                "maxOutputTokens": config.get("max_tokens", 8192),
            }
        }

        # ì¬ì‹œë„ ë¡œì§ (exponential backoff with longer waits + base delay)
        for attempt in range(max_retries):
            try:
                # ëª¨ë“  ìš”ì²­ ì „ ê¸°ë³¸ ëŒ€ê¸° (rate limit íšŒí”¼)
                if attempt == 0 and base_delay > 0:
                    self.logger.info(f"â³ Rate limit íšŒí”¼ ëŒ€ê¸°... {base_delay}ì´ˆ")
                    time.sleep(base_delay)
                elif attempt > 0:
                    # ì¬ì‹œë„ ì‹œ exponential backoff
                    wait_time = (2 ** attempt) * 5  # 5, 10, 20, 40, 80ì´ˆ
                    self.logger.warning(f"â³ Rate limit ëŒ€ê¸° ì¤‘... {wait_time}ì´ˆ ({attempt + 1}/{max_retries})")
                    time.sleep(wait_time)

                api_call_start = time.time()
                self.logger.info(f"ğŸ“¡ Gemini API í˜¸ì¶œ ì‹œì‘... (ëª¨ë¸: {model}, ì‹œë„: {attempt + 1}/{max_retries})")

                response = requests.post(url, json=payload, timeout=timeout)
                response.raise_for_status()

                result = response.json()
                api_call_duration = time.time() - api_call_start

                # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                candidates = result.get("candidates", [])
                if not candidates:
                    return {"success": False, "error": "Gemini ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}

                content = candidates[0].get("content", {})
                parts = content.get("parts", [])

                if not parts:
                    return {"success": False, "error": "Gemini ì‘ë‹µì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"}

                response_text = parts[0].get("text", "")

                # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ (Gemini APIëŠ” usageMetadataì— í† í° ì •ë³´ í¬í•¨)
                usage_metadata = result.get("usageMetadata", {})
                input_tokens = usage_metadata.get("promptTokenCount", 0)
                output_tokens = usage_metadata.get("candidatesTokenCount", 0)
                total_tokens = usage_metadata.get("totalTokenCount", 0)

                self.logger.info(
                    f"âœ… Gemini ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ: {len(response_text):,}ì "
                    f"(ì†Œìš”ì‹œê°„: {api_call_duration:.2f}ì´ˆ, "
                    f"í† í°: ì…ë ¥ {input_tokens:,} + ì¶œë ¥ {output_tokens:,} = ì´ {total_tokens:,})"
                )

                return {
                    "success": True,
                    "response": response_text,
                    "duration": api_call_duration,
                    "tokens": {
                        "input": input_tokens,
                        "output": output_tokens,
                        "total": total_tokens
                    }
                }

            except requests.exceptions.Timeout:
                return {"success": False, "error": f"Gemini API íƒ€ì„ì•„ì›ƒ ({timeout}ì´ˆ)"}
            except requests.exceptions.HTTPError as e:
                # 429 Too Many Requests - ì¬ì‹œë„
                if e.response.status_code == 429 and attempt < max_retries - 1:
                    self.logger.warning(f"âš ï¸ Rate limit ì´ˆê³¼ (429), ì¬ì‹œë„ {attempt + 1}/{max_retries}")
                    continue
                # ë‹¤ë¥¸ HTTP ì—ëŸ¬ ë˜ëŠ” ë§ˆì§€ë§‰ ì¬ì‹œë„ - ì‹¤íŒ¨
                return {"success": False, "error": f"Gemini API ì˜¤ë¥˜: {str(e)}"}
            except requests.exceptions.RequestException as e:
                return {"success": False, "error": f"Gemini API ì˜¤ë¥˜: {str(e)}"}
            except Exception as e:
                self.logger.error(f"Gemini í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
                return {"success": False, "error": str(e)}

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        return {"success": False, "error": f"Gemini API rate limit ì´ˆê³¼ - {max_retries}íšŒ ì¬ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨"}

    def _call_openai_for_kg(self, prompt: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """OpenAI API í˜¸ì¶œ (ìƒˆë¡œìš´ /v1/responses API ì‚¬ìš©)"""
        try:
            import requests

            api_key = config.get("api_key")
            model = config.get("model", "gpt-4")
            base_url = config.get("base_url", "https://api.openai.com/v1")
            timeout = config.get("timeout", 600)

            if not api_key:
                return {"success": False, "error": "OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤"}

            # GPT-5 ëª¨ë¸ì€ ìƒˆë¡œìš´ /v1/responses ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
            use_responses_api = "gpt-5" in model.lower()

            if use_responses_api:
                url = f"{base_url}/responses"
            else:
                url = f"{base_url}/chat/completions"

            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }

            # GPT-5 ëª¨ë¸ì˜ ê²½ìš° ìƒˆë¡œìš´ API í˜•ì‹ ì‚¬ìš©
            if use_responses_api:
                payload = {
                    "model": model,
                    "input": prompt,
                    "reasoning": {
                        "effort": config.get("reasoning_effort", "minimal")  # minimal, medium, high
                    }
                }
                self.logger.info(f"ğŸ“¡ OpenAI /v1/responses API í˜¸ì¶œ (ëª¨ë¸: {model}, reasoning: {payload['reasoning']['effort']})")
            else:
                # ê¸°ì¡´ chat/completions API í˜•ì‹
                max_output = config.get("max_tokens", config.get("max_completion_tokens", 8192))

                payload = {
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": config.get("temperature", 0.1),
                }

                # ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ íŒŒë¼ë¯¸í„° ì‚¬ìš©
                if "o1" in model.lower() or "gpt-4o" in model.lower():
                    payload["max_completion_tokens"] = max_output
                else:
                    payload["max_tokens"] = max_output

                self.logger.info(f"ğŸ“¡ OpenAI /v1/chat/completions API í˜¸ì¶œ (ëª¨ë¸: {model})")

            api_call_start = time.time()
            response = requests.post(url, headers=headers, json=payload, timeout=timeout)

            # 400 ì˜¤ë¥˜ ì‹œ ìƒì„¸ ì—ëŸ¬ ë©”ì‹œì§€ ë¡œê¹…
            if response.status_code == 400:
                try:
                    error_detail = response.json()
                    self.logger.error(f"âŒ OpenAI 400 Bad Request ìƒì„¸:")
                    self.logger.error(f"   - ì˜¤ë¥˜ ë©”ì‹œì§€: {error_detail.get('error', {}).get('message', 'Unknown')}")
                    self.logger.error(f"   - ìš”ì²­ ëª¨ë¸: {model}")
                    self.logger.error(f"   - API ì—”ë“œí¬ì¸íŠ¸: {url}")
                    self.logger.error(f"   - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
                except:
                    pass

            response.raise_for_status()

            result = response.json()

            # ì‘ë‹µ íŒŒì‹± (API í˜•ì‹ì— ë”°ë¼ ë‹¤ë¦„)
            if use_responses_api:
                # GPT-5 API ì‘ë‹µ êµ¬ì¡°: {"output": [reasoning, message], ...}
                if isinstance(result, dict) and "output" in result:
                    output_array = result["output"]

                    # output ë°°ì—´ì—ì„œ 'message' íƒ€ì… ì°¾ê¸°
                    message_obj = None
                    for item in output_array:
                        if item.get('type') == 'message':
                            message_obj = item
                            break

                    if message_obj and 'content' in message_obj:
                        content_list = message_obj['content']
                        for content_item in content_list:
                            if content_item.get('type') == 'output_text':
                                response_text = content_item.get('text', '')
                                self.logger.info(f"âœ… /v1/responses API ì‘ë‹µ íŒŒì‹± ì„±ê³µ: {len(response_text)}ì")
                                break
                        else:
                            # output_textë¥¼ ì°¾ì§€ ëª»í•¨
                            self.logger.error(f"âŒ /v1/responses API: output_textë¥¼ ì°¾ì§€ ëª»í•¨")
                            raise RuntimeError("OpenAI /v1/responses API: output_textë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    else:
                        self.logger.error(f"âŒ /v1/responses API: message ê°ì²´ë¥¼ ì°¾ì§€ ëª»í•¨")
                        raise RuntimeError("OpenAI /v1/responses API: message ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                else:
                    self.logger.error(f"âŒ ì˜ˆìƒí•˜ì§€ ëª»í•œ ì‘ë‹µ í˜•ì‹")
                    raise RuntimeError(f"OpenAI /v1/responses API ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜")
            else:
                # ê¸°ì¡´ chat/completions API
                response_text = result["choices"][0]["message"]["content"]

            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ (OpenAI APIëŠ” usage í•„ë“œì— í† í° ì •ë³´ í¬í•¨)
            usage = result.get("usage", {})
            input_tokens = usage.get("prompt_tokens", 0)
            output_tokens = usage.get("completion_tokens", 0)
            total_tokens = usage.get("total_tokens", 0)

            # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
            api_call_duration = time.time() - api_call_start if 'api_call_start' in locals() else 0

            self.logger.info(
                f"âœ… OpenAI ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ: {len(response_text):,}ì "
                f"(í† í°: ì…ë ¥ {input_tokens:,} + ì¶œë ¥ {output_tokens:,} = ì´ {total_tokens:,})"
            )

            return {
                "success": True,
                "response": response_text,
                "duration": api_call_duration,
                "tokens": {
                    "input": input_tokens,
                    "output": output_tokens,
                    "total": total_tokens
                }
            }

        except requests.exceptions.HTTPError as e:
            # HTTP ì˜¤ë¥˜ ìƒì„¸ ë¡œê¹…
            error_msg = str(e)
            if e.response is not None:
                try:
                    error_detail = e.response.json()
                    error_msg = f"{e} | Detail: {error_detail.get('error', {}).get('message', error_detail)}"
                except:
                    pass
            self.logger.error(f"OpenAI HTTP ì˜¤ë¥˜: {error_msg}", exc_info=True)
            return {"success": False, "error": str(e)}
        except Exception as e:
            self.logger.error(f"OpenAI í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    def _call_ollama_for_kg(self, prompt: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """Ollama API í˜¸ì¶œ"""
        try:
            import requests

            # config ì¤‘ì²© êµ¬ì¡° ì²˜ë¦¬: {"provider": "ollama", "config": {...}}
            ollama_config = config.get("config", config)  # config ì•ˆì˜ configë¥¼ ì‚¬ìš©, ì—†ìœ¼ë©´ ì „ì²´ ì‚¬ìš©

            base_url = ollama_config.get("base_url", "http://localhost:11434")
            model = ollama_config.get("model", "llama3.2")
            timeout = ollama_config.get("timeout", 600)

            url = f"{base_url}/api/generate"
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": ollama_config.get("temperature", 0.1),
                }
            }

            self.logger.info(f"ğŸ“¡ Ollama API í˜¸ì¶œ ì‹œì‘... (ëª¨ë¸: {model}, URL: {base_url})")

            # Retry logic for server errors (500, 502, 503, 504)
            max_retries = 3
            retry_delays = [3, 6, 12]  # seconds

            for attempt in range(max_retries):
                try:
                    response = requests.post(url, json=payload, timeout=timeout)
                    response.raise_for_status()

                    result = response.json()
                    response_text = result.get("response", "")

                    self.logger.info(f"âœ… Ollama ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ: {len(response_text):,}ì")

                    return {"success": True, "response": response_text}

                except requests.exceptions.HTTPError as e:
                    if e.response.status_code in [500, 502, 503, 504]:
                        if attempt < max_retries - 1:
                            delay = retry_delays[attempt]
                            self.logger.warning(
                                f"âš ï¸ Ollama ì„œë²„ ì˜¤ë¥˜ {e.response.status_code}, "
                                f"{delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})"
                            )
                            time.sleep(delay)
                            continue
                        else:
                            self.logger.error(f"âŒ Ollama ì„œë²„ ì˜¤ë¥˜ (ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬): {e}")
                            return {"success": False, "error": str(e)}
                    else:
                        # Other HTTP errors (4xx) - don't retry
                        self.logger.error(f"Ollama HTTP ì˜¤ë¥˜: {e}", exc_info=True)
                        return {"success": False, "error": str(e)}
                except Exception as e:
                    if attempt < max_retries - 1:
                        delay = retry_delays[attempt]
                        self.logger.warning(
                            f"âš ï¸ Ollama í˜¸ì¶œ ì˜¤ë¥˜, {delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries}): {e}"
                        )
                        time.sleep(delay)
                        continue
                    else:
                        raise

        except Exception as e:
            self.logger.error(f"Ollama í˜¸ì¶œ ì˜¤ë¥˜: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    def _parse_kg_response(self, response: str) -> Dict[str, Any]:
        """LLM ì‘ë‹µì„ Knowledge Graph êµ¬ì¡°ë¡œ íŒŒì‹±"""
        try:
            # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
            kg_data = json.loads(response)

            # ê·¸ë˜í”„ êµ¬ì¡° ê²€ì¦
            if "graph" in kg_data:
                # Neo4j/Memgraph ìŠ¤íƒ€ì¼ (nodes, edges)
                return self._normalize_graph_structure(kg_data["graph"])
            elif "entities" in kg_data and "relationships" in kg_data:
                # ê¸°ì¡´ ìŠ¤íƒ€ì¼ (entities, relationships)
                return {
                    "nodes": kg_data.get("entities", []),
                    "edges": kg_data.get("relationships", [])
                }
            else:
                # ìµœìƒìœ„ ë ˆë²¨ì´ ì§ì ‘ ê·¸ë˜í”„ì¸ ê²½ìš°
                return kg_data

        except json.JSONDecodeError as e:
            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ì§„ ê²½ìš° ì˜ˆìƒë˜ëŠ” ìƒí™©ì´ë¯€ë¡œ DEBUG ë ˆë²¨ë¡œ ê¸°ë¡
            self.logger.debug(f"JSON ì§ì ‘ íŒŒì‹± ì‹¤íŒ¨ (ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ ì‹œë„): {e}")
            # ë°±ì—…: ì‘ë‹µì—ì„œ JSON ë¸”ë¡ ì¶”ì¶œ ì‹œë„
            extracted = self._extract_json_from_text(response)
            # ì¶”ì¶œëœ ë°ì´í„°ë„ êµ¬ì¡° ì •ê·œí™” í•„ìš”
            if "graph" in extracted:
                return self._normalize_graph_structure(extracted["graph"])
            elif "entities" in extracted and "relationships" in extracted:
                return {
                    "nodes": extracted.get("entities", []),
                    "edges": extracted.get("relationships", [])
                }
            else:
                return extracted
        except Exception as e:
            self.logger.error(f"KG ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}", exc_info=True)
            return {"nodes": [], "edges": []}

    def _normalize_graph_structure(self, graph: Dict[str, Any]) -> Dict[str, Any]:
        """ê·¸ë˜í”„ êµ¬ì¡° ì •ê·œí™” (nodes, edges í˜•ì‹ìœ¼ë¡œ í†µì¼)"""
        return {
            "nodes": graph.get("nodes", graph.get("entities", [])),
            "edges": graph.get("edges", graph.get("relationships", []))
        }

    def _extract_json_from_text(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ JSON ë¸”ë¡ ì¶”ì¶œ (ë¶ˆì™„ì „í•œ JSON ë³µêµ¬ í¬í•¨)"""
        import re

        # JSON ì½”ë“œ ë¸”ë¡ íŒ¨í„´ (```json ... ```) - greedy ë§¤ì¹­ìœ¼ë¡œ ì „ì²´ JSON ì¶”ì¶œ
        json_block_pattern = r'```(?:json)?\s*(\{.*\})\s*```'
        match = re.search(json_block_pattern, text, re.DOTALL)

        json_str = None
        if match:
            json_str = match.group(1)
            self.logger.info(f"âœ… ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ({len(json_str)}ì)")
        else:
            # ì§ì ‘ { } ë¸”ë¡ ì°¾ê¸° (greedy ë§¤ì¹­)
            brace_pattern = r'\{.*\}'
            match = re.search(brace_pattern, text, re.DOTALL)
            if match:
                json_str = match.group(0)
                self.logger.info(f"âœ… ì¤‘ê´„í˜¸ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ ({len(json_str)}ì)")

        if json_str:
            # ë¨¼ì € ì •ìƒ íŒŒì‹± ì‹œë„
            try:
                return json.loads(json_str)
            except json.JSONDecodeError as e:
                self.logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, ë¶ˆì™„ì „í•œ JSON ë³µêµ¬ ì‹œë„: {e}")
                # ë¶ˆì™„ì „í•œ JSON ë³µêµ¬ ì‹œë„
                repaired = self._repair_incomplete_json(json_str)
                if repaired:
                    try:
                        result = json.loads(repaired)
                        self.logger.info(f"âœ… ë¶ˆì™„ì „í•œ JSON ë³µêµ¬ ì„±ê³µ: {len(result.get('entities', []))}ê°œ ì—”í‹°í‹°")
                        return result
                    except json.JSONDecodeError as e2:
                        self.logger.error(f"âŒ JSON ë³µêµ¬ ì‹¤íŒ¨: {e2}")

                        # ë””ë²„ê·¸: ì‹¤íŒ¨í•œ JSONì„ íŒŒì¼ë¡œ ì €ì¥
                        try:
                            debug_dir = Path("/tmp/kg_json_debug")
                            debug_dir.mkdir(exist_ok=True)
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

                            original_file = debug_dir / f"original_{timestamp}.txt"
                            repaired_file = debug_dir / f"repaired_{timestamp}.txt"
                            error_file = debug_dir / f"error_{timestamp}.txt"

                            with open(original_file, 'w', encoding='utf-8') as f:
                                f.write(json_str)
                            with open(repaired_file, 'w', encoding='utf-8') as f:
                                f.write(repaired)
                            with open(error_file, 'w', encoding='utf-8') as f:
                                f.write(f"Original error: {e}\n")
                                f.write(f"Repair error: {e2}\n")
                                f.write(f"Error position: line {e2.lineno if hasattr(e2, 'lineno') else '?'}, col {e2.colno if hasattr(e2, 'colno') else '?'}\n")

                            self.logger.warning(f"ğŸ› JSON ë””ë²„ê·¸ íŒŒì¼ ì €ì¥: {debug_dir}")
                        except Exception as save_error:
                            self.logger.debug(f"ë””ë²„ê·¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {save_error}")

                        # ë¡œê·¸ì—ë„ ìƒ˜í”Œ ì¶œë ¥
                        self.logger.debug(f"ì›ë³¸ JSON (ì²˜ìŒ 500ì): {json_str[:500]}")
                        self.logger.debug(f"ì›ë³¸ JSON (ë§ˆì§€ë§‰ 500ì): {json_str[-500:]}")

        self.logger.warning("JSON ì¶”ì¶œ ì‹¤íŒ¨, ë¹ˆ ê·¸ë˜í”„ ë°˜í™˜")
        return {"nodes": [], "edges": []}

    def _repair_incomplete_json(self, json_str: str) -> Optional[str]:
        """ë¶ˆì™„ì „í•œ JSONì„ ìˆ˜ì • (LLM ì‘ë‹µì´ ì˜ë ¸ì„ ë•Œ)"""
        try:
            import re

            # 0-0. JSON ì£¼ì„ ì œê±° (LLMì´ ì˜ëª» ìƒì„±í•œ ì£¼ì„)
            # íŒ¨í„´ 1: // ì£¼ì„ (í•œ ì¤„ ì£¼ì„)
            # íŒ¨í„´ 2: /* */ ì£¼ì„ (ë¸”ë¡ ì£¼ì„)
            before_comment = json_str.count('//')
            # ë¬¸ìì—´ ì™¸ë¶€ì˜ // ì£¼ì„ë§Œ ì œê±° (ë¬¸ìì—´ ë‚´ë¶€ // ëŠ” ìœ ì§€)
            json_str = re.sub(r',\s*//[^\n]*', ',', json_str)  # ", // comment" â†’ ","
            json_str = re.sub(r'\}\s*,\s*//[^\n]*', '},', json_str)  # "}, // comment" â†’ "},"
            json_str = re.sub(r'//[^\n]*\n', '\n', json_str)  # ë…ë¦½ ì£¼ì„ ë¼ì¸ ì œê±°
            after_comment = json_str.count('//')
            if before_comment > after_comment:
                self.logger.info(f"ğŸ”§ JSON ì£¼ì„ ì œê±° ì™„ë£Œ ({before_comment - after_comment}ê°œ)")

            # ë¸”ë¡ ì£¼ì„ ì œê±°
            before_block = json_str.count('/*')
            json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)
            after_block = json_str.count('/*')
            if before_block > after_block:
                self.logger.info(f"ğŸ”§ JSON ë¸”ë¡ ì£¼ì„ ì œê±° ì™„ë£Œ ({before_block - after_block}ê°œ)")

            # 0-0-1. ë°°ì—´ì´ ë„ˆë¬´ ì¼ì° ë‹«íŒ ê²½ìš° ìˆ˜ì •
            # íŒ¨í„´ 1: ...}]}\n{"id": ... â†’ ...}},\n{"id": ... (entities ë°°ì—´ ì¡°ê¸° ì¢…ë£Œ)
            # íŒ¨í„´ 2: ...}}],\n{"id": ... â†’ ...}},\n{"id": ... (ì¤‘ê°„ ë°°ì—´ ë‹«í˜)
            early_close_pattern1 = r'(\}\})\s*\]\s*\}\s*,?\s*\n(\s*\{\s*"id":)'
            matches1 = list(re.finditer(early_close_pattern1, json_str))
            if matches1:
                for match in reversed(matches1):
                    json_str = json_str[:match.start()] + match.group(1) + ',\n' + match.group(2) + json_str[match.end():]
                self.logger.info(f"ğŸ”§ ë°°ì—´ ì¡°ê¸° ì¢…ë£Œ ìˆ˜ì • (íŒ¨í„´1: ]}}): {len(matches1)}ê°œ")

            # íŒ¨í„´ 2: ì¤‘ê°„ì— ë°°ì—´ì´ ë‹«íŒ ê²½ìš°
            early_close_pattern2 = r'(\}\})\s*\]\s*,\s*\n(\s*\{\s*"id":)'
            matches2 = list(re.finditer(early_close_pattern2, json_str))
            if matches2:
                for match in reversed(matches2):
                    json_str = json_str[:match.start()] + match.group(1) + ',\n' + match.group(2) + json_str[match.end():]
                self.logger.info(f"ğŸ”§ ë°°ì—´ ì¡°ê¸° ì¢…ë£Œ ìˆ˜ì • (íŒ¨í„´2: ]],): {len(matches2)}ê°œ")

            # íŒ¨í„´ 3: ì—”í‹°í‹° ë’¤ì— ì¶”ê°€ ë‹«ëŠ” ì¤‘ê´„í˜¸ì™€ ì¤„ë°”ê¿ˆ
            # ...}}\n    },\n\n    {"id": ... â†’ ...}},\n\n    {"id": ...
            # ì´ íŒ¨í„´ì€ TableRow ê°™ì€ ì—”í‹°í‹° ë’¤ì— ì˜ëª»ëœ ì¤‘ì²© ë‹«ê¸°ê°€ ìˆì„ ë•Œ ë°œìƒ
            extra_closing_pattern = r'(\}\})\s*\n\s+\}\s*,\s*\n+(\s*\{\s*"id":)'
            matches3 = list(re.finditer(extra_closing_pattern, json_str))
            if matches3:
                for match in reversed(matches3):
                    # ë‹¤ìŒ ì—”í‹°í‹°ì˜ ë“¤ì—¬ì“°ê¸° ë³´ì¡´
                    next_indent = match.group(2)
                    json_str = json_str[:match.start()] + match.group(1) + ',\n' + next_indent + json_str[match.end():]
                self.logger.info(f"ğŸ”§ ì—”í‹°í‹° ë’¤ ì¶”ê°€ ë‹«ëŠ” ì¤‘ê´„í˜¸ ì œê±° (íŒ¨í„´3): {len(matches3)}ê°œ")

            # íŒ¨í„´ 4: ë¬¸ìì—´ ê°’ ë‚´ë¶€ì˜ ì˜ëª»ëœ ì¤‘ê´„í˜¸ (íŠ¹íˆ notes í•„ë“œ)
            # "notes": "Contextual} placeholder" â†’ "notes": "Contextual placeholder"
            # ë¬¸ìì—´ ë‚´ë¶€ì˜ ë‹¨ë… } ë˜ëŠ” { ì œê±°
            string_brace_pattern = r'("\w+"\s*:\s*"[^"]*?)(\}|\{)([^"]*")'
            before_str_fix = json_str.count('"}')

            def fix_string_braces(match):
                """ë¬¸ìì—´ ê°’ ë‚´ë¶€ì˜ ë‹¨ë… ì¤‘ê´„í˜¸ ì œê±°"""
                prefix = match.group(1)  # "field": "text before
                brace = match.group(2)    # } or {
                suffix = match.group(3)   # text after"

                # ì¤‘ê´„í˜¸ê°€ ë¬¸ìì—´ ì¤‘ê°„ì— ìˆìœ¼ë©´ ì œê±°
                # ë‹¨, JSON êµ¬ì¡°ê°€ ì•„ë‹Œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ íŒë‹¨ë˜ëŠ” ê²½ìš°ë§Œ
                if brace in '{}' and not suffix.startswith('}}'):  # ì‹¤ì œ JSON ì¢…ë£Œê°€ ì•„ë‹Œ ê²½ìš°
                    return prefix + suffix  # ì¤‘ê´„í˜¸ ì œê±°
                return match.group(0)  # ë³€ê²½ ì—†ìŒ

            json_str = re.sub(string_brace_pattern, fix_string_braces, json_str)
            after_str_fix = json_str.count('"}')
            if before_str_fix != after_str_fix:
                self.logger.info(f"ğŸ”§ ë¬¸ìì—´ ê°’ ë‚´ë¶€ ì˜ëª»ëœ ì¤‘ê´„í˜¸ ì œê±° (íŒ¨í„´4): {abs(before_str_fix - after_str_fix)}ê°œ")

            # 0. ì—¬ë¶„ì˜ ë‹«ëŠ” ì¤‘ê´„í˜¸ ì œê±° (íŠ¹íˆ }}}+ íŒ¨í„´)
            # íŒ¨í„´ 1: properties ë‚´ë¶€ ì—¬ë¶„ ê´„í˜¸ - "name": "value"}}} â†’ "name": "value"}}
            # íŒ¨í„´ 2: ë°°ì—´ í•­ëª© ì—¬ë¶„ ê´„í˜¸ - {...}}}}, â†’ {...}},

            # íŠ¹ì • íŒ¨í„´ ìˆ˜ì •: properties ëì— ì—¬ë¶„ ì¤‘ê´„í˜¸
            # "name": "value"}}}, â†’ "name": "value"}},
            before_fix = json_str.count('}}}')
            json_str = re.sub(r'("\s*:\s*"[^"]*")\}\}\}', r'\1}}', json_str)
            after_fix = json_str.count('}}}')
            if before_fix > after_fix:
                self.logger.info(f"ğŸ”§ properties ê°’ ë’¤ ì—¬ë¶„ ì¤‘ê´„í˜¸ ì œê±° ({before_fix - after_fix}ê°œ)")

            # ì—¬ì „íˆ }}} íŒ¨í„´ì´ ë‚¨ì•„ìˆìœ¼ë©´ ì¼ê´„ ìˆ˜ì •
            remaining_triple = json_str.count('}}}')
            if remaining_triple > 0:
                json_str = re.sub(r'\}\}\}', '}}', json_str)
                self.logger.info(f"ğŸ”§ ë‚¨ì€ 3ê°œ ì—°ì† ë‹«ëŠ” ì¤‘ê´„í˜¸ ìˆ˜ì • ì™„ë£Œ ({remaining_triple}ê°œ)")

            # ë°°ì—´ ì•„ì´í…œ ë’¤ ì—¬ë¶„ ê´„í˜¸: }}}, â†’ }},
            json_str = re.sub(r'\}\}\},', '}},', json_str)

            # ì´ì¤‘ ë‹«ëŠ” ê´„í˜¸ íŒ¨í„´ ì •ë¦¬
            double_closing_pattern = r'(\{[^}]*\})\s*\}\s*\}'
            before_count = json_str.count('}}')
            json_str = re.sub(r'(\})\s+\}\s*\}', r'\1}', json_str)
            after_count = json_str.count('}}')
            if before_count != after_count:
                self.logger.info(f"ğŸ”§ ì´ì¤‘ ë‹«ëŠ” ê´„í˜¸ ìˆ˜ì • ì™„ë£Œ ({before_count - after_count}ê°œ)")

            # 0-1. ì˜ëª»ëœ ì—”í‹°í‹° í˜•ì‹ ìˆ˜ì •
            # {"id": "n156", "type": "LegalRegulation", "name": "..."}
            # â†’ {"id": "n156", "type": "LegalRegulation", "properties": {"name": "..."}}
            malformed_entity_pattern = r'\{"id":\s*"([^"]+)",\s*"type":\s*"([^"]+)",\s*("name"|"text"|"value"):\s*"([^"]*)"(\s*,\s*"[^"]+"\s*:\s*"[^"]*")*\s*\}'

            def fix_malformed_entity(match):
                entity_id = match.group(1)
                entity_type = match.group(2)
                field_name = match.group(3).strip('"')  # "name" â†’ name
                field_value = match.group(4)

                # ì¶”ê°€ í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                extra_fields = match.group(5) or ""

                # properties í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                properties_content = f'"{field_name}": "{field_value}"'

                # ì¶”ê°€ í•„ë“œê°€ ìˆìœ¼ë©´ properties ì•ˆì— í¬í•¨
                if extra_fields:
                    # ì‰¼í‘œë¡œ ì‹œì‘í•˜ëŠ” ì¶”ê°€ í•„ë“œë¥¼ properties ì•ˆì— ë„£ê¸°
                    properties_content += extra_fields

                result = f'{{"id": "{entity_id}", "type": "{entity_type}", "properties": {{{properties_content}}}}}'
                return result

            fixed_entities_count = 0
            matches = list(re.finditer(malformed_entity_pattern, json_str))
            for match in reversed(matches):  # ë’¤ì—ì„œë¶€í„° ìˆ˜ì •í•˜ì—¬ ì¸ë±ìŠ¤ ìœ ì§€
                fixed = fix_malformed_entity(match)
                json_str = json_str[:match.start()] + fixed + json_str[match.end():]
                fixed_entities_count += 1

            if fixed_entities_count > 0:
                self.logger.info(f"ğŸ”§ ì˜ëª»ëœ ì—”í‹°í‹° í˜•ì‹ {fixed_entities_count}ê°œ ìˆ˜ì • ì™„ë£Œ (properties ì¶”ê°€)")

            # 0-2. properties ë’¤ì— ì¶”ê°€ í•„ë“œê°€ ìˆëŠ” ê²½ìš° ìˆ˜ì •
            # {"properties": {...},"row_count": 17, "column_count": 3}
            # â†’ {"properties": {..., "row_count": 17, "column_count": 3}}
            properties_with_external_fields = r'("properties"\s*:\s*\{[^}]+\})\s*,\s*("([^"]+)"\s*:\s*([^,}\s]+)(?:\s*,\s*"([^"]+)"\s*:\s*([^,}\s]+))*)'

            def fix_properties_external_fields(match):
                """properties ì™¸ë¶€ì˜ í•„ë“œë¥¼ ë‚´ë¶€ë¡œ ì´ë™"""
                properties_part = match.group(1)  # "properties": {...}
                external_fields = match.group(2)  # "field": value, "field2": value2

                # properties ë‚´ìš© ì¶”ì¶œ (ë§ˆì§€ë§‰ } ì œê±°)
                if properties_part.endswith('}'):
                    properties_content = properties_part[:-1]  # "properties": {...
                else:
                    return match.group(0)  # ë³€ê²½ ì—†ìŒ

                # ì™¸ë¶€ í•„ë“œë¥¼ properties ì•ˆìœ¼ë¡œ ì´ë™
                result = properties_content + ", " + external_fields + "}"
                return result

            json_str_before = json_str
            json_str = re.sub(properties_with_external_fields, fix_properties_external_fields, json_str)
            if json_str != json_str_before:
                self.logger.info(f"ğŸ”§ properties ì™¸ë¶€ í•„ë“œë¥¼ ë‚´ë¶€ë¡œ ì´ë™ ì™„ë£Œ")

            # 1. ì œì–´ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ (JSON íŒŒì‹± ì˜¤ë¥˜ ë°©ì§€)
            def escape_control_chars(match):
                """JSON ë¬¸ìì—´ ë‚´ ì œì–´ ë¬¸ìë¥¼ ì´ìŠ¤ì¼€ì´í”„"""
                s = match.group(0)
                # ì œì–´ ë¬¸ìë¥¼ ì´ìŠ¤ì¼€ì´í”„ëœ í˜•íƒœë¡œ ë³€í™˜
                s = s.replace('\n', '\\n')
                s = s.replace('\r', '\\r')
                s = s.replace('\t', '\\t')
                s = s.replace('\b', '\\b')
                s = s.replace('\f', '\\f')
                # ê¸°íƒ€ ì œì–´ ë¬¸ì (ASCII 0-31) ì œê±°
                s = ''.join(char if ord(char) >= 32 or char in '\n\r\t\b\f' else '' for char in s)
                return s

            json_str = re.sub(r'"[^"\\]*(?:\\.[^"\\]*)*"', escape_control_chars, json_str)
            self.logger.info(f"ğŸ”§ ì œì–´ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ ì™„ë£Œ")

            # 2. ì˜ëª»ëœ ë°±ìŠ¬ë˜ì‹œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
            def fix_escapes_in_strings(match):
                """ë¬¸ìì—´ ë‚´ë¶€ì˜ ì˜ëª»ëœ ì´ìŠ¤ì¼€ì´í”„ ìˆ˜ì •

                ìœ íš¨í•œ JSON ì´ìŠ¤ì¼€ì´í”„: " \\ / b f n r t u
                ì˜ëª»ëœ ì´ìŠ¤ì¼€ì´í”„ (ì˜ˆ: \_ \() ëŠ” ë°±ìŠ¬ë˜ì‹œë¥¼ ì œê±°
                """
                s = match.group(0)
                # ì˜ëª»ëœ ì´ìŠ¤ì¼€ì´í”„: ë°±ìŠ¬ë˜ì‹œ ë’¤ì— ìœ íš¨í•˜ì§€ ì•Šì€ ë¬¸ìê°€ ì˜¤ëŠ” ê²½ìš°
                # í•´ê²°: ë°±ìŠ¬ë˜ì‹œë¥¼ ì œê±° (ì˜ˆ: /\_ -> /_, /\( -> /()
                s = re.sub(r'\\(?!["\\/bfnrtu])', '', s)
                return s

            json_str = re.sub(r'"[^"\\]*(?:\\.[^"\\]*)*"', fix_escapes_in_strings, json_str)
            self.logger.info(f"ğŸ”§ ë°±ìŠ¬ë˜ì‹œ ì´ìŠ¤ì¼€ì´í”„ ìˆ˜ì • ì™„ë£Œ (ì˜ëª»ëœ ì´ìŠ¤ì¼€ì´í”„ ì œê±°)")

            # 2. LLMì´ ìƒì„±í•œ ì˜ëª»ëœ JSON íŒ¨í„´ ìˆ˜ì •
            # íŒ¨í„´: {"properties": {...}},\n    "name": "..."}}, â†’ {"properties": {...}}},
            # ì´ëŠ” LLMì´ properties ë‹«ì€ í›„ ì™¸ë¶€ì— ì¤‘ë³µ í•„ë“œë¥¼ ì¶”ê°€í•œ ê²½ìš°
            # ì˜ˆ: {"properties": {"name": "A"}}, "name": "A"}}
            malformed_pattern = r'("properties"\s*:\s*\{[^}]+\})\s*\}\s*,\s*("[^"]+"\s*:\s*"[^"]*")(\s*\}\s*\})'
            def fix_malformed_properties(match):
                properties_part = match.group(1)  # "properties": {...}
                external_field = match.group(2)   # "name": "ê°’"
                closing = match.group(3)           # }}

                # ì™¸ë¶€ í•„ë“œ íŒŒì‹±
                field_match = re.match(r'"(\w+)"\s*:\s*"([^"]*)"', external_field)
                if field_match:
                    field_name = field_match.group(1)
                    field_value = field_match.group(2)

                    # ì™¸ë¶€ í•„ë“œê°€ ì´ë¯¸ properties ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
                    if f'"{field_name}"' in properties_part:
                        # ì¤‘ë³µì´ë¯€ë¡œ ì™¸ë¶€ í•„ë“œ ì œê±°í•˜ê³  propertiesë§Œ ìœ ì§€
                        self.logger.debug(f"ğŸ”§ ì¤‘ë³µ í•„ë“œ '{field_name}' ì œê±° (properties ë‚´ë¶€ì— ì´ë¯¸ ì¡´ì¬)")
                        return properties_part + "}" + closing
                    else:
                        # properties ì•ˆì— ì—†ìœ¼ë©´ ì¶”ê°€
                        self.logger.debug(f"ğŸ”§ ì™¸ë¶€ í•„ë“œ '{field_name}'ë¥¼ properties ì•ˆìœ¼ë¡œ ì´ë™")
                        return properties_part[:-1] + f', "{field_name}": "{field_value}"' + "}}" + closing
                else:
                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€
                    return match.group(0)

            json_str = re.sub(malformed_pattern, fix_malformed_properties, json_str, flags=re.DOTALL)
            fixed_count = len(re.findall(malformed_pattern, json_str, flags=re.DOTALL))
            if fixed_count > 0:
                self.logger.info(f"ğŸ”§ ì˜ëª»ëœ JSON íŒ¨í„´ {fixed_count}ê°œ ìˆ˜ì • ì™„ë£Œ (ì¤‘ë³µ í•„ë“œ ì²˜ë¦¬)")

            # 3. ë¶ˆì™„ì „í•œ JSON ê°ì²´/ë°°ì—´ ì°¾ì•„ì„œ ì œê±°
            # ì „ëµ: ë§ˆì§€ë§‰ ì™„ì „í•œ ê°ì²´/ë°°ì—´ê¹Œì§€ë§Œ ìœ ì§€

            # 2-1. ë‹«ëŠ” ê´„í˜¸ê°€ ë¶€ì¡±í•œ ê²½ìš° ë¨¼ì € ì²˜ë¦¬
            open_braces = json_str.count('{') - json_str.count('}')
            open_brackets = json_str.count('[') - json_str.count(']')

            # 2-2. ë°°ì—´ ë‚´ì—ì„œ ë¶ˆì™„ì „í•œ ë§ˆì§€ë§‰ í•­ëª© ì œê±°
            # "entities": [ {...}, {...}, {ë¶ˆì™„ì „ <- ì—¬ê¸°ë¥¼ ì œê±°
            # ì „ëµ: ë§ˆì§€ë§‰ ì‰¼í‘œ ì´í›„ê°€ ì™„ì „í•œ ê°ì²´ê°€ ì•„ë‹ˆë©´ ë§ˆì§€ë§‰ ì‰¼í‘œë¶€í„° ì œê±°
            if '"entities"' in json_str or '"nodes"' in json_str:
                # ë§ˆì§€ë§‰ ì‰¼í‘œ ìœ„ì¹˜ ì°¾ê¸° (ë¬¸ìì—´ ë‚´ë¶€ ì œì™¸)
                last_comma = -1
                in_string = False
                escape_next = False

                for i, char in enumerate(json_str):
                    if escape_next:
                        escape_next = False
                        continue
                    if char == '\\':
                        escape_next = True
                        continue
                    if char == '"' and not escape_next:
                        in_string = not in_string
                        continue
                    if in_string:
                        continue

                    if char == ',':
                        # ë°°ì—´ ë‚´ë¶€ì˜ ì‰¼í‘œì¸ì§€ í™•ì¸ (depth ì²´í¬ëŠ” ë³µì¡í•˜ë‹ˆ ë‹¨ìˆœí™”)
                        last_comma = i

                # ë§ˆì§€ë§‰ ì‰¼í‘œ ì´í›„ ì²« ë²ˆì§¸ ê°ì²´ê°€ ì™„ì „í•œì§€ í™•ì¸
                if last_comma > 0:
                    after_comma = json_str[last_comma+1:]

                    # ì‰¼í‘œ ì´í›„ ì²« ë²ˆì§¸ ê°ì²´ì˜ ê´„í˜¸ ê· í˜• í™•ì¸
                    first_obj_brace_count = 0
                    in_string = False
                    escape_next = False
                    obj_started = False
                    obj_ended = False

                    for char in after_comma:
                        if escape_next:
                            escape_next = False
                            continue
                        if char == '\\':
                            escape_next = True
                            continue
                        if char == '"' and not escape_next:
                            in_string = not in_string
                            continue
                        if in_string:
                            continue

                        if char == '{':
                            first_obj_brace_count += 1
                            obj_started = True
                        elif char == '}':
                            first_obj_brace_count -= 1
                            if obj_started and first_obj_brace_count == 0:
                                obj_ended = True
                                break  # ì²« ë²ˆì§¸ ê°ì²´ ë
                        elif char in ('[', ']') and not obj_started:
                            # ê°ì²´ ì‹œì‘ ì „ì— ë°°ì—´ ê´„í˜¸ ë‚˜ì˜´ â†’ ì´ë¯¸ ë°°ì—´ ë‹«í˜”ìŒ
                            break

                    # ì²« ë²ˆì§¸ ê°ì²´ê°€ ë¶ˆì™„ì „í•˜ë©´ (ì‹œì‘í–ˆì§€ë§Œ ì•ˆ ë‹«í˜)
                    if obj_started and not obj_ended:
                        json_str = json_str[:last_comma].strip()
                        self.logger.info(f"ğŸ”§ ë¶ˆì™„ì „í•œ ë§ˆì§€ë§‰ í•­ëª© ì œê±° (ê´„í˜¸ ë¶ˆê· í˜•: {first_obj_brace_count})")

                        # ì ì ˆí•œ ë‹«ê¸° ì¶”ê°€
                        if not json_str.rstrip().endswith(']'):
                            json_str = json_str.rstrip() + '\n  ]\n}'
                            self.logger.debug(f"ğŸ”§ ë°°ì—´ ë° ê°ì²´ ë‹«ê¸° ì¶”ê°€")

            # 3. Trailing comma ì œê±° (,] ë˜ëŠ” ,} í˜•íƒœ)
            json_str = re.sub(r',\s*]', ']', json_str)
            json_str = re.sub(r',\s*}', '}', json_str)
            self.logger.debug(f"ğŸ”§ Trailing comma ì œê±° ì™„ë£Œ")

            # 4. ë¹ˆ ê°’ ì²˜ë¦¬ (: , ë˜ëŠ” : } ë˜ëŠ” : ] í˜•íƒœ)
            json_str = re.sub(r':\s*,', ': null,', json_str)
            json_str = re.sub(r':\s*}', ': null}', json_str)
            json_str = re.sub(r':\s*]', ': null]', json_str)
            self.logger.debug(f"ğŸ”§ ë¹ˆ ê°’ì„ nullë¡œ ëŒ€ì²´ ì™„ë£Œ")

            # 5. í•„ìš”í•œ ë‹«ëŠ” ê´„í˜¸ ì¬ê³„ì‚° ë° ì¶”ê°€
            # ë¨¼ì € í˜„ì¬ ìƒíƒœ íŒŒì•… (ë¬¸ìì—´ ë‚´ë¶€ ì œì™¸)
            brace_count = 0
            bracket_count = 0
            in_string = False
            escape_next = False

            for char in json_str:
                if escape_next:
                    escape_next = False
                    continue
                if char == '\\':
                    escape_next = True
                    continue
                if char == '"' and not escape_next:
                    in_string = not in_string
                    continue
                if in_string:
                    continue

                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                elif char == '[':
                    bracket_count += 1
                elif char == ']':
                    bracket_count -= 1

            # í•„ìš”í•œ ë‹«ëŠ” ê´„í˜¸ ì¶”ê°€
            if bracket_count > 0:
                json_str += '\n]' * bracket_count
                self.logger.info(f"ğŸ”§ ë‹«ëŠ” ë°°ì—´ ê´„í˜¸ {bracket_count}ê°œ ì¶”ê°€")

            if brace_count > 0:
                json_str += '\n}' * brace_count
                self.logger.info(f"ğŸ”§ ë‹«ëŠ” ê°ì²´ ê´„í˜¸ {brace_count}ê°œ ì¶”ê°€")

            return json_str
        except Exception as e:
            self.logger.error(f"JSON ë³µêµ¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def _save_checkpoint(
        self,
        checkpoint_file: Path,
        chunk_graphs: list,
        last_completed_idx: int
    ):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ë°ì´í„°ë§Œ ì €ì¥)"""
        try:
            import json

            # chunk_graphsë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            serializable_graphs = []
            for graph in chunk_graphs:
                if isinstance(graph, dict):
                    # ì´ë¯¸ dictì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    serializable_graphs.append(graph)
                elif hasattr(graph, '__dict__'):
                    # ê°ì²´ë©´ __dict__ ì‚¬ìš© (ì£¼ì˜: ì¬ê·€ì ìœ¼ë¡œ ë³€í™˜ í•„ìš”í•  ìˆ˜ ìˆìŒ)
                    # í•˜ì§€ë§Œ ì²´í¬í¬ì¸íŠ¸ëŠ” ë‚˜ì¤‘ì— ë³‘í•©í•  ìˆ˜ ìˆëŠ” graph í˜•íƒœë§Œ ì €ì¥í•˜ë©´ ë¨
                    # graph êµ¬ì¡°: {"nodes": [...], "edges": [...]}
                    if hasattr(graph, 'get'):
                        serializable_graphs.append(graph)
                    else:
                        self.logger.debug(f"âš ï¸ ì§ë ¬í™” ë¶ˆê°€ëŠ¥í•œ ê°ì²´ ê±´ë„ˆë›°ê¸°: {type(graph)}")
                        continue

            checkpoint = {
                "last_completed_idx": last_completed_idx,
                "chunk_graphs": serializable_graphs,
                "timestamp": datetime.now().isoformat()
            }

            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint, f, ensure_ascii=False, indent=2, default=str)

            self.logger.debug(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {last_completed_idx + 1}ê°œ ì²­í¬ ì™„ë£Œ ({len(serializable_graphs)}ê°œ ê·¸ë˜í”„)")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            self.logger.debug(traceback.format_exc())

    def _assign_uuids_to_graph(self, kg_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Knowledge Graphì˜ ëª¨ë“  ë…¸ë“œì™€ ì—£ì§€ì— UUID í• ë‹¹

        LLMì´ ìƒì„±í•œ ID(n1, n2, e1 ë“±)ë¥¼ UUIDë¡œ ë³€í™˜í•˜ì—¬ ì „ì—­ ê³ ìœ ì„± ë³´ì¥
        """
        nodes = kg_data.get("nodes", [])
        edges = kg_data.get("edges", [])

        # ì›ë³¸ ID â†’ UUID ë§¤í•‘
        id_mapping = {}

        # 1. ë…¸ë“œì— UUID í• ë‹¹
        for node in nodes:
            old_id = node.get("id", "")
            new_id = str(uuid.uuid4())
            node["id"] = new_id
            id_mapping[old_id] = new_id

        # 2. ì—£ì§€ì— UUID í• ë‹¹ ë° source/target ì—…ë°ì´íŠ¸
        for edge in edges:
            # ì—£ì§€ IDë¥¼ UUIDë¡œ ë³€ê²½
            edge["id"] = str(uuid.uuid4())

            # source/targetì„ UUIDë¡œ ë§¤í•‘
            old_source = edge.get("source", "")
            old_target = edge.get("target", "")

            edge["source"] = id_mapping.get(old_source, str(uuid.uuid4()))
            edge["target"] = id_mapping.get(old_target, str(uuid.uuid4()))

        return {
            "nodes": nodes,
            "edges": edges
        }

    def _enrich_kg_with_metadata(
        self,
        kg_data: Dict[str, Any],
        file_path: str,
        domain: str,
        structure_info: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Knowledge Graphì— ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        # UUID í• ë‹¹ ë¨¼ì € ìˆ˜í–‰
        kg_data = self._assign_uuids_to_graph(kg_data)

        nodes = kg_data.get("nodes", [])
        edges = kg_data.get("edges", [])

        # í†µê³„ ê³„ì‚°
        entity_types = {}
        relationship_types = {}

        for node in nodes:
            node_type = node.get("type", "Unknown")
            entity_types[node_type] = entity_types.get(node_type, 0) + 1

        for edge in edges:
            edge_type = edge.get("type", "UNKNOWN")
            relationship_types[edge_type] = relationship_types.get(edge_type, 0) + 1

        # ê²°ê³¼ êµ¬ì¡° ìƒì„±
        result = {
            "success": True,
            "file_path": file_path,
            "domain": domain,
            "extraction_date": datetime.now().isoformat(),
            "graph": {
                "nodes": nodes,
                "edges": edges
            },
            "stats": {
                "entity_count": len(nodes),
                "relationship_count": len(edges),
                "entity_types": entity_types,
                "relationship_types": relationship_types,
                "density": self._calculate_graph_density(len(nodes), len(edges))
            },
            "metadata": {
                "source_document": Path(file_path).name,
                "domain": domain,
                "has_structure_info": structure_info is not None,
                "version": "1.0"
            }
        }

        return result

    def _calculate_graph_density(self, node_count: int, edge_count: int) -> float:
        """ê·¸ë˜í”„ ë°€ë„ ê³„ì‚° (0~1 ë²”ìœ„)"""
        if node_count <= 1:
            return 0.0
        max_possible_edges = node_count * (node_count - 1)  # ë°©í–¥ ê·¸ë˜í”„ ê¸°ì¤€
        return round(edge_count / max_possible_edges, 4) if max_possible_edges > 0 else 0.0

    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ê²°ê³¼ ìƒì„±"""
        return {
            "success": False,
            "error": error_message,
            "graph": {
                "nodes": [],
                "edges": []
            },
            "stats": {
                "entity_count": 0,
                "relationship_count": 0,
                "entity_types": {},
                "relationship_types": {},
                "density": 0.0
            }
        }

    def save_knowledge_graph(
        self,
        kg_result: Dict[str, Any],
        output_dir: Path,
        format: str = "json",
        target_db: str = "memgraph"
    ) -> Dict[str, str]:
        """
        Knowledge Graphë¥¼ íŒŒì¼ë¡œ ì €ì¥

        Args:
            kg_result: Knowledge Graph ê²°ê³¼
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            format: ì €ì¥ í˜•ì‹ (json, cypher, graphml, all)
            target_db: Cypher ëŒ€ìƒ DB (memgraph, neo4j)

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        saved_files = {}

        try:
            if format == "json" or format == "all":
                # JSON í˜•ì‹ ì €ì¥
                json_path = output_dir / "knowledge_graph.json"
                with json_path.open('w', encoding='utf-8') as f:
                    json.dump(kg_result, f, ensure_ascii=False, indent=2)
                saved_files["json"] = str(json_path)
                self.logger.info(f"ğŸ“ Knowledge Graph JSON ì €ì¥: {json_path}")

            if format == "cypher" or format == "all":
                # Cypher ì¿¼ë¦¬ í˜•ì‹ ì €ì¥
                cypher_path = output_dir / "knowledge_graph.cypher"
                cypher_content = self._generate_cypher_queries(kg_result, target_db=target_db)
                with cypher_path.open('w', encoding='utf-8') as f:
                    f.write(cypher_content)
                saved_files["cypher"] = str(cypher_path)
                self.logger.info(f"ğŸ“ Knowledge Graph Cypher ì €ì¥ (ëŒ€ìƒ: {target_db}): {cypher_path}")

            if format == "graphml" or format == "all":
                # GraphML í˜•ì‹ ì €ì¥
                graphml_path = output_dir / "knowledge_graph.graphml"
                graphml_content = self._generate_graphml(kg_result)
                with graphml_path.open('w', encoding='utf-8') as f:
                    f.write(graphml_content)
                saved_files["graphml"] = str(graphml_path)
                self.logger.info(f"ğŸ“ Knowledge Graph GraphML ì €ì¥: {graphml_path}")

            return saved_files

        except Exception as e:
            self.logger.error(f"âŒ Knowledge Graph ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)
            return saved_files

    def _generate_cypher_queries(self, kg_result: Dict[str, Any], target_db: str = "memgraph") -> str:
        """Cypher CREATE ì¿¼ë¦¬ ìƒì„± (Neo4j/Memgraph í˜¸í™˜)

        Args:
            kg_result: Knowledge Graph ê²°ê³¼
            target_db: ëŒ€ìƒ DB (memgraph, neo4j)

        Returns:
            Cypher ì¿¼ë¦¬ ë¬¸ìì—´
        """
        queries = []
        queries.append("// Knowledge Graph Cypher Queries")
        queries.append(f"// Target Database: {target_db.upper()}")
        queries.append(f"// Generated: {datetime.now().isoformat()}")
        queries.append(f"// Total Nodes: {len(kg_result.get('graph', {}).get('nodes', []))}")
        queries.append(f"// Total Edges: {len(kg_result.get('graph', {}).get('edges', []))}\n")

        # ë…¸ë“œ ìƒì„± ì¿¼ë¦¬
        queries.append("// Create Nodes")
        for node in kg_result.get("graph", {}).get("nodes", []):
            node_id = node.get("id", "")
            node_type = node.get("type", "Node")
            properties = node.get("properties", {})

            # í”„ë¡œí¼í‹° ë¬¸ìì—´ ìƒì„±
            props_str = ", ".join([
                f"{k}: {self._cypher_value(v)}"
                for k, v in properties.items()
            ])

            query = f"CREATE (n:{node_type} {{id: '{node_id}', {props_str}}});"
            queries.append(query)

        queries.append("\n// Create Relationships")
        for edge in kg_result.get("graph", {}).get("edges", []):
            source = edge.get("source", "")
            target = edge.get("target", "")
            edge_type = edge.get("type", "RELATED_TO")
            properties = edge.get("properties", {})

            # í”„ë¡œí¼í‹° ë¬¸ìì—´ ìƒì„±
            props_str = ", ".join([
                f"{k}: {self._cypher_value(v)}"
                for k, v in properties.items()
            ])

            query = (
                f"MATCH (a {{id: '{source}'}}), (b {{id: '{target}'}}) "
                f"CREATE (a)-[r:{edge_type} {{{props_str}}}]->(b);"
            )
            queries.append(query)

        return "\n".join(queries)

    def _cypher_value(self, value: Any) -> str:
        """Python ê°’ì„ Cypher ê°’ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        if isinstance(value, str):
            # ë¬¸ìì—´ ì´ìŠ¤ì¼€ì´í”„
            escaped = value.replace("'", "\\'").replace('"', '\\"')
            return f"'{escaped}'"
        elif isinstance(value, bool):
            return "true" if value else "false"
        elif isinstance(value, (int, float)):
            return str(value)
        elif isinstance(value, list):
            items = [self._cypher_value(v) for v in value]
            return f"[{', '.join(items)}]"
        elif value is None:
            return "null"
        else:
            return f"'{str(value)}'"

    def _generate_graphml(self, kg_result: Dict[str, Any]) -> str:
        """GraphML XML í˜•ì‹ ìƒì„±"""
        lines = []
        lines.append('<?xml version="1.0" encoding="UTF-8"?>')
        lines.append('<graphml xmlns="http://graphml.graphdrawing.org/xmlns">')
        lines.append('  <graph id="KnowledgeGraph" edgedefault="directed">')

        # ë…¸ë“œ ì¶”ê°€
        for node in kg_result.get("graph", {}).get("nodes", []):
            node_id = node.get("id", "")
            node_type = node.get("type", "Node")
            lines.append(f'    <node id="{node_id}">')
            lines.append(f'      <data key="type">{node_type}</data>')

            # í”„ë¡œí¼í‹° ì¶”ê°€
            for key, value in node.get("properties", {}).items():
                lines.append(f'      <data key="{key}">{self._xml_escape(str(value))}</data>')

            lines.append('    </node>')

        # ì—£ì§€ ì¶”ê°€
        for idx, edge in enumerate(kg_result.get("graph", {}).get("edges", [])):
            edge_id = edge.get("id", f"e{idx}")
            source = edge.get("source", "")
            target = edge.get("target", "")
            edge_type = edge.get("type", "RELATED_TO")

            lines.append(f'    <edge id="{edge_id}" source="{source}" target="{target}">')
            lines.append(f'      <data key="type">{edge_type}</data>')

            # í”„ë¡œí¼í‹° ì¶”ê°€
            for key, value in edge.get("properties", {}).items():
                lines.append(f'      <data key="{key}">{self._xml_escape(str(value))}</data>')

            lines.append('    </edge>')

        lines.append('  </graph>')
        lines.append('</graphml>')

        return "\n".join(lines)

    def _xml_escape(self, text: str) -> str:
        """XML íŠ¹ìˆ˜ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„"""
        return (
            text.replace("&", "&amp;")
            .replace("<", "&lt;")
            .replace(">", "&gt;")
            .replace('"', "&quot;")
            .replace("'", "&apos;")
        )

    # ========== ì²­í‚¹ ê¸°ë°˜ Full KG ì¶”ì¶œ (ì‹ ê·œ) ==========

    def build_full_knowledge_graph_with_chunking(
        self,
        text: str,
        file_path: str,
        domain: str = "general",
        structure_info: Optional[Dict[str, Any]] = None,
        llm_config: Optional[Dict[str, Any]] = None,
        max_chunk_tokens: int = 16000,  # ê¸°ë³¸ê°’ ì¦ê°€: ì²­í¬ ìˆ˜ ê°ì†Œ
        output_dir: Optional[Path] = None,
        extraction_level: str = "standard",
        fail_fast: bool = False,
        force_restart: bool = False  # ë³€ê²½: resume â†’ force_restart (ê¸°ë³¸ê°’ì€ ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ)
    ) -> Dict[str, Any]:
        """
        êµ¬ì¡° ê¸°ë°˜ ì²­í‚¹ì„ ì‚¬ìš©í•œ ì™„ì „í•œ Knowledge Graph ìƒì„±

        Args:
            text: ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸
            extraction_level: ì¶”ì¶œ ìˆ˜ì¤€ ("brief", "standard", "deep")
            file_path: íŒŒì¼ ê²½ë¡œ
            domain: ë¬¸ì„œ ë„ë©”ì¸
            structure_info: ë¬¸ì„œ êµ¬ì¡° ì •ë³´
            llm_config: LLM ì„¤ì •
            max_chunk_tokens: ì²­í¬ë‹¹ ìµœëŒ€ í† í° ìˆ˜
            fail_fast: ì²­í¬ ì˜¤ë¥˜ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨ (True: ê°œë°œ/í…ŒìŠ¤íŠ¸ ëª¨ë“œ, False: ìš´ìš© ëª¨ë“œ)
            force_restart: ì²´í¬í¬ì¸íŠ¸ ë¬´ì‹œí•˜ê³  ì²˜ìŒë¶€í„° ì‹œì‘ (True: ë¬´ì‹œ, False: ì¬ê°œ - ê¸°ë³¸ê°’)

        Returns:
            ë³‘í•©ëœ ì™„ì „í•œ Knowledge Graph
        """
        try:
            total_start = time.time()

            # íŒŒì¼ëª…ì—ì„œ ë¬¸ì„œ ì œëª© ì¶”ì¶œ
            document_title = Path(file_path).stem  # í™•ì¥ì ì œì™¸í•œ íŒŒì¼ëª…
            self.logger.info(f"ğŸ” ì²­í‚¹ ê¸°ë°˜ Full KG ìƒì„± ì‹œì‘: {document_title}")

            # 1. ë¬¸ì„œ ì²­í‚¹
            chunking_start = time.time()
            from .document_chunker import StructuralChunker
            chunker = StructuralChunker()

            # ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ë° ì²­í‚¹
            document_tree = chunker.analyzer.analyze_structure(text)
            chunker_level = chunker.determine_chunking_level(len(text), document_tree)
            chunks = chunker.create_chunks(
                document_tree,
                chunk_level=chunker_level,
                max_chunk_tokens=max_chunk_tokens  # API íŒŒë¼ë¯¸í„° ì „ë‹¬
            )

            chunking_duration = time.time() - chunking_start
            self.logger.info(f"ğŸ“„ ë¬¸ì„œë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ (ì†Œìš”ì‹œê°„: {chunking_duration:.2f}ì´ˆ)")

            # 2. ê° ì²­í¬ì—ì„œ KG ì¶”ì¶œ
            chunk_graphs = []
            total_tokens_used = {"input": 0, "output": 0, "total": 0}  # ì „ì²´ í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 

            # ì²­í¬ ë””ë²„ê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
            if output_dir:
                chunk_debug_dir = Path(output_dir) / "chunk_kg_debug"
                chunk_debug_dir.mkdir(parents=True, exist_ok=True)
            else:
                chunk_debug_dir = None

            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
            checkpoint_file = None
            start_idx = 0

            if chunk_debug_dir:
                checkpoint_file = chunk_debug_dir / "checkpoint.json"

                # force_restart=Falseì´ê³  ì²´í¬í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ ì¬ê°œ (ê¸°ë³¸ ë™ì‘)
                if not force_restart and checkpoint_file.exists():
                    try:
                        import json
                        with open(checkpoint_file, 'r', encoding='utf-8') as f:
                            checkpoint = json.load(f)

                        # ì´ì „ ì²­í¬ ê²°ê³¼ ë¡œë“œ
                        chunk_graphs = checkpoint.get('chunk_graphs', [])
                        start_idx = checkpoint.get('last_completed_idx', 0) + 1

                        self.logger.info(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: {start_idx}/{len(chunks)} ì²­í¬ë¶€í„° ì‹œì‘ ({len(chunk_graphs)}ê°œ ì²­í¬ ì´ë¯¸ ì™„ë£Œ)")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}, ì²˜ìŒë¶€í„° ì‹œì‘")
                        start_idx = 0
                        chunk_graphs = []
                elif force_restart and checkpoint_file.exists():
                    # force_restart=Trueì¸ ê²½ìš° ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
                    try:
                        checkpoint_file.unlink()
                        self.logger.info(f"ğŸ—‘ï¸ force_restart=True: ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ, ì²˜ìŒë¶€í„° ì‹œì‘")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì‹¤íŒ¨: {e}")

            for idx, chunk_group in enumerate(chunks):
                chunk_id = f"chunk_{idx+1:03d}"

                # ì´ë¯¸ ì²˜ë¦¬ëœ ì²­í¬ëŠ” ê±´ë„ˆë›°ê¸°
                if idx < start_idx:
                    self.logger.info(f"â© ì²­í¬ {idx+1}/{len(chunks)} ê±´ë„ˆë›°ê¸° (ì´ë¯¸ ì™„ë£Œ)")
                    continue

                chunk_text = chunk_group.get_total_content()
                parent_context = chunk_group.parent_context or "ë¬¸ì„œ ë£¨íŠ¸"

                self.logger.info(f"ğŸ” ì²­í¬ {idx+1}/{len(chunks)} KG ì¶”ì¶œ ì¤‘... ({len(chunk_text):,}ì)")

                try:
                    # 2-Phase ì¶”ì¶œ ì‚¬ìš© (ì—”í‹°í‹° ë¨¼ì €, ê´€ê³„ ë‚˜ì¤‘)
                    kg_data = self._extract_kg_from_chunk_2phase(
                        chunk_text=chunk_text,
                        chunk_id=chunk_id,
                        parent_context=parent_context,
                        structure_info=structure_info,
                        llm_config=llm_config or {},
                        debug_dir=chunk_debug_dir,
                        extraction_level=extraction_level,
                        document_title=document_title
                    )

                    # í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 
                    chunk_tokens = kg_data.get("tokens", {})
                    total_tokens_used["input"] += chunk_tokens.get("input", 0)
                    total_tokens_used["output"] += chunk_tokens.get("output", 0)
                    total_tokens_used["total"] += chunk_tokens.get("total", 0)

                    # kg_dataê°€ Noneì´ë©´ ì´ë¯¸ ì˜ˆì™¸ê°€ ë°œìƒí–ˆì„ ê²ƒì´ë¯€ë¡œ ì—¬ê¸°ê¹Œì§€ ì˜¤ì§€ ì•ŠìŒ
                    chunk_graphs.append({
                        "chunk_id": chunk_id,
                        "graph": kg_data,
                        "level": chunk_group.level,
                        "nodes_in_chunk": chunk_group.nodes
                    })

                    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì„±ê³µí•œ ê²½ìš°ì—ë§Œ)
                    if checkpoint_file:
                        self._save_checkpoint(checkpoint_file, chunk_graphs, idx)

                except Exception as e:
                    if fail_fast:
                        # ê°œë°œ/í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í›„ ì¦‰ì‹œ ì¤‘ë‹¨
                        if checkpoint_file:
                            self._save_checkpoint(checkpoint_file, chunk_graphs, idx - 1)
                        self.logger.error(f"âŒ {chunk_id} KG ì¶”ì¶œ ì‹¤íŒ¨ (fail_fast=True, ì¦‰ì‹œ ì¤‘ë‹¨)")
                        self.logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ë¨: ê°™ì€ ì˜µì…˜ìœ¼ë¡œ ì¬ì‹¤í–‰í•˜ë©´ ìë™ ì¬ê°œë¨ (force_restart=trueë¡œ ì²˜ìŒë¶€í„° ì‹œì‘ ê°€ëŠ¥)")
                        raise
                    else:
                        # ìš´ìš© ëª¨ë“œ: ê±´ë„ˆë›°ê³  ê³„ì† (ê¸°ë³¸ê°’)
                        self.logger.error(f"âš ï¸ {chunk_id} KG ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        self.logger.warning(f"â­ï¸ {chunk_id} ê±´ë„ˆë›°ê³  ë‹¤ìŒ ì²­í¬ ì²˜ë¦¬ ê³„ì†... (fail_fast=False)")
                        # ì‹¤íŒ¨í•œ ì²­í¬ëŠ” ë¹ˆ ê·¸ë˜í”„ë¡œ ì¶”ê°€
                        chunk_graphs.append({
                            "chunk_id": chunk_id,
                            "graph": {"nodes": [], "edges": []},
                            "level": chunk_group.level,
                            "nodes_in_chunk": chunk_group.nodes,
                            "error": str(e)
                        })

                        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„)
                        if checkpoint_file:
                            self._save_checkpoint(checkpoint_file, chunk_graphs, idx)

            # 3. ì„±ê³µí•œ ì²­í¬ í™•ì¸ (ìš´ìš© ëª¨ë“œì—ì„œë§Œ)
            if not fail_fast:
                successful_chunks = [cg for cg in chunk_graphs if not cg.get("error")]
                failed_chunks = [cg for cg in chunk_graphs if cg.get("error")]

                if failed_chunks:
                    self.logger.warning(f"âš ï¸ {len(failed_chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨")
                    for fc in failed_chunks:
                        self.logger.warning(f"  - {fc['chunk_id']}: {fc.get('error', 'Unknown error')}")

                if not successful_chunks:
                    raise ValueError(f"ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨ ({len(chunks)}ê°œ ì²­í¬ ì¤‘ 0ê°œ ì„±ê³µ)")

                self.logger.info(f"âœ… {len(successful_chunks)}/{len(chunks)} ì²­í¬ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë¨")
            else:
                # ê°œë°œ/í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” ëª¨ë“  ì²­í¬ê°€ ì„±ê³µí–ˆë‹¤ê³  ê°€ì • (ì‹¤íŒ¨ ì‹œ ìœ„ì—ì„œ ì´ë¯¸ raise)
                successful_chunks = chunk_graphs
                failed_chunks = []

            # 4. ëª¨ë“  ì²­í¬ ì™„ë£Œ ì‹œ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
            if checkpoint_file and checkpoint_file.exists():
                try:
                    checkpoint_file.unlink()
                    self.logger.info(f"ğŸ—‘ï¸ ëª¨ë“  ì²­í¬ ì™„ë£Œ: ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì‹¤íŒ¨: {e}")

            # 5. ì²­í¬ë³„ KG ë³‘í•©
            merge_start = time.time()
            self.logger.info(f"ğŸ”— {len(chunk_graphs)}ê°œ ì²­í¬ KG ë³‘í•© ì¤‘...")
            merged_kg = self._merge_chunk_graphs(chunk_graphs)
            merge_duration = time.time() - merge_start
            self.logger.info(f"âœ… ì²­í¬ ë³‘í•© ì™„ë£Œ (ì†Œìš”ì‹œê°„: {merge_duration:.2f}ì´ˆ)")

            # 6. ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result = self._enrich_kg_with_metadata(
                merged_kg,
                file_path,
                domain,
                structure_info
            )

            # ì²­í‚¹ ì •ë³´ ë° ì‹œê°„ ì •ë³´ ì¶”ê°€
            total_duration = time.time() - total_start
            result["chunking_stats"] = {
                "total_chunks": len(chunks),
                "successful_extractions": len(successful_chunks),
                "failed_extractions": len(failed_chunks),
                "max_chunk_tokens": max_chunk_tokens
            }

            result["performance_metrics"] = {
                "total_duration": total_duration,
                "chunking_duration": chunking_duration,
                "extraction_duration": total_duration - chunking_duration - merge_duration,
                "merge_duration": merge_duration,
                "average_chunk_duration": (total_duration - chunking_duration - merge_duration) / len(chunks) if chunks else 0,
                "total_tokens_used": total_tokens_used
            }

            self.logger.info(
                f"âœ… Full KG ìƒì„± ì™„ë£Œ: "
                f"{result['stats']['entity_count']}ê°œ ì—”í‹°í‹°, "
                f"{result['stats']['relationship_count']}ê°œ ê´€ê³„ "
                f"(ì´ ì†Œìš”ì‹œê°„: {total_duration:.2f}ì´ˆ, "
                f"ì²­í¬ë‹¹ í‰ê· : {result['performance_metrics']['average_chunk_duration']:.2f}ì´ˆ, "
                f"ì´ í† í°: ì…ë ¥ {total_tokens_used['input']:,} + ì¶œë ¥ {total_tokens_used['output']:,} = {total_tokens_used['total']:,})"
            )

            return result

        except Exception as e:
            self.logger.error(f"âŒ Full KG ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            return self._create_error_result(str(e))

    def _extract_kg_from_chunk_2phase(
        self,
        chunk_text: str,
        chunk_id: str,
        parent_context: str,
        structure_info: Optional[Dict[str, Any]],
        llm_config: Dict[str, Any],
        debug_dir: Optional[Path] = None,
        extraction_level: str = "standard",
        document_title: str = "Untitled Document"
    ) -> Optional[Dict[str, Any]]:
        """2-Phase ì¶”ì¶œ: 1ë‹¨ê³„ ì—”í‹°í‹°, 2ë‹¨ê³„ ê´€ê³„

        Args:
            extraction_level: ì¶”ì¶œ ìˆ˜ì¤€ ("brief", "standard", "deep")
            document_title: ë¬¸ì„œ ì œëª© (íŒŒì¼ëª… ë˜ëŠ” íƒ€ì´í‹€)
        """
        try:
            from prompts.templates import KnowledgeGraphPrompts
            import json

            chunk_total_start = time.time()

            # === Phase 1: ì—”í‹°í‹°ë§Œ ì¶”ì¶œ ===
            phase1_start = time.time()

            # ì¶”ì¶œ ë ˆë²¨ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ì„ íƒ
            level_prompts = {
                "brief": KnowledgeGraphPrompts.PHASE1_ENTITY_BRIEF,
                "standard": KnowledgeGraphPrompts.PHASE1_ENTITY_STANDARD,
                "deep": KnowledgeGraphPrompts.PHASE1_ENTITY_DEEP
            }

            entity_template = level_prompts.get(extraction_level.lower(), level_prompts["standard"])

            self.logger.info(f"ğŸ” {chunk_id} Phase 1: ì—”í‹°í‹° ì¶”ì¶œ ì‹œì‘... (ë ˆë²¨: {extraction_level}, ë¬¸ì„œ: {document_title})")

            entity_prompt = entity_template.format(text=chunk_text, document_title=document_title)

            # ë””ë²„ê·¸: Phase 1 í”„ë¡¬í”„íŠ¸ ì €ì¥
            if debug_dir:
                (debug_dir / f"{chunk_id}_phase1_prompt.txt").write_text(entity_prompt, encoding='utf-8')

            # Phase 1 LLM í˜¸ì¶œ
            llm_call_start = time.time()
            phase1_response = self._call_llm_for_kg(entity_prompt, llm_config)
            llm_call_duration = time.time() - llm_call_start

            if not phase1_response.get("success"):
                error_msg = f"{chunk_id} Phase 1 LLM í˜¸ì¶œ ì‹¤íŒ¨: {phase1_response.get('error')}"
                self.logger.error(f"âŒ {error_msg}")
                if debug_dir:
                    (debug_dir / f"{chunk_id}_phase1_error.txt").write_text(phase1_response.get('error', ''), encoding='utf-8')
                raise ValueError(error_msg)

            phase1_raw = phase1_response.get("response", "")
            phase1_tokens = phase1_response.get("tokens", {})

            # ë””ë²„ê·¸: Phase 1 ì‘ë‹µ ì €ì¥
            if debug_dir:
                (debug_dir / f"{chunk_id}_phase1_response.txt").write_text(phase1_raw, encoding='utf-8')

            # Phase 1 íŒŒì‹±
            parse_start = time.time()
            entities_data = self._parse_kg_response(phase1_raw)
            entities = entities_data.get('entities', entities_data.get('nodes', []))
            parse_duration = time.time() - parse_start

            if not entities:
                error_msg = f"{chunk_id} Phase 1 ì‹¤íŒ¨: ì—”í‹°í‹°ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
                self.logger.error(f"âŒ {error_msg}")
                if debug_dir:
                    (debug_dir / f"{chunk_id}_phase1_parse_error.txt").write_text(
                        f"{error_msg}\n\nResponse: {phase1_raw[:1000]}", encoding='utf-8'
                    )
                raise ValueError(error_msg)

            phase1_duration = time.time() - phase1_start
            self.logger.info(
                f"âœ… {chunk_id} Phase 1 ì™„ë£Œ: {len(entities)}ê°œ ì—”í‹°í‹° ì¶”ì¶œ "
                f"(LLM: {llm_call_duration:.2f}ì´ˆ, íŒŒì‹±: {parse_duration:.2f}ì´ˆ, ì „ì²´: {phase1_duration:.2f}ì´ˆ, "
                f"í† í°: ì…ë ¥ {phase1_tokens.get('input', 0):,} + ì¶œë ¥ {phase1_tokens.get('output', 0):,} = ì´ {phase1_tokens.get('total', 0):,})"
            )

            # === Phase 2: ê´€ê³„ë§Œ ì¶”ì¶œ ===
            phase2_start = time.time()
            self.logger.info(f"ğŸ”— {chunk_id} Phase 2: ê´€ê³„ ì¶”ì¶œ ì‹œì‘...")

            # ì—”í‹°í‹° ëª©ë¡ì„ JSONìœ¼ë¡œ ë³€í™˜ (ê°„ê²°í•˜ê²Œ)
            entities_json = json.dumps([
                {"id": e.get("id"), "type": e.get("type"), "name": e.get("properties", {}).get("name", "Unknown")}
                for e in entities
            ], ensure_ascii=False, indent=2)

            relation_prompt = KnowledgeGraphPrompts.PHASE2_RELATION_ONLY.format(
                entities_json=entities_json,
                text=chunk_text[:5000]  # í…ìŠ¤íŠ¸ëŠ” ì•ë¶€ë¶„ë§Œ (í† í° ì ˆì•½)
            )

            # ë””ë²„ê·¸: Phase 2 í”„ë¡¬í”„íŠ¸ ì €ì¥
            if debug_dir:
                (debug_dir / f"{chunk_id}_phase2_prompt.txt").write_text(relation_prompt, encoding='utf-8')

            # Phase 2 LLM í˜¸ì¶œ
            llm_call_start = time.time()
            phase2_response = self._call_llm_for_kg(relation_prompt, llm_config)
            llm_call_duration = time.time() - llm_call_start

            if not phase2_response.get("success"):
                error_msg = f"{chunk_id} Phase 2 LLM í˜¸ì¶œ ì‹¤íŒ¨: {phase2_response.get('error')}"
                self.logger.error(f"âŒ {error_msg}")
                if debug_dir:
                    (debug_dir / f"{chunk_id}_phase2_error.txt").write_text(phase2_response.get('error', ''), encoding='utf-8')
                raise ValueError(error_msg)

            phase2_raw = phase2_response.get("response", "")
            phase2_tokens = phase2_response.get("tokens", {})

            # ë””ë²„ê·¸: Phase 2 ì‘ë‹µ ì €ì¥
            if debug_dir:
                (debug_dir / f"{chunk_id}_phase2_response.txt").write_text(phase2_raw, encoding='utf-8')

            # Phase 2 íŒŒì‹±
            parse_start = time.time()
            relations_data = self._parse_kg_response(phase2_raw)
            relationships = relations_data.get('relationships', relations_data.get('edges', []))
            parse_duration = time.time() - parse_start

            phase2_duration = time.time() - phase2_start
            self.logger.info(
                f"âœ… {chunk_id} Phase 2 ì™„ë£Œ: {len(relationships)}ê°œ ê´€ê³„ ì¶”ì¶œ "
                f"(LLM: {llm_call_duration:.2f}ì´ˆ, íŒŒì‹±: {parse_duration:.2f}ì´ˆ, ì „ì²´: {phase2_duration:.2f}ì´ˆ, "
                f"í† í°: ì…ë ¥ {phase2_tokens.get('input', 0):,} + ì¶œë ¥ {phase2_tokens.get('output', 0):,} = ì´ {phase2_tokens.get('total', 0):,})"
            )

            # === ê²°ê³¼ ë³‘í•© ===
            kg_data = {
                "nodes": entities,
                "edges": relationships
            }

            # ë””ë²„ê·¸: ìµœì¢… KG ì €ì¥
            if debug_dir:
                (debug_dir / f"{chunk_id}_kg_2phase.json").write_text(
                    json.dumps(kg_data, ensure_ascii=False, indent=2),
                    encoding='utf-8'
                )

            chunk_total_duration = time.time() - chunk_total_start

            # ì „ì²´ í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
            total_input_tokens = phase1_tokens.get('input', 0) + phase2_tokens.get('input', 0)
            total_output_tokens = phase1_tokens.get('output', 0) + phase2_tokens.get('output', 0)
            total_tokens = phase1_tokens.get('total', 0) + phase2_tokens.get('total', 0)

            self.logger.info(
                f"âœ… {chunk_id} 2-Phase ì¶”ì¶œ ì™„ë£Œ: "
                f"{len(entities)}ê°œ ì—”í‹°í‹°, {len(relationships)}ê°œ ê´€ê³„ "
                f"(ì „ì²´ ì†Œìš”ì‹œê°„: {chunk_total_duration:.2f}ì´ˆ, "
                f"ì´ í† í°: ì…ë ¥ {total_input_tokens:,} + ì¶œë ¥ {total_output_tokens:,} = {total_tokens:,})"
            )

            # í† í° ì •ë³´ë¥¼ kg_dataì— ì¶”ê°€
            kg_data["tokens"] = {
                "input": total_input_tokens,
                "output": total_output_tokens,
                "total": total_tokens
            }

            return kg_data

        except Exception as e:
            self.logger.error(f"âŒ {chunk_id} 2-Phase KG ì¶”ì¶œ ì‹¤íŒ¨: {e}", exc_info=True)
            if debug_dir:
                (debug_dir / f"{chunk_id}_2phase_exception.txt").write_text(str(e), encoding='utf-8')
            raise

    def _extract_kg_from_chunk(
        self,
        chunk_text: str,
        chunk_id: str,
        parent_context: str,
        structure_info: Optional[Dict[str, Any]],
        llm_config: Dict[str, Any],
        debug_dir: Optional[Path] = None
    ) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ì²­í¬ì—ì„œ KG ì¶”ì¶œ"""
        try:
            from prompts.templates import KnowledgeGraphPrompts

            # êµ¬ì¡° ì •ë³´ ìš”ì•½
            structure_summary = self._summarize_structure(structure_info) if structure_info else "êµ¬ì¡° ì •ë³´ ì—†ìŒ"

            # ìƒì„¸ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = KnowledgeGraphPrompts.DETAILED_KG_EXTRACTION.format(
                text=chunk_text,
                chunk_id=chunk_id,
                structure_info=structure_summary,
                parent_context=parent_context
            )

            # ë””ë²„ê·¸: ì²­í¬ í…ìŠ¤íŠ¸ ì €ì¥
            if debug_dir:
                chunk_text_file = debug_dir / f"{chunk_id}_text.txt"
                chunk_text_file.write_text(chunk_text, encoding='utf-8')

            # ë””ë²„ê·¸: í”„ë¡¬í”„íŠ¸ ì €ì¥
            if debug_dir:
                prompt_file = debug_dir / f"{chunk_id}_prompt.txt"
                prompt_file.write_text(prompt, encoding='utf-8')
                self.logger.debug(f"ğŸ“ {chunk_id} í”„ë¡¬í”„íŠ¸ ì €ì¥: {prompt_file}")

            # LLM í˜¸ì¶œ
            llm_response = self._call_llm_for_kg(prompt, llm_config)

            if not llm_response.get("success"):
                error_msg = f"{chunk_id} LLM í˜¸ì¶œ ì‹¤íŒ¨: {llm_response.get('error')}"
                self.logger.error(f"âŒ {error_msg}")
                # ë””ë²„ê·¸: ì˜¤ë¥˜ ì €ì¥
                if debug_dir:
                    error_file = debug_dir / f"{chunk_id}_error.txt"
                    error_file.write_text(llm_response.get('error', 'Unknown error'), encoding='utf-8')
                # ì¹˜ëª…ì  ì˜¤ë¥˜ë¡œ ì˜ˆì™¸ ë°œìƒì‹œì¼œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ë‹¨
                raise ValueError(error_msg)

            # ì‘ë‹µ íŒŒì‹±
            raw_response = llm_response.get("response", "")

            # ë””ë²„ê·¸: LLM ì‘ë‹µ ì €ì¥
            if debug_dir:
                response_file = debug_dir / f"{chunk_id}_response.txt"
                response_file.write_text(raw_response, encoding='utf-8')
                self.logger.debug(f"ğŸ“ {chunk_id} ì‘ë‹µ ì €ì¥: {response_file}")

            kg_data = self._parse_kg_response(raw_response)

            # í•µì‹¬ ê²€ì¦: íŒŒì‹± ê²°ê³¼ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¹˜ëª…ì  ì˜¤ë¥˜ë¡œ ì²˜ë¦¬
            if not kg_data.get('nodes') and not kg_data.get('edges'):
                error_msg = f"{chunk_id} KG ì¶”ì¶œ ì‹¤íŒ¨: LLM ì‘ë‹µ íŒŒì‹± ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. JSON í˜•ì‹ ì˜¤ë¥˜ ë˜ëŠ” max_tokens ì´ˆê³¼ ê°€ëŠ¥ì„±"
                self.logger.error(f"âŒ {error_msg}")

                # ë””ë²„ê·¸: íŒŒì‹± ì‹¤íŒ¨ ìƒì„¸ ì •ë³´ ì €ì¥
                if debug_dir:
                    error_detail_file = debug_dir / f"{chunk_id}_parse_error.txt"
                    error_detail_file.write_text(
                        f"Error: {error_msg}\n\n"
                        f"Response length: {len(raw_response)}\n"
                        f"Response preview (last 500 chars):\n{raw_response[-500:]}\n\n"
                        f"Parsed result: {json.dumps(kg_data, ensure_ascii=False, indent=2)}",
                        encoding='utf-8'
                    )

                # ì¹˜ëª…ì  ì˜¤ë¥˜ë¡œ None ë°˜í™˜í•˜ì—¬ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ë‹¨
                raise ValueError(error_msg)

            # ë””ë²„ê·¸: íŒŒì‹±ëœ KG ë°ì´í„° ì €ì¥
            if debug_dir:
                kg_file = debug_dir / f"{chunk_id}_kg.json"
                kg_file.write_text(
                    json.dumps(kg_data, ensure_ascii=False, indent=2),
                    encoding='utf-8'
                )
                self.logger.debug(f"ğŸ“ {chunk_id} KG ì €ì¥: {kg_file}")

            self.logger.info(
                f"âœ… {chunk_id} ì¶”ì¶œ ì™„ë£Œ: "
                f"{len(kg_data.get('nodes', []))}ê°œ ì—”í‹°í‹°, "
                f"{len(kg_data.get('edges', []))}ê°œ ê´€ê³„"
            )

            return kg_data

        except Exception as e:
            self.logger.error(f"âŒ {chunk_id} KG ì¶”ì¶œ ì‹¤íŒ¨: {e}", exc_info=True)
            # ë””ë²„ê·¸: ì˜ˆì™¸ ì €ì¥
            if debug_dir:
                exception_file = debug_dir / f"{chunk_id}_exception.txt"
                exception_file.write_text(str(e), encoding='utf-8')
            # ì¹˜ëª…ì  ì˜¤ë¥˜ì´ë¯€ë¡œ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ë‹¨
            raise

    def _merge_chunk_graphs(self, chunk_graphs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ì²­í¬ì˜ KGë¥¼ í•˜ë‚˜ë¡œ ë³‘í•© (UUID ì‚¬ìš©ìœ¼ë¡œ ì „ì—­ ê³ ìœ ì„± ë³´ì¥)"""
        merged_nodes = []
        merged_edges = []
        node_id_map = {}  # ì›ë³¸ ID â†’ UUID ë§¤í•‘
        node_dedup_map = {}  # (type, name) â†’ UUID ë§¤í•‘ (ì¤‘ë³µ ì œê±°ìš©)

        for chunk_data in chunk_graphs:
            chunk_id = chunk_data["chunk_id"]
            graph = chunk_data["graph"]

            # ë…¸ë“œ ë³‘í•© (UUID ì‚¬ìš©ìœ¼ë¡œ ì „ì—­ ê³ ìœ ì„± ë³´ì¥)
            for node in graph.get("nodes", []):
                original_id = node["id"]

                # ë™ì¼í•œ ì—”í‹°í‹° ì¤‘ë³µ ì²´í¬ (ì´ë¦„ê³¼ íƒ€ì…ì´ ê°™ìœ¼ë©´ ë³‘í•©)
                node_key = (node.get("type"), node.get("properties", {}).get("name"))

                if node_key in node_dedup_map:
                    # ê¸°ì¡´ ë…¸ë“œ UUID ì¬ì‚¬ìš© (ì¤‘ë³µ ì œê±°)
                    uuid_id = node_dedup_map[node_key]
                    node_id_map[original_id] = uuid_id
                else:
                    # ìƒˆ UUID ìƒì„±
                    uuid_id = str(uuid.uuid4())
                    node["id"] = uuid_id
                    merged_nodes.append(node)
                    node_id_map[original_id] = uuid_id
                    node_dedup_map[node_key] = uuid_id

            # ê´€ê³„ ë³‘í•© (UUID ì‚¬ìš©)
            for edge in graph.get("edges", []):
                # ì›ë³¸ IDë¥¼ UUIDë¡œ ë³€í™˜
                source = edge.get("source", "")
                target = edge.get("target", "")

                # chunk_id ì ‘ë‘ì‚¬ ì œê±° í›„ ë§¤í•‘
                source_base = source.split("_", 1)[-1] if "_" in source else source
                target_base = target.split("_", 1)[-1] if "_" in target else target

                new_source = node_id_map.get(source_base, node_id_map.get(source, str(uuid.uuid4())))
                new_target = node_id_map.get(target_base, node_id_map.get(target, str(uuid.uuid4())))

                edge["source"] = new_source
                edge["target"] = new_target
                edge["id"] = str(uuid.uuid4())  # ê´€ê³„ë„ UUID ì‚¬ìš©

                merged_edges.append(edge)

        self.logger.info(
            f"ğŸ”— ë³‘í•© ì™„ë£Œ: {len(merged_nodes)}ê°œ ì—”í‹°í‹° (ì¤‘ë³µ ì œê±° í›„), "
            f"{len(merged_edges)}ê°œ ê´€ê³„"
        )

        return {
            "nodes": merged_nodes,
            "edges": merged_edges
        }
