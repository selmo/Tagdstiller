"""
ë¡œì»¬ íŒŒì¼ ë¶„ì„ API ë¼ìš°í„°
"""
from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.orm import Session
from typing import List, Optional, Dict, Any
from pydantic import BaseModel
from datetime import datetime
from pathlib import Path

from dependencies import get_db
from services.local_file_analyzer import LocalFileAnalyzer
from services.document_parser_service import DocumentParserService
from services.memgraph_service import MemgraphService


# Request/Response ëª¨ë¸
class AnalyzeFileRequest(BaseModel):
    file_path: str
    extractors: Optional[List[str]] = None
    force_reanalyze: bool = False
    force_reparse: bool = False  # íŒŒì‹±ë¶€í„° ë‹¤ì‹œ ìˆ˜í–‰í• ì§€ ì—¬ë¶€
    directory: Optional[str] = None


class FileAnalysisResponse(BaseModel):
    file_info: Dict[str, Any]
    content_info: Optional[Dict[str, Any]] = None
    extraction_info: Optional[Dict[str, Any]] = None
    keywords: Optional[Dict[str, List[Dict[str, Any]]]] = None
    analysis_status: str
    analysis_timestamp: Optional[str] = None
    result_file: Optional[str] = None
    error_message: Optional[str] = None


class FileStatusResponse(BaseModel):
    file_path: str
    exists: bool
    supported: bool
    has_analysis: bool
    analysis_timestamp: Optional[str] = None
    result_file: Optional[str] = None


class ParseDocumentRequest(BaseModel):
    file_path: str
    force_reparse: bool = False
    directory: Optional[str] = None


class DocumentParsingResponse(BaseModel):
    file_info: Dict[str, Any]
    parsing_timestamp: str
    parsers_used: List[str]
    parsing_results: Dict[str, Any]
    summary: Dict[str, Any]
    output_directory: str
    saved_files: Optional[List[Dict[str, Any]]] = None


# í—¬í¼ í•¨ìˆ˜
def _collect_saved_files(output_dir: Path, parsing_results: dict) -> list:
    """ì €ì¥ëœ íŒŒì¼ë“¤ì˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    saved_files = []
    
    # íŒŒì‹± ê²°ê³¼ ì¢…í•© íŒŒì¼
    parsing_result_path = output_dir / "parsing_results.json"
    if parsing_result_path.exists():
        saved_files.append({
            "type": "parsing_summary",
            "path": str(parsing_result_path),
            "description": "íŒŒì‹± ê²°ê³¼ ì¢…í•© íŒŒì¼"
        })
    
    # Markdown íŒŒì¼ë“¤
    docling_md = output_dir / "docling.md"
    if docling_md.exists():
        saved_files.append({
            "type": "markdown",
            "parser": "docling",
            "path": str(docling_md),
            "description": "Docling íŒŒì„œë¡œ ìƒì„±ëœ Markdown íŒŒì¼"
        })
    
    pymupdf_md = output_dir / "pymupdf4llm.md"
    if pymupdf_md.exists():
        saved_files.append({
            "type": "markdown", 
            "parser": "pdf_parser",
            "path": str(pymupdf_md),
            "description": "PyMuPDF4LLMìœ¼ë¡œ ìƒì„±ëœ Markdown íŒŒì¼"
        })
    
    # í‚¤ì›Œë“œ ë¶„ì„ íŒŒì¼
    keyword_analysis = output_dir / "keyword_analysis.json"
    if keyword_analysis.exists():
        saved_files.append({
            "type": "keyword_analysis",
            "path": str(keyword_analysis),
            "description": "í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼"
        })
    
    # ê° íŒŒì„œë³„ ì €ì¥ëœ íŒŒì¼ë“¤
    for parser_name, parser_result in parsing_results.get("parsing_results", {}).items():
        if parser_result.get("success"):
            parser_dir = output_dir / parser_name
            
            # í…ìŠ¤íŠ¸ íŒŒì¼
            text_file = parser_dir / f"{parser_name}_text.txt"
            if text_file.exists():
                saved_files.append({
                    "type": "extracted_text",
                    "parser": parser_name,
                    "path": str(text_file),
                    "description": f"{parser_name} íŒŒì„œë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸"
                })
            
            # ë©”íƒ€ë°ì´í„° íŒŒì¼
            metadata_file = parser_dir / f"{parser_name}_metadata.json"
            if metadata_file.exists():
                saved_files.append({
                    "type": "metadata",
                    "parser": parser_name,
                    "path": str(metadata_file),
                    "description": f"{parser_name} íŒŒì„œ ë©”íƒ€ë°ì´í„°"
                })
            
            # êµ¬ì¡° ì •ë³´ íŒŒì¼
            structure_file = parser_dir / f"{parser_name}_structure.json"
            if structure_file.exists():
                saved_files.append({
                    "type": "parser_structure",
                    "parser": parser_name,
                    "path": str(structure_file),
                    "description": f"{parser_name} íŒŒì„œ êµ¬ì¡° ì •ë³´"
                })
    
    return saved_files


def _move_markdown_files_to_correct_location(parsing_results, file_path_obj, output_dir):
    """Markdown íŒŒì¼ë“¤ì„ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ë¡œ ì´ë™í•˜ê³  ì›ë³¸ ìœ„ì¹˜ì˜ íŒŒì¼ë“¤ì„ ì •ë¦¬"""
    import shutil
    import logging
    from pathlib import Path
    
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ” Markdown íŒŒì¼ ì´ë™ ê²€ì‚¬ ì‹œì‘: {output_dir}")
    
    # ì›ë³¸ íŒŒì¼ ë””ë ‰í† ë¦¬ (íŒŒì„œê°€ ê¸°ë³¸ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ìœ„ì¹˜)
    original_dir = file_path_obj.parent / file_path_obj.stem
    
    # ë¨¼ì € ì›ë³¸ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  Markdown íŒŒì¼ì„ ì°¾ì•„ì„œ ì´ë™
    if original_dir.exists() and original_dir != output_dir:
        logger.info(f"ğŸ“ ì›ë³¸ ë””ë ‰í† ë¦¬ ê²€ì‚¬: {original_dir}")
        
        # docling.md íŒŒì¼ ì²˜ë¦¬
        docling_md = original_dir / "docling.md"
        if docling_md.exists():
            target_md = output_dir / "docling.md"
            try:
                target_md.parent.mkdir(parents=True, exist_ok=True)
                if target_md.exists():
                    target_md.unlink()
                shutil.copy2(str(docling_md), str(target_md))
                docling_md.unlink()
                logger.info(f"âœ… docling.md ì´ë™ ì™„ë£Œ: {docling_md} â†’ {target_md}")
                
                # íŒŒì‹± ê²°ê³¼ì—ì„œ ê²½ë¡œ ì—…ë°ì´íŠ¸
                if "docling" in parsing_results.get("parsing_results", {}):
                    parsing_results["parsing_results"]["docling"]["md_file_path"] = str(target_md)
            except Exception as e:
                logger.warning(f"âš ï¸ docling.md ì´ë™ ì‹¤íŒ¨: {e}")
        
        # pymupdf4llm.md íŒŒì¼ ì²˜ë¦¬
        pymupdf_md = original_dir / "pymupdf4llm.md"
        if pymupdf_md.exists():
            target_md = output_dir / "pymupdf4llm.md"
            try:
                target_md.parent.mkdir(parents=True, exist_ok=True)
                if target_md.exists():
                    target_md.unlink()
                shutil.copy2(str(pymupdf_md), str(target_md))
                pymupdf_md.unlink()
                logger.info(f"âœ… pymupdf4llm.md ì´ë™ ì™„ë£Œ: {pymupdf_md} â†’ {target_md}")
                
                # íŒŒì‹± ê²°ê³¼ì—ì„œ ê²½ë¡œ ì—…ë°ì´íŠ¸
                if "pdf_parser" in parsing_results.get("parsing_results", {}):
                    parsing_results["parsing_results"]["pdf_parser"]["md_file_path"] = str(target_md)
            except Exception as e:
                logger.warning(f"âš ï¸ pymupdf4llm.md ì´ë™ ì‹¤íŒ¨: {e}")
        
        # ì›ë³¸ ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì‚­ì œ
        try:
            if original_dir.exists() and not any(original_dir.iterdir()):
                original_dir.rmdir()
                logger.info(f"ğŸ—‘ï¸ ë¹ˆ ì›ë³¸ ë””ë ‰í† ë¦¬ ì‚­ì œ: {original_dir}")
        except Exception as e:
            logger.warning(f"âš ï¸ ì›ë³¸ ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨: {e}")
    
    # ê¸°ì¡´ ë¡œì§ë„ ìœ ì§€ (íŒŒì‹± ê²°ê³¼ì— ìˆëŠ” md_file_path ì²˜ë¦¬)
    for parser_name, parser_result in parsing_results.get("parsing_results", {}).items():
        if not parser_result.get("success"):
            continue
            
        md_file_path = parser_result.get("md_file_path")
        
        if not md_file_path:
            continue
            
        source_md_file = Path(md_file_path)
        
        if not source_md_file.exists():
            continue
            
        # ì˜¬ë°”ë¥¸ ìœ„ì¹˜ë¡œ ì´ë™
        target_md_file = output_dir / source_md_file.name
        
        # ì´ë¯¸ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸
        if source_md_file == target_md_file:
            continue
            
        try:
            # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            target_md_file.parent.mkdir(parents=True, exist_ok=True)
            
            # íƒ€ê²Ÿ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë®ì–´ì“°ê¸°
            if target_md_file.exists():
                target_md_file.unlink()
            
            # íŒŒì¼ ë³µì‚¬ í›„ ì›ë³¸ ê°•ì œ ì‚­ì œ
            shutil.copy2(str(source_md_file), str(target_md_file))
            
            # ì›ë³¸ íŒŒì¼ ê°•ì œ ì‚­ì œ
            try:
                source_md_file.unlink()
                logger.info(f"ğŸ“ Markdown íŒŒì¼ ì´ë™ ì™„ë£Œ: {source_md_file} â†’ {target_md_file}")
            except Exception as delete_error:
                logger.warning(f"âš ï¸ ì›ë³¸ Markdown íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {delete_error}")
            
            # íŒŒì‹± ê²°ê³¼ ì—…ë°ì´íŠ¸
            parser_result["md_file_path"] = str(target_md_file)
            
        except Exception as move_error:
            logger.warning(f"âš ï¸ Markdown íŒŒì¼ ì´ë™ ì‹¤íŒ¨ ({parser_name}): {move_error}")


router = APIRouter(prefix="/local-analysis", tags=["local-analysis"])


@router.post("/parse", response_model=DocumentParsingResponse)
async def parse_document_comprehensive(
    request: ParseDocumentRequest,
    db: Session = Depends(get_db)
):
    """
    ë¬¸ì„œë¥¼ ëª¨ë“  ì ìš© ê°€ëŠ¥í•œ íŒŒì„œë¡œ ì™„ì „ íŒŒì‹±í•©ë‹ˆë‹¤.
    
    - **file_path**: íŒŒì‹±í•  ë¬¸ì„œ ê²½ë¡œ
    - **force_reparse**: ê¸°ì¡´ ê²°ê³¼ ë¬´ì‹œí•˜ê³  ì¬íŒŒì‹± ì—¬ë¶€ (ê¸°ë³¸ê°’: false)
    
    í•­ìƒ ëª¨ë“  íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœìƒì˜ íŒŒì‹± ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    êµ¬ì¡°í™”ëœ íŒŒì„œ(Docling, PyMuPDF4LLM ë“±)ëŠ” êµ¬ì¡° ì •ë³´ë„ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.
    """
    from pathlib import Path
    
    parser_service = DocumentParserService()
    
    try:
        file_path = Path(request.file_path)
        
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not file_path.exists():
            if not file_path.is_absolute():
                # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ í™•ì¸
                file_path = Path.cwd() / file_path
                
        if not file_path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {request.file_path}")
        
        # ì§€ì› íŒŒì¼ í˜•ì‹ í™•ì¸
        if not parser_service.is_supported_file(file_path):
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_path.suffix}")
        
        # ë””ë ‰í† ë¦¬ íŒŒë¼ë¯¸í„° ì²˜ë¦¬
        directory = None
        if request.directory:
            directory = Path(request.directory)
            if not directory.is_absolute():
                directory = Path.cwd() / directory
            directory.mkdir(parents=True, exist_ok=True)
        
        # ì™„ì „ íŒŒì‹± ìˆ˜í–‰
        results = parser_service.parse_document_comprehensive(
            file_path=file_path,
            force_reparse=request.force_reparse,
            directory=directory
        )
        
        # ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
        output_dir = parser_service.get_output_directory(file_path, directory)
        saved_files = []
        
        # íŒŒì‹± ê²°ê³¼ ì¢…í•© íŒŒì¼
        parsing_result_path = parser_service.get_parsing_result_path(file_path, directory)
        if parsing_result_path.exists():
            saved_files.append({
                "type": "parsing_summary",
                "path": str(parsing_result_path),
                "description": "íŒŒì‹± ê²°ê³¼ ì¢…í•© íŒŒì¼"
            })
        
        # ê° íŒŒì„œë³„ ì €ì¥ëœ íŒŒì¼ë“¤
        for parser_name, parser_result in results["parsing_results"].items():
            if parser_result.get("success"):
                parser_dir = output_dir / parser_name
                
                # í…ìŠ¤íŠ¸ íŒŒì¼
                text_file = parser_dir / f"{parser_name}_text.txt"
                if text_file.exists():
                    saved_files.append({
                        "type": "extracted_text",
                        "parser": parser_name,
                        "path": str(text_file),
                        "description": f"{parser_name} íŒŒì„œë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸"
                    })
                
                # ë©”íƒ€ë°ì´í„° íŒŒì¼
                metadata_file = parser_dir / f"{parser_name}_metadata.json"
                if metadata_file.exists():
                    saved_files.append({
                        "type": "metadata",
                        "parser": parser_name,
                        "path": str(metadata_file),
                        "description": f"{parser_name} íŒŒì„œ ë©”íƒ€ë°ì´í„°"
                    })
                
                # êµ¬ì¡° ì •ë³´ íŒŒì¼
                structure_file = parser_dir / f"{parser_name}_structure.json"
                if structure_file.exists():
                    saved_files.append({
                        "type": "parser_structure",
                        "parser": parser_name,
                        "path": str(structure_file),
                        "description": f"{parser_name} íŒŒì„œ êµ¬ì¡° ì •ë³´"
                    })
                
                # Docling markdown íŒŒì¼
                if parser_name == "docling" and parser_result.get("md_file_path"):
                    md_file_path = Path(parser_result["md_file_path"])
                    if md_file_path.exists():
                        saved_files.append({
                            "type": "markdown",
                            "parser": parser_name,
                            "path": str(md_file_path),
                            "description": "Docling íŒŒì„œë¡œ ìƒì„±ëœ Markdown íŒŒì¼"
                        })
        
        return DocumentParsingResponse(
            file_info=results["file_info"],
            parsing_timestamp=results["parsing_timestamp"],
            parsers_used=results["parsers_used"],
            parsing_results=results["parsing_results"],
            summary=results["summary"],
            output_directory=str(output_dir),
            saved_files=saved_files
        )
        
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        from utils.error_handler import log_and_raise_http_exception, collect_context_info
        
        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìˆ˜ì§‘
        context = collect_context_info(locals(), ["file_path", "directory", "force_reparse"])
        
        # ìƒì„¸í•œ ì˜¤ë¥˜ ë¡œê¹… ë° HTTPException ë°œìƒ
        log_and_raise_http_exception(
            e, 
            "ë¬¸ì„œ íŒŒì‹±", 
            context=context,
            logger_name=__name__
        )


@router.get("/parse", response_model=DocumentParsingResponse)
async def parse_document_comprehensive_get(
    file_path: str = Query(..., description="íŒŒì‹±í•  ë¬¸ì„œ ê²½ë¡œ"),
    force_reparse: bool = Query(False, description="ì¬íŒŒì‹± ì—¬ë¶€"),
    directory: Optional[str] = Query(None, description="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬"),
    db: Session = Depends(get_db)
):
    """
    GET ë°©ì‹ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì™„ì „ íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    request = ParseDocumentRequest(
        file_path=file_path,
        force_reparse=force_reparse,
        directory=directory
    )
    
    return await parse_document_comprehensive(request, db)


@router.get("/parse/status")
async def get_parsing_status(
    file_path: str = Query(..., description="íŒŒì‹± ìƒíƒœë¥¼ í™•ì¸í•  íŒŒì¼ ê²½ë¡œ"),
    db: Session = Depends(get_db)
):
    """
    ë¬¸ì„œì˜ íŒŒì‹± ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    from pathlib import Path
    
    parser_service = DocumentParserService()
    
    try:
        file_path_obj = Path(file_path)
        
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not file_path_obj.exists():
            if not file_path_obj.is_absolute():
                file_path_obj = Path.cwd() / file_path_obj
                
        exists = file_path_obj.exists()
        supported = parser_service.is_supported_file(file_path_obj) if exists else False
        has_parsing = parser_service.has_parsing_results(file_path_obj) if exists and supported else False
        
        result = {
            "file_path": file_path,
            "exists": exists,
            "supported": supported,
            "has_parsing_results": has_parsing,
            "supported_extensions": parser_service.get_supported_extensions()
        }
        
        if has_parsing:
            parsing_results = parser_service.load_existing_parsing_results(file_path_obj)
            if parsing_results:
                result.update({
                    "parsing_timestamp": parsing_results.get("parsing_timestamp"),
                    "parsers_used": parsing_results.get("parsers_used", []),
                    "summary": parsing_results.get("summary", {}),
                    "output_directory": str(parser_service.get_output_directory(file_path_obj))
                })
        
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.get("/parse/results")
async def get_parsing_results(
    file_path: str = Query(..., description="íŒŒì‹± ê²°ê³¼ë¥¼ ì¡°íšŒí•  íŒŒì¼ ê²½ë¡œ"),
    parser_name: Optional[str] = Query(None, description="íŠ¹ì • íŒŒì„œ ê²°ê³¼ë§Œ ì¡°íšŒ (ì˜ˆ: docling, pdf_parser)"),
    db: Session = Depends(get_db)
):
    """
    ì €ì¥ëœ íŒŒì‹± ê²°ê³¼ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    from pathlib import Path
    
    parser_service = DocumentParserService()
    
    try:
        file_path_obj = Path(file_path)
        if not file_path_obj.is_absolute():
            file_path_obj = Path.cwd() / file_path_obj
            
        if not parser_service.has_parsing_results(file_path_obj):
            raise HTTPException(
                status_code=404, 
                detail=f"íŒŒì¼ì˜ íŒŒì‹± ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"
            )
        
        results = parser_service.load_existing_parsing_results(file_path_obj)
        
        if parser_name:
            # íŠ¹ì • íŒŒì„œ ê²°ê³¼ë§Œ ë°˜í™˜
            if parser_name not in results.get("parsing_results", {}):
                raise HTTPException(
                    status_code=404,
                    detail=f"íŒŒì„œ '{parser_name}'ì˜ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                )
            return {
                "file_info": results["file_info"],
                "parsing_timestamp": results["parsing_timestamp"],
                "parser_result": results["parsing_results"][parser_name]
            }
        else:
            # ì „ì²´ ê²°ê³¼ ë°˜í™˜
            return results
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.post("/analyze", response_model=FileAnalysisResponse)
async def analyze_local_file(
    request: AnalyzeFileRequest,
    db: Session = Depends(get_db)
):
    """
    ë¡œì»¬ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    - **file_path**: ë¶„ì„í•  íŒŒì¼ì˜ ê²½ë¡œ
    - **extractors**: ì‚¬ìš©í•  ì¶”ì¶œê¸° ëª©ë¡ (ê¸°ë³¸ê°’: ì„¤ì •ëœ ê¸°ë³¸ ì¶”ì¶œê¸°)
    - **force_reanalyze**: ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ë¬´ì‹œí•˜ê³  ì¬ë¶„ì„ ì—¬ë¶€ (ê¸°ë³¸ê°’: false)
    - **force_reparse**: íŒŒì‹±ë¶€í„° ë‹¤ì‹œ ìˆ˜í–‰í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: false)
    
    íŒŒì‹± ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¨¼ì € ì™„ì „ íŒŒì‹±ì„ ìˆ˜í–‰í•œ í›„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    from pathlib import Path
    
    analyzer = LocalFileAnalyzer(db)
    parser_service = DocumentParserService()
    
    try:
        file_path = Path(request.file_path)
        if not file_path.is_absolute():
            file_path = Path.cwd() / file_path
            
        # ë””ë ‰í† ë¦¬ íŒŒë¼ë¯¸í„° ì²˜ë¦¬
        directory = None
        if request.directory:
            directory = Path(request.directory)
            if not directory.is_absolute():
                directory = Path.cwd() / directory
            directory.mkdir(parents=True, exist_ok=True)
        
        # 1. íŒŒì‹± ê²°ê³¼ í™•ì¸ ë° í•„ìš”ì‹œ íŒŒì‹± ìˆ˜í–‰
        if not parser_service.has_parsing_results(file_path, directory) or request.force_reparse:
            # íŒŒì‹± ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì¬íŒŒì‹± ìš”ì²­ì‹œ ì™„ì „ íŒŒì‹± ìˆ˜í–‰
            parsing_results = parser_service.parse_document_comprehensive(
                file_path=file_path,
                force_reparse=request.force_reparse,
                directory=directory
            )
        else:
            # ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ ë¡œë“œ
            parsing_results = parser_service.load_existing_parsing_results(file_path, directory)
        
        # 2. íŒŒì‹± ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ë¶„ì„ ìˆ˜í–‰
        result = analyzer.analyze_file(
            file_path=str(file_path),
            extractors=request.extractors,
            force_reanalyze=request.force_reanalyze
        )
        
        # 3. íŒŒì‹± ì •ë³´ë¥¼ ê²°ê³¼ì— ì¶”ê°€
        result["parsing_info"] = {
            "parsing_timestamp": parsing_results.get("parsing_timestamp"),
            "parsers_used": parsing_results.get("parsers_used", []),
            "best_parser": parsing_results.get("summary", {}).get("best_parser"),
            "total_parsers": parsing_results.get("summary", {}).get("total_parsers", 0)
        }
        
        return FileAnalysisResponse(**result)
        
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        from utils.error_handler import log_and_raise_http_exception, collect_context_info
        
        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìˆ˜ì§‘
        context = collect_context_info(locals(), ["file_path", "extractors", "force_reanalyze", "force_reparse"])
        
        # ìƒì„¸í•œ ì˜¤ë¥˜ ë¡œê¹… ë° HTTPException ë°œìƒ
        log_and_raise_http_exception(
            e, 
            "ë¡œì»¬ íŒŒì¼ ë¶„ì„", 
            context=context,
            logger_name=__name__
        )


@router.get("/analyze", response_model=FileAnalysisResponse)
async def analyze_local_file_get(
    file_path: str = Query(..., description="ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ"),
    extractors: Optional[str] = Query(None, description="ì‚¬ìš©í•  ì¶”ì¶œê¸° (ì‰¼í‘œë¡œ êµ¬ë¶„)"),
    force_reanalyze: bool = Query(False, description="ì¬ë¶„ì„ ì—¬ë¶€"),
    force_reparse: bool = Query(False, description="ì¬íŒŒì‹± ì—¬ë¶€"),
    directory: Optional[str] = Query(None, description="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬"),
    db: Session = Depends(get_db)
):
    """
    GET ë°©ì‹ìœ¼ë¡œ ë¡œì»¬ íŒŒì¼ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    extractor_list = None
    if extractors:
        extractor_list = [e.strip() for e in extractors.split(",") if e.strip()]
    
    request = AnalyzeFileRequest(
        file_path=file_path,
        extractors=extractor_list,
        force_reanalyze=force_reanalyze,
        force_reparse=force_reparse,
        directory=directory
    )
    
    return await analyze_local_file(request, db)


@router.get("/status", response_model=FileStatusResponse)
async def get_file_status(
    file_path: str = Query(..., description="í™•ì¸í•  íŒŒì¼ ê²½ë¡œ"),
    db: Session = Depends(get_db)
):
    """
    íŒŒì¼ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤ (ì¡´ì¬ ì—¬ë¶€, ì§€ì› í˜•ì‹ ì—¬ë¶€, ë¶„ì„ ê²°ê³¼ ì¡´ì¬ ì—¬ë¶€).
    """
    analyzer = LocalFileAnalyzer(db)
    
    exists = analyzer.file_exists(file_path)
    supported = analyzer.is_supported_file(file_path) if exists else False
    
    analysis_timestamp = None
    result_file = None
    has_analysis = False
    
    if exists and supported:
        existing_result = analyzer.load_existing_result(file_path)
        if existing_result:
            has_analysis = True
            analysis_timestamp = existing_result.get("analysis_timestamp")
            result_file = str(analyzer.get_result_file_path(file_path))
    
    return FileStatusResponse(
        file_path=file_path,
        exists=exists,
        supported=supported,
        has_analysis=has_analysis,
        analysis_timestamp=analysis_timestamp,
        result_file=result_file
    )


@router.get("/result", response_model=FileAnalysisResponse)
async def get_analysis_result(
    file_path: str = Query(..., description="ê²°ê³¼ë¥¼ ì¡°íšŒí•  íŒŒì¼ ê²½ë¡œ"),
    db: Session = Depends(get_db)
):
    """
    ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    analyzer = LocalFileAnalyzer(db)
    
    existing_result = analyzer.load_existing_result(file_path)
    if not existing_result:
        raise HTTPException(
            status_code=404,
            detail=f"íŒŒì¼ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"
        )
    
    return FileAnalysisResponse(**existing_result)


@router.get("/metadata")
async def get_file_metadata(
    file_path: str = Query(..., description="ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•  íŒŒì¼ ê²½ë¡œ"),
    force_reparse: bool = Query(False, description="íŒŒì‹±ë¶€í„° ë‹¤ì‹œ ìˆ˜í–‰í• ì§€ ì—¬ë¶€"),
    parser_name: Optional[str] = Query(None, description="íŠ¹ì • íŒŒì„œì˜ ë©”íƒ€ë°ì´í„°ë§Œ ì¡°íšŒ (ì˜ˆ: docling, pdf_parser)"),
    directory: Optional[str] = Query(None, description="ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ"),
    use_llm: bool = Query(False, description="LLM ê¸°ë°˜ ë¶„ì„ ì‚¬ìš© ì—¬ë¶€"),
    db: Session = Depends(get_db)
):
    """
    íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    - íŒŒì‹± ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¨¼ì € ì™„ì „ íŒŒì‹±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤
    - ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  íŒŒì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤
    - parser_name ì§€ì •ì‹œ í•´ë‹¹ íŒŒì„œì˜ ë©”íƒ€ë°ì´í„°ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤
    
    Dublin Core í‘œì¤€ ë©”íƒ€ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    from pathlib import Path
    
    parser_service = DocumentParserService()
    
    try:
        file_path_obj = Path(file_path)
        if not file_path_obj.is_absolute():
            file_path_obj = Path.cwd() / file_path_obj
            
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not file_path_obj.exists():
            raise HTTPException(status_code=404, detail=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        # ì§€ì› íŒŒì¼ í˜•ì‹ í™•ì¸
        if not parser_service.is_supported_file(file_path_obj):
            raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_path}")
        
        # ë””ë ‰í† ë¦¬ íŒŒë¼ë¯¸í„° ì²˜ë¦¬
        directory_path = None
        if directory:
            directory_path = Path(directory)
            if not directory_path.is_absolute():
                directory_path = Path.cwd() / directory_path
            directory_path.mkdir(parents=True, exist_ok=True)
        
        # 1. íŒŒì‹± ê²°ê³¼ í™•ì¸ ë° í•„ìš”ì‹œ íŒŒì‹± ìˆ˜í–‰
        if not parser_service.has_parsing_results(file_path_obj, directory_path) or force_reparse:
            parsing_results = parser_service.parse_document_comprehensive(
                file_path=file_path_obj,
                force_reparse=force_reparse,
                directory=directory_path
            )
        else:
            parsing_results = parser_service.load_existing_parsing_results(file_path_obj, directory_path)
            
        # Markdown íŒŒì¼ë“¤ì„ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ë¡œ ì´ë™
        if directory_path:
            _move_markdown_files_to_correct_location(parsing_results, file_path_obj, 
                                                    parser_service.get_output_directory(file_path_obj, directory_path))
        
        # 2. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        if parser_name:
            # íŠ¹ì • íŒŒì„œì˜ ë©”íƒ€ë°ì´í„°ë§Œ ë°˜í™˜
            if parser_name not in parsing_results.get("parsing_results", {}):
                raise HTTPException(
                    status_code=404,
                    detail=f"íŒŒì„œ '{parser_name}'ì˜ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                )
            parser_result = parsing_results["parsing_results"][parser_name]
            if not parser_result.get("success"):
                raise HTTPException(
                    status_code=400,
                    detail=f"íŒŒì„œ '{parser_name}'ì˜ íŒŒì‹±ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {parser_result.get('error_message')}"
                )
            # ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
            output_dir = parser_service.get_output_directory(file_path_obj, directory_path)
            saved_files = _collect_saved_files(output_dir, parsing_results)
            
            return {
                "file_info": parsing_results["file_info"],
                "parser_name": parser_name,
                "metadata": parser_result.get("metadata"),
                "parsing_timestamp": parsing_results.get("parsing_timestamp"),
                "output_directory": str(output_dir),
                "saved_files": saved_files
            }
        else:
            # ëª¨ë“  íŒŒì„œì˜ ë©”íƒ€ë°ì´í„° ë°˜í™˜
            all_metadata = {}
            for parser, result in parsing_results.get("parsing_results", {}).items():
                if result.get("success") and result.get("metadata"):
                    all_metadata[parser] = result["metadata"]
            
            # ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
            output_dir = parser_service.get_output_directory(file_path_obj, directory_path)
            saved_files = _collect_saved_files(output_dir, parsing_results)
            
            return {
                "file_info": parsing_results["file_info"],
                "parsing_timestamp": parsing_results.get("parsing_timestamp"),
                "parsers_used": parsing_results.get("parsers_used", []),
                "metadata_by_parser": all_metadata,
                "summary": parsing_results.get("summary", {}),
                "output_directory": str(output_dir),
                "saved_files": saved_files
            }
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.post("/metadata")
async def extract_file_metadata_post(
    request: dict,
    db: Session = Depends(get_db)
):
    """
    íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤ (POST ë°©ì‹).
    """
    file_path = request.get("file_path")
    if not file_path:
        raise HTTPException(status_code=400, detail="íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤")
    
    force_reparse = request.get("force_reparse", False)
    parser_name = request.get("parser_name", None)
    directory = request.get("directory", None)
    use_llm = request.get("use_llm", False)
    
    return await get_file_metadata(file_path, force_reparse, parser_name, directory, use_llm, db)


@router.post("/structure-analysis")
async def analyze_document_structure(
    request: dict,
    db: Session = Depends(get_db)
):
    """
    ë¬¸ì„œì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    - íŒŒì‹± ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¨¼ì € ì™„ì „ íŒŒì‹±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤
    - ë¬¸ì„œì˜ êµ¬ì¡°ì  ìš”ì†Œë“¤ (í—¤ë”, ë‹¨ë½, í…Œì´ë¸”, ì´ë¯¸ì§€ ë“±)ì„ ë¶„ì„í•©ë‹ˆë‹¤
    - ê²°ê³¼ëŠ” íŒŒì¼ë¡œ ì €ì¥ë˜ë©° ê¸°ë³¸ì ìœ¼ë¡œ ì¬ì‚¬ìš©ë©ë‹ˆë‹¤
    """
    from pathlib import Path
    import json
    
    file_path = request.get("file_path")
    if not file_path:
        raise HTTPException(status_code=400, detail="íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤")
    
    force_reparse = request.get("force_reparse", False)
    force_reanalyze = request.get("force_reanalyze", False)
    use_llm = request.get("use_llm", True)  # LLM ê¸°ë°˜ êµ¬ì¡° ë¶„ì„ ì˜µì…˜ (ê¸°ë³¸ê°’: True)
    directory = request.get("directory")  # ë””ë ‰í† ë¦¬ ì˜µì…˜ ì¶”ê°€
    
    parser_service = DocumentParserService()
    analyzer = LocalFileAnalyzer(db)
    
    try:
        # ìš°ì„  í‘œì¤€í™”ëœ ì ˆëŒ€ ê²½ë¡œ í•´ì„(í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€)
        file_path_obj = analyzer.get_absolute_path(file_path)

        # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì—…ë¡œë“œ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì¬ì‹œë„
        if not file_path_obj.exists():
            try:
                file_root = analyzer.get_file_root()
                candidate = Path(file_root) / file_path if not Path(file_path).is_absolute() else Path(file_path)
                if candidate.exists():
                    file_path_obj = candidate.resolve()
            except Exception:
                pass

        # ìµœì¢… ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not file_path_obj.exists() or not file_path_obj.is_file():
            raise HTTPException(status_code=404, detail=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        # ë””ë ‰í† ë¦¬ íŒŒë¼ë¯¸í„° ì²˜ë¦¬
        directory_path = None
        if directory:
            directory_path = Path(directory)
            if not directory_path.is_absolute():
                directory_path = Path.cwd() / directory_path
            directory_path.mkdir(parents=True, exist_ok=True)
        
        # êµ¬ì¡° ë¶„ì„ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
        output_dir = parser_service.get_output_directory(file_path_obj, directory_path)
        structure_result_path = output_dir / ("llm_structure_analysis.json" if use_llm else "structure_analysis.json")
        
        # ê¸°ì¡´ êµ¬ì¡° ë¶„ì„ ê²°ê³¼ í™•ì¸
        if not force_reanalyze and structure_result_path.exists():
            with open(structure_result_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # 1. íŒŒì‹± ê²°ê³¼ í™•ì¸ ë° í•„ìš”ì‹œ íŒŒì‹± ìˆ˜í–‰
        if not parser_service.has_parsing_results(file_path_obj, directory_path) or force_reparse:
            parsing_results = parser_service.parse_document_comprehensive(
                file_path=file_path_obj,
                force_reparse=force_reparse,
                directory=directory_path
            )
        else:
            parsing_results = parser_service.load_existing_parsing_results(file_path_obj, directory_path)
            
        # Markdown íŒŒì¼ë“¤ì„ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ë¡œ ì´ë™ (ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ ë¡œë“œ ì‹œì—ë„ í•„ìš”)
        if parsing_results.get("parsing_results"):
            _move_markdown_files_to_correct_location(parsing_results, file_path_obj, output_dir)
        
        # 2. êµ¬ì¡° ë¶„ì„ ìˆ˜í–‰
        if use_llm:
            # LLM ê¸°ë°˜ êµ¬ì¡° ë¶„ì„ ìˆ˜í–‰
            # ìµœê³  í’ˆì§ˆ íŒŒì„œì˜ í…ìŠ¤íŠ¸ ì‚¬ìš©
            best_parser = parsing_results.get("summary", {}).get("best_parser")
            document_text = ""
            if best_parser and best_parser in parsing_results.get("parsing_results", {}):
                parser_dir = output_dir / best_parser
                text_file = parser_dir / f"{best_parser}_text.txt"
                if text_file.exists():
                    with open(text_file, 'r', encoding='utf-8') as f:
                        document_text = f.read()
            
            structure_analysis = analyzer.analyze_document_structure_with_llm(
                text=document_text,
                file_path=str(file_path_obj),
                file_extension=file_path_obj.suffix.lower()
            )

            # LLM ë¶„ì„ ê²°ê³¼ ê²€ì¦
            if not structure_analysis or not structure_analysis.get("llm_analysis"):
                raise HTTPException(
                    status_code=500,
                    detail="LLM êµ¬ì¡° ë¶„ì„ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. JSON íŒŒì‹± ì˜¤ë¥˜ ë˜ëŠ” LLM ì‘ë‹µ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                )

            # íŒŒì‹± ì •ë³´ ì¶”ê°€
            structure_analysis["file_info"] = parsing_results["file_info"]
            structure_analysis["analysis_timestamp"] = datetime.now().isoformat()
            structure_analysis["source_parser"] = best_parser
            
        else:
            # ê¸°ì¡´ ê¸°ë³¸ êµ¬ì¡° ë¶„ì„ ìˆ˜í–‰
            structure_analysis = {
                "file_info": parsing_results["file_info"],
                "analysis_timestamp": datetime.now().isoformat(),
                "structure_elements": {},
                "summary": {
                    "total_elements": 0,
                    "element_types": {},
                    "complexity_score": 0
                }
            }
        
        # ê¸°ë³¸ êµ¬ì¡° ë¶„ì„ì˜ ê²½ìš°ì—ë§Œ íŒŒì„œë³„ êµ¬ì¡° ì •ë³´ ìˆ˜ì§‘
        if not use_llm:
            # ê° íŒŒì„œë³„ êµ¬ì¡° ì •ë³´ ìˆ˜ì§‘
            for parser_name, parser_result in parsing_results.get("parsing_results", {}).items():
                if not parser_result.get("success"):
                    continue
                    
                elements = {}
                
                # êµ¬ì¡°í™”ëœ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° í™œìš©
                if parser_result.get("structured_info"):
                    structured_info = parser_result["structured_info"]
                    
                    # ê¸°ë³¸ êµ¬ì¡° ìš”ì†Œë“¤
                    elements.update({
                        "total_lines": structured_info.get("total_lines", 0),
                        "paragraphs": structured_info.get("paragraphs", 0),
                        "headers": structured_info.get("headers", 0),
                        "non_empty_lines": structured_info.get("non_empty_lines", 0)
                    })
                    
                    # Docling íŒŒì„œì˜ ê²½ìš° ì¶”ê°€ êµ¬ì¡° ì •ë³´
                    if parser_name == "docling" and "document_structure" in structured_info:
                        doc_structure = structured_info["document_structure"]
                        elements.update({
                            "tables": doc_structure.get("tables", []),
                            "images": doc_structure.get("images", []),
                            "sections": doc_structure.get("sections", []),
                            "table_count": len(doc_structure.get("tables", [])),
                            "image_count": len(doc_structure.get("images", [])),
                            "section_count": len(doc_structure.get("sections", []))
                        })
                
                # í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ê°€ ë¶„ì„ (ëª¨ë“  íŒŒì„œì— ëŒ€í•´)
                if "text_length" in parser_result:
                    text_length = parser_result["text_length"]
                    word_count = parser_result.get("word_count", 0)
                    
                    # ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°
                    complexity = 0
                    if word_count > 0:
                        complexity += min(word_count / 1000, 1.0) * 0.4  # ë‹¨ì–´ ìˆ˜ ê¸°ë°˜
                    if elements.get("headers", 0) > 0:
                        complexity += min(elements["headers"] / 10, 1.0) * 0.3  # í—¤ë” ìˆ˜ ê¸°ë°˜
                    if elements.get("table_count", 0) > 0:
                        complexity += min(elements["table_count"] / 5, 1.0) * 0.3  # í…Œì´ë¸” ìˆ˜ ê¸°ë°˜
                        
                    elements["complexity_score"] = complexity
                
                structure_analysis["structure_elements"][parser_name] = elements
            
            # ì „ì²´ ìš”ì•½ ê³„ì‚° (ê¸°ë³¸ ë¶„ì„ì˜ ê²½ìš°ì—ë§Œ)
            all_elements = structure_analysis["structure_elements"]
            if all_elements:
                # ê°€ì¥ ë³µì¡ë„ê°€ ë†’ì€ íŒŒì„œì˜ ê²°ê³¼ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìš”ì•½
                best_parser = max(all_elements.keys(), 
                                key=lambda p: all_elements[p].get("complexity_score", 0))
                best_elements = all_elements[best_parser]
                
                structure_analysis["summary"] = {
                    "best_parser": best_parser,
                    "total_elements": sum(1 for k, v in best_elements.items() 
                                        if isinstance(v, int) and v > 0),
                    "element_types": {k: v for k, v in best_elements.items() 
                                    if isinstance(v, int) and v > 0},
                    "complexity_score": best_elements.get("complexity_score", 0),
                    "has_tables": best_elements.get("table_count", 0) > 0,
                    "has_images": best_elements.get("image_count", 0) > 0,
                    "has_sections": best_elements.get("section_count", 0) > 0
                }
        
        # ê²°ê³¼ ì €ì¥
        output_dir.mkdir(exist_ok=True)
        with open(structure_result_path, 'w', encoding='utf-8') as f:
            json.dump(structure_analysis, f, ensure_ascii=False, indent=2)
        
        # ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
        saved_files = []
        
        # êµ¬ì¡° ë¶„ì„ ê²°ê³¼ íŒŒì¼
        saved_files.append({
            "type": "structure_analysis",
            "path": str(structure_result_path),
            "description": "LLM ê¸°ë°˜ êµ¬ì¡° ë¶„ì„ ê²°ê³¼" if use_llm else "ê¸°ë³¸ êµ¬ì¡° ë¶„ì„ ê²°ê³¼"
        })
        
        # íŒŒì‹± ê´€ë ¨ íŒŒì¼ë“¤
        if parsing_results.get("parsing_results"):
            for parser_name, parser_result in parsing_results["parsing_results"].items():
                if parser_result.get("success"):
                    parser_dir = output_dir / parser_name
                    
                    # í…ìŠ¤íŠ¸ íŒŒì¼
                    text_file = parser_dir / f"{parser_name}_text.txt"
                    if text_file.exists():
                        saved_files.append({
                            "type": "extracted_text",
                            "parser": parser_name,
                            "path": str(text_file),
                            "description": f"{parser_name} íŒŒì„œë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸"
                        })
                    
                    # ë©”íƒ€ë°ì´í„° íŒŒì¼
                    metadata_file = parser_dir / f"{parser_name}_metadata.json"
                    if metadata_file.exists():
                        saved_files.append({
                            "type": "metadata",
                            "parser": parser_name,
                            "path": str(metadata_file),
                            "description": f"{parser_name} íŒŒì„œ ë©”íƒ€ë°ì´í„°"
                        })
                    
                    # êµ¬ì¡° ì •ë³´ íŒŒì¼
                    structure_file = parser_dir / f"{parser_name}_structure.json"
                    if structure_file.exists():
                        saved_files.append({
                            "type": "parser_structure",
                            "parser": parser_name,
                            "path": str(structure_file),
                            "description": f"{parser_name} íŒŒì„œ êµ¬ì¡° ì •ë³´"
                        })
                    
                    # Markdown íŒŒì¼ë“¤ í™•ì¸ (Docling, PyMuPDF4LLM)
                    if parser_name == "docling":
                        # Docling markdown íŒŒì¼
                        md_file = output_dir / "docling.md"
                        if md_file.exists():
                            saved_files.append({
                                "type": "markdown",
                                "parser": parser_name,
                                "path": str(md_file),
                                "description": "Docling íŒŒì„œë¡œ ìƒì„±ëœ Markdown"
                            })
                    elif parser_name == "pdf_parser":
                        # PyMuPDF4LLM markdown íŒŒì¼ (pdf_parser ë‚´ì—ì„œ ìƒì„±ë¨)
                        pymupdf_md_file = output_dir / "pymupdf4llm.md"
                        if pymupdf_md_file.exists():
                            saved_files.append({
                                "type": "markdown",
                                "parser": "pymupdf4llm",
                                "path": str(pymupdf_md_file),
                                "description": "PyMuPDF4LLM íŒŒì„œë¡œ ìƒì„±ëœ Markdown"
                            })
        
        # íŒŒì‹± ê²°ê³¼ ì¢…í•© íŒŒì¼
        parsing_result_path = parser_service.get_parsing_result_path(file_path_obj, directory_path)
        if parsing_result_path.exists():
            saved_files.append({
                "type": "parsing_summary",
                "path": str(parsing_result_path),
                "description": "íŒŒì‹± ê²°ê³¼ ì¢…í•©"
            })
        
        # ì‘ë‹µì— íŒŒì¼ ê²½ë¡œ ì •ë³´ ì¶”ê°€
        structure_analysis["saved_files"] = saved_files
        structure_analysis["output_directory"] = str(output_dir)
        
        return structure_analysis
        
    except HTTPException:
        raise
    except Exception as e:
        from utils.error_handler import log_and_raise_http_exception, collect_context_info
        
        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìˆ˜ì§‘
        context = collect_context_info(locals())
        
        # ìƒì„¸í•œ ì˜¤ë¥˜ ë¡œê¹… ë° HTTPException ë°œìƒ
        log_and_raise_http_exception(
            e, 
            "ë¬¸ì„œ êµ¬ì¡° ë¶„ì„", 
            context=context,
            logger_name=__name__
        )


@router.get("/structure-analysis")
async def get_structure_analysis(
    file_path: str = Query(..., description="êµ¬ì¡° ë¶„ì„ ê²°ê³¼ë¥¼ ì¡°íšŒí•  íŒŒì¼ ê²½ë¡œ"),
    force_reparse: bool = Query(False, description="ì¬íŒŒì‹± ì—¬ë¶€"),
    force_reanalyze: bool = Query(False, description="ì¬ë¶„ì„ ì—¬ë¶€"),
    use_llm: bool = Query(False, description="LLM ê¸°ë°˜ êµ¬ì¡° ë¶„ì„ ì‚¬ìš© ì—¬ë¶€"),
    directory: Optional[str] = Query(None, description="ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ"),
    db: Session = Depends(get_db)
):
    """
    GET ë°©ì‹ìœ¼ë¡œ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ì„ ìˆ˜í–‰í•˜ê±°ë‚˜ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    Parameters:
    - file_path: ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ
    - force_reparse: ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ ë¬´ì‹œí•˜ê³  ì¬íŒŒì‹± ìˆ˜í–‰
    - force_reanalyze: ê¸°ì¡´ êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ë¬´ì‹œí•˜ê³  ì¬ë¶„ì„ ìˆ˜í–‰
    - use_llm: LLMì„ ì‚¬ìš©í•œ ê³ ê¸‰ êµ¬ì¡° ë¶„ì„ ìˆ˜í–‰ (ê¸°ë³¸ê°’: False)
    - directory: ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: íŒŒì¼ëª… ê¸°ë°˜)
    """
    request = {
        "file_path": file_path,
        "force_reparse": force_reparse,
        "force_reanalyze": force_reanalyze,
        "use_llm": use_llm,
        "directory": directory
    }
    
    return await analyze_document_structure(request, db)


def _integrate_llm_structure_into_kg(kg_result: Dict[str, Any], llm_analysis: Dict[str, Any], file_path: str, dataset_id: str = None) -> Dict[str, Any]:
    """
    LLM êµ¬ì¡° ë¶„ì„ ê²°ê³¼ë¥¼ Knowledge Graphì— í†µí•©í•©ë‹ˆë‹¤.
    structureAnalysis ì„¹ì…˜ë§Œ ì‚¬ìš©í•˜ì—¬ ê¹¨ë—í•œ ì§€ì‹ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
    
    Args:
        kg_result: ê¸°ì¡´ KG ê²°ê³¼
        llm_analysis: LLM êµ¬ì¡° ë¶„ì„ ê²°ê³¼
        file_path: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        í†µí•©ëœ KG ê²°ê³¼
    """
    import hashlib
    import os
    
    def _hash(s: str) -> str:
        # None ë˜ëŠ” ë¹ˆ ê°’ ì²˜ë¦¬
        if s is None:
            s = ""
        elif not isinstance(s, str):
            s = str(s)
        return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()[:16]
    
    # íƒ€ì… ì•ˆì „ì„± í™•ì¸
    if not isinstance(kg_result, dict) or not isinstance(llm_analysis, dict):
        return kg_result
    
    if not llm_analysis:
        return kg_result
    
    # structureAnalysisë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ìƒˆë¡œìš´ ì—”í‹°í‹°/ê´€ê³„ ë¦¬ìŠ¤íŠ¸ë¡œ ì‹œì‘
    entities = []
    relationships = []
    structural_hierarchy = []
    
    # ë¬¸ì„œ ë…¸ë“œ ìƒì„± (structureAnalysis ì „ìš©)
    doc_info = llm_analysis.get("documentInfo", {})
    doc_entity_id = f"doc_{_hash(file_path)}"
    
    # ë¬¸ì„œ ì—”í‹°í‹° ìƒì„±
    doc_properties = {
        "title": doc_info.get("title", "") if doc_info else os.path.basename(file_path),
        "document_type": doc_info.get("documentType", "") if doc_info else "document",
        "file_path": file_path,
        "publication_date": doc_info.get("publicationInfo", {}).get("publicationDate", "") if doc_info else "",
        "publishing_institution": doc_info.get("publicationInfo", {}).get("publishingInstitution", "") if doc_info else ""
    }
    
    # dataset_idê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if dataset_id:
        doc_properties["dataset_id"] = dataset_id
    
    doc_entity = {
        "id": doc_entity_id,
        "type": "Document",
        "properties": doc_properties
    }
    entities.append(doc_entity)
    
    # structureAnalysisì˜ ê³„ì¸µì  ë¬¸ì„œ êµ¬ì¡°ë¥¼ ê·¸ë˜í”„ë¡œ êµ¬ì„±
    for section_idx, section in enumerate(llm_analysis.get("structureAnalysis", [])):
        # ë©”ì¸ ì„¹ì…˜ ì—”í‹°í‹°
        section_properties = {
            "title": section.get("title", ""),
            "unit": section.get("unit", ""),
            "content": section.get("mainContent", ""),
            "keywords": section.get("keywords", []),
            "page": section.get("page", ""),
            "section_index": section_idx,
            "hierarchical_level": 1,
            "section_type": "main_section"
        }
        
        # dataset_idê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if dataset_id:
            section_properties["dataset_id"] = dataset_id
        
        # titleì´ Noneì¸ ê²½ìš° ì²˜ë¦¬
        section_title = section.get('title')
        if section_title is None:
            section_title = f"Section_{section_idx}"
            
        section_entity = {
            "id": f"llm_section_{_hash(section_title)}_{section_idx}",
            "type": "DocumentSection",
            "properties": section_properties
        }
        entities.append(section_entity)
        
        # ë¬¸ì„œ-ì„¹ì…˜ ê´€ê³„
        relationships.append({
            "id": f"rel_{_hash(doc_entity_id + section_entity['id'])}",
            "type": "CONTAINS_SECTION",
            "source": doc_entity_id,
            "target": section_entity["id"],
            "properties": {
                "unit": section.get("unit", ""),
                "section_index": section_idx,
                "relationship_type": "document_structure"
            }
        })
        
        # ì„¹ì…˜ì˜ í‚¤ì›Œë“œë“¤ì„ ê°œë³„ ì—”í‹°í‹°ë¡œ ì¶”ê°€í•˜ê³  ê´€ê³„ ì„¤ì •
        for keyword in section.get("keywords", []):
            keyword_properties = {
                "text": keyword,
                "source_section": section.get("title", ""),
                "section_unit": section.get("unit", ""),
                "extraction_method": "llm_structure_analysis"
            }
            
            # dataset_idê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if dataset_id:
                keyword_properties["dataset_id"] = dataset_id
            
            keyword_entity = {
                "id": f"llm_keyword_{_hash(keyword)}_{section_idx}",
                "type": "SectionKeyword",
                "properties": keyword_properties
            }
            entities.append(keyword_entity)
            
            # ì„¹ì…˜-í‚¤ì›Œë“œ ê´€ê³„
            relationships.append({
                "id": f"rel_{_hash(section_entity['id'] + keyword_entity['id'])}",
                "type": "HAS_KEYWORD",
                "source": section_entity["id"],
                "target": keyword_entity["id"],
                "properties": {"extraction_source": "section_content"}
            })
        
        # í•˜ìœ„ êµ¬ì¡° (subStructure) ì²˜ë¦¬
        for subsection_idx, subsection in enumerate(section.get("subStructure", [])):
            subsection_properties = {
                "title": subsection.get("title", ""),
                "unit": subsection.get("unit", ""),
                "content": subsection.get("mainContent", ""),
                "keywords": subsection.get("keywords", []),
                "parent_section": section.get("title", ""),
                "subsection_index": subsection_idx,
                "hierarchical_level": 2,
                "section_type": "subsection"
            }
            
            # dataset_idê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if dataset_id:
                subsection_properties["dataset_id"] = dataset_id
            
            subsection_entity = {
                "id": f"llm_subsection_{_hash(subsection.get('title', ''))}_{section_idx}_{subsection_idx}",
                "type": "DocumentSubsection",
                "properties": subsection_properties
            }
            entities.append(subsection_entity)
            
            # ì„¹ì…˜-í•˜ìœ„ì„¹ì…˜ ê´€ê³„
            relationships.append({
                "id": f"rel_{_hash(section_entity['id'] + subsection_entity['id'])}",
                "type": "HAS_SUBSECTION",
                "source": section_entity["id"],
                "target": subsection_entity["id"],
                "properties": {
                    "unit": subsection.get("unit", ""),
                    "subsection_index": subsection_idx,
                    "relationship_type": "hierarchical_structure"
                }
            })
            
            # í•˜ìœ„ì„¹ì…˜ì˜ í‚¤ì›Œë“œë“¤ì„ ê°œë³„ ì—”í‹°í‹°ë¡œ ì¶”ê°€
            for keyword in subsection.get("keywords", []):
                subsection_keyword_entity = {
                    "id": f"llm_sub_keyword_{_hash(keyword)}_{section_idx}_{subsection_idx}",
                    "type": "SubsectionKeyword",
                    "properties": {
                        "text": keyword,
                        "source_subsection": subsection.get("title", ""),
                        "parent_section": section.get("title", ""),
                        "subsection_unit": subsection.get("unit", ""),
                        "extraction_method": "llm_structure_analysis"
                    }
                }
                entities.append(subsection_keyword_entity)
                
                # í•˜ìœ„ì„¹ì…˜-í‚¤ì›Œë“œ ê´€ê³„
                relationships.append({
                    "id": f"rel_{_hash(subsection_entity['id'] + subsection_keyword_entity['id'])}",
                    "type": "HAS_KEYWORD",
                    "source": subsection_entity["id"],
                    "target": subsection_keyword_entity["id"],
                    "properties": {"extraction_source": "subsection_content"}
                })
    
    # structureAnalysisë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ coreContent, keyData ë“±ì€ ì œê±°
    # ì˜¤ì§ ë¬¸ì„œ êµ¬ì¡°(ì„¹ì…˜, í•˜ìœ„ì„¹ì…˜, í‚¤ì›Œë“œ)ë§Œ ê·¸ë˜í”„ë¡œ êµ¬ì„±
    
    return {
        **kg_result,
        "entities": entities,
        "relationships": relationships,
        "structural_hierarchy": structural_hierarchy
    }


@router.post("/knowledge-graph")
async def generate_knowledge_graph(
    request: dict,
    db: Session = Depends(get_db)
):
    """
    ë¬¸ì„œë¡œë¶€í„° Knowledge Graphë¥¼ ìƒì„±í•˜ê³  ì €ì¥ëœ íŒŒì¼ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    - íŒŒì‹± ë° í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ KGë¥¼ ìƒì„±í•©ë‹ˆë‹¤
    - ìƒì„±ëœ ê²°ê³¼ëŠ” íŒŒì¼ë¡œ ì €ì¥ë˜ë©° ê¸°ë³¸ì ìœ¼ë¡œ ì¬ì‚¬ìš©ë©ë‹ˆë‹¤
    - force_* ì˜µì…˜ì´ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ê²°ê³¼ë¥¼ ë°”ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤
    - ì‘ë‹µì€ saved_files ëª©ë¡ê³¼ í†µê³„ ì •ë³´ë§Œ í¬í•¨í•©ë‹ˆë‹¤
    - dataset_idê°€ ì œê³µë˜ë©´ ëª¨ë“  ë…¸ë“œì— dataset í”„ë¡œí¼í‹°ê°€ ì¶”ê°€ë©ë‹ˆë‹¤
    """
    from pathlib import Path
    import json
    from services.kg_builder import KGBuilder
    
    file_path = request.get("file_path")
    if not file_path:
        raise HTTPException(status_code=400, detail="íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤")
    
    # íŒŒì¼ ê²½ë¡œ ì´ˆê¸° ê²€ì¦
    file_path_obj = Path(file_path)
    if not file_path_obj.is_absolute():
        file_path_obj = Path.cwd() / file_path_obj
    
    # ì¦‰ì‹œ íŒŒì¼ ê²½ë¡œ ê²€ì¦
    if not file_path_obj.exists():
        raise HTTPException(status_code=404, detail=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    if file_path_obj.is_dir():
        raise HTTPException(status_code=400, detail=f"ë””ë ‰í† ë¦¬ê°€ ì•„ë‹Œ íŒŒì¼ì´ì–´ì•¼ í•©ë‹ˆë‹¤: {file_path}")
    
    if file_path_obj.stat().st_size == 0:
        raise HTTPException(status_code=400, detail=f"íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {file_path}")
    
    # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ì¸ì§€ í™•ì¸
    supported_extensions = {'.pdf', '.docx', '.doc', '.txt', '.md', '.html', '.xml', '.hwp'}
    if file_path_obj.suffix.lower() not in supported_extensions:
        raise HTTPException(status_code=400, detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_path_obj.suffix}")
    
    try:
        # íŒŒì¼ ì ‘ê·¼ ê¶Œí•œ í™•ì¸
        with open(file_path_obj, 'rb') as f:
            f.read(1)  # 1ë°”ì´íŠ¸ ì½ê¸° í…ŒìŠ¤íŠ¸
    except PermissionError:
        raise HTTPException(status_code=403, detail=f"íŒŒì¼ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
    
    force_reparse = request.get("force_reparse", False)
    force_reanalyze = request.get("force_reanalyze", False)
    force_rebuild = request.get("force_rebuild", False)
    use_llm = request.get("use_llm", True)  # LLM ê¸°ë°˜ ë¶„ì„ ì˜µì…˜ (ê¸°ë³¸ê°’: True)
    llm_overrides = request.get("llm") or request.get("llm_config")  # ë‹¨ì¼ ìš”ì²­ìš© LLM ì„¤ì •
    if use_llm and isinstance(llm_overrides, dict) and "enabled" not in llm_overrides:
        llm_overrides["enabled"] = True
    directory = request.get("directory")
    dataset_id = request.get("dataset_id")  # ì„ íƒì  dataset_id íŒŒë¼ë¯¸í„°
    
    parser_service = DocumentParserService()
    analyzer = LocalFileAnalyzer(db)
    
    try:
        # ë””ë ‰í† ë¦¬ íŒŒë¼ë¯¸í„° ì²˜ë¦¬
        directory_path = None
        if directory:
            directory_path = Path(directory)
            if not directory_path.is_absolute():
                directory_path = Path.cwd() / directory_path
            directory_path.mkdir(parents=True, exist_ok=True)
        
        # Knowledge Graph ê²°ê³¼ íŒŒì¼ ê²½ë¡œë“¤
        output_dir = parser_service.get_output_directory(file_path_obj, directory_path)
        kg_result_path = output_dir / "knowledge_graph.json"  # ì „ì²´ KG ë°ì´í„°
        kg_response_path = output_dir / "knowledge_graph_response.json"  # API ì‘ë‹µìš©

        # force ì˜µì…˜ì´ ì—†ê³  ê¸°ì¡´ ì‘ë‹µì´ ìˆëŠ” ê²½ìš° ë°”ë¡œ ë°˜í™˜
        if not any([force_reparse, force_reanalyze, force_rebuild]) and kg_response_path.exists():
            with open(kg_response_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # 1. íŒŒì‹± ê²°ê³¼ í™•ì¸ ë° í•„ìš”ì‹œ íŒŒì‹± ìˆ˜í–‰
        parsing_results = {}
        try:
            if not parser_service.has_parsing_results(file_path_obj, directory_path) or force_reparse:
                parsing_results = parser_service.parse_document_comprehensive(
                    file_path=file_path_obj,
                    force_reparse=force_reparse,
                    directory=directory_path
                )
            else:
                parsing_results = parser_service.load_existing_parsing_results(file_path_obj, directory_path)
        except Exception as parsing_error:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ ê²°ê³¼ë¡œ ê³„ì† ì§„í–‰
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(f"âš ï¸ íŒŒì‹± ì‹¤íŒ¨, ë¹ˆ ê²°ê³¼ë¡œ KG ìƒì„± ì§„í–‰: {parsing_error}")
            parsing_results = {
                "parsing_results": {},
                "summary": {"success": False, "best_parser": None, "error": str(parsing_error)},
                "file_info": {
                    "name": file_path_obj.name,
                    "path": str(file_path_obj),
                    "size": file_path_obj.stat().st_size if file_path_obj.exists() else 0,
                    "extension": file_path_obj.suffix.lower()
                }
            }
            
        # Markdown íŒŒì¼ë“¤ì„ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ë¡œ ì´ë™ (ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ ë¡œë“œ ì‹œì—ë„ í•„ìš”)
        if parsing_results.get("parsing_results"):
            _move_markdown_files_to_correct_location(parsing_results, file_path_obj, output_dir)
        
        # 2. êµ¬ì¡° ë¶„ì„ ìˆ˜í–‰ (KG êµ¬ì¶•ì˜ í•„ìˆ˜ ì „ì œì¡°ê±´)
        structure_result_path = output_dir / ("llm_structure_analysis.json" if use_llm else "structure_analysis.json")
        structure_results = {}
        
        # êµ¬ì¡° ë¶„ì„ì´ ì—†ê±°ë‚˜ force_rebuildì¸ ê²½ìš° êµ¬ì¡° ë¶„ì„ ìˆ˜í–‰
        if not structure_result_path.exists() or force_rebuild or force_reanalyze:
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            best_parser = parsing_results.get("summary", {}).get("best_parser")
            document_text = ""
            if best_parser and best_parser in parsing_results.get("parsing_results", {}):
                parser_dir = parser_service.get_output_directory(file_path_obj, directory_path) / best_parser
                text_file = parser_dir / f"{best_parser}_text.txt"
                if text_file.exists():
                    with open(text_file, 'r', encoding='utf-8') as f:
                        document_text = f.read()
            
            if use_llm:
                # LLM ê¸°ë°˜ êµ¬ì¡° ë¶„ì„ ìˆ˜í–‰
                structure_results = analyzer.analyze_document_structure_with_llm(
                    text=document_text,
                    file_path=str(file_path_obj),
                    file_extension=file_path_obj.suffix.lower(),
                    overrides=llm_overrides
                )
            else:
                # ê¸°ë³¸ êµ¬ì¡° ë¶„ì„ ìˆ˜í–‰
                structure_results = analyzer.analyze_document_structure(
                    text=document_text,
                    file_extension=file_path_obj.suffix
                )
            
            # íŒŒì‹± ì •ë³´ ì¶”ê°€
            structure_results["file_info"] = parsing_results["file_info"]
            structure_results["analysis_timestamp"] = datetime.now().isoformat()
            structure_results["source_parser"] = best_parser
            
            # êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ì €ì¥
            with open(structure_result_path, 'w', encoding='utf-8') as f:
                json.dump(structure_results, f, ensure_ascii=False, indent=2)
        else:
            with open(structure_result_path, 'r', encoding='utf-8') as f:
                structure_results = json.load(f)
        
        # 3. í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ í™•ì¸ ë° í•„ìš”ì‹œ ë¶„ì„ ìˆ˜í–‰
        analysis_result_path = output_dir / "keyword_analysis.json"
        if use_llm:
            # LLM ëª¨ë“œì—ì„œëŠ” êµ¬ì¡° ë¶„ì„ ë‚´ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë³„ë„ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìƒëµ
            analysis_results = {"keywords": {}}
        else:
            if not force_reanalyze and analysis_result_path.exists():
                with open(analysis_result_path, 'r', encoding='utf-8') as f:
                    analysis_results = json.load(f)
            else:
                # í‚¤ì›Œë“œ ì¶”ì¶œ ìˆ˜í–‰
                result = analyzer.analyze_file(
                    file_path=str(file_path_obj),
                    extractors=None,  # ëª¨ë“  ì¶”ì¶œê¸° ì‚¬ìš©
                    force_reanalyze=force_reanalyze
                )
                analysis_results = result
        
        # 4. ê³„ì¸µì  Knowledge Graph ìƒì„±
        from services.hierarchical_kg_builder import HierarchicalKGBuilder
        # LLMì„ ì‚¬ìš©í•  ê²½ìš° ìë™ Memgraph ì €ì¥ì„ ë¹„í™œì„±í™” (í–¥ìƒëœ ë²„ì „ì„ ë‚˜ì¤‘ì— ì €ì¥)
        kg_builder = HierarchicalKGBuilder(db_session=db, auto_save_to_memgraph=not use_llm, llm_config=llm_overrides if llm_overrides else None)
        
        # ìµœê³  í’ˆì§ˆ íŒŒì„œì˜ í…ìŠ¤íŠ¸ ì‚¬ìš©  
        best_parser = parsing_results.get("summary", {}).get("best_parser")
        if best_parser and best_parser in parsing_results.get("parsing_results", {}):
            parser_result = parsing_results["parsing_results"][best_parser]
            # ê°œë³„ íŒŒì„œ í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì½ê¸°
            parser_dir = output_dir / best_parser
            text_file = parser_dir / f"{best_parser}_text.txt"
            
            if text_file.exists():
                with open(text_file, 'r', encoding='utf-8') as f:
                    document_text = f.read()
            else:
                document_text = ""
        else:
            document_text = ""
        
        # ê³„ì¸µì  KG ìƒì„± - ë¬¸ì„œ êµ¬ì¡° ê¸°ë°˜
        # LLM ëª¨ë“œì¼ ë•ŒëŠ” í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (structureAnalysisë§Œ ì‚¬ìš©)
        kg_result = kg_builder.build_hierarchical_knowledge_graph(
            file_path=str(file_path_obj),
            document_text=document_text,
            keywords={} if use_llm else analysis_results.get("keywords", {}),
            metadata=parsing_results.get("file_info", {}),
            structure_analysis=structure_results,
            parsing_results=parsing_results,
            force_rebuild=force_rebuild,
            dataset_id=dataset_id  # dataset_id ì „ë‹¬
        )
        
        # LLM êµ¬ì¡° ë¶„ì„ ê²°ê³¼ë¥¼ KGì— í†µí•©
        if use_llm and structure_results.get("llm_analysis"):
            llm_analysis = structure_results["llm_analysis"]
            # structureAnalysisê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if llm_analysis.get("structureAnalysis") and len(llm_analysis["structureAnalysis"]) > 0:
                kg_result = _integrate_llm_structure_into_kg(kg_result, llm_analysis, str(file_path_obj), dataset_id)
            else:
                # structureAnalysisê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ ë¬¸ì„œ ì—”í‹°í‹°ë§Œ ìƒì„±
                import logging
                logger = logging.getLogger(__name__)
                logger.warning("âš ï¸ LLM structureAnalysisê°€ ë¹„ì–´ìˆì–´ ê¸°ë³¸ ë¬¸ì„œ ì—”í‹°í‹°ë§Œ ìƒì„±í•©ë‹ˆë‹¤.")
                
                # ê¸°ë³¸ ë¬¸ì„œ ì—”í‹°í‹° ìƒì„±
                import hashlib
                import os
                def _hash(s: str) -> str:
                    # None ë˜ëŠ” ë¹ˆ ê°’ ì²˜ë¦¬
                    if s is None:
                        s = ""
                    elif not isinstance(s, str):
                        s = str(s)
                    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()[:16]
                
                doc_info = llm_analysis.get("documentInfo", {})
                doc_entity_id = f"doc_{_hash(str(file_path_obj))}"
                
                # ê¸°ë³¸ ë¬¸ì„œ ì—”í‹°í‹° properties ìƒì„±
                doc_properties = {
                    "title": doc_info.get("title", "") if doc_info else os.path.basename(str(file_path_obj)),
                    "document_type": doc_info.get("documentType", "") if doc_info else "document",
                    "file_path": str(file_path_obj),
                    "parsing_status": "failed",
                    "note": "PDF íŒŒì‹± ì‹¤íŒ¨ë¡œ êµ¬ì¡° ë¶„ì„ ë¶ˆê°€"
                }
                
                # dataset_idê°€ ìˆìœ¼ë©´ ì¶”ê°€
                if dataset_id:
                    doc_properties["dataset_id"] = dataset_id
                
                kg_result = {
                    "entities": [{
                        "id": doc_entity_id,
                        "type": "Document",
                        "properties": doc_properties
                    }],
                    "relationships": [],
                    "structural_hierarchy": []
                }
        elif use_llm and not structure_results.get("llm_analysis"):
            # LLM ë¶„ì„ ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš° - ê¸°ë³¸ KG ê²°ê³¼ë¡œ ê³„ì† ì§„í–‰ (500 ë°˜í™˜ ëŒ€ì‹  ê²½ê³ )
            import logging
            logger = logging.getLogger(__name__)
            logger.warning("âš ï¸ LLM êµ¬ì¡° ë¶„ì„ì´ ì‹¤íŒ¨í•˜ì—¬ ê¸°ë³¸ KG ê²°ê³¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.")
            # ìµœì†Œí•œì˜ ë©”íƒ€ë°ì´í„°ì— ì‹¤íŒ¨ ì •ë³´ ê¸°ë¡
            if isinstance(kg_result, dict):
                kg_result.setdefault("metadata", {})
                kg_result["metadata"]["llm_success"] = False
                if isinstance(structure_results, dict) and structure_results.get("llm_error"):
                    kg_result["metadata"]["llm_error"] = structure_results.get("llm_error")
        
        # LLM êµ¬ì¡° í†µí•© ì™„ë£Œ í›„ í–¥ìƒëœ KGë¥¼ Memgraphì— ì €ì¥
        if use_llm and isinstance(kg_result, dict):
            try:
                import logging
                logger = logging.getLogger(__name__)
                
                # í–¥ìƒëœ ì—”í‹°í‹° íƒ€ì… ëª©ë¡ í™•ì¸
                entity_types = set()
                for entity in kg_result.get('entities', []):
                    entity_types.add(entity.get('type', 'Unknown'))
                
                logger.info(f"ğŸ”„ LLM í–¥ìƒëœ KGë¥¼ Memgraphì— ì €ì¥ ì‹œë„ ì¤‘...")
                logger.info(f"ğŸ“Š ì´ ì—”í‹°í‹°: {len(kg_result.get('entities', []))}, ì´ ê´€ê³„: {len(kg_result.get('relationships', []))}")
                logger.info(f"ğŸ“ ì—”í‹°í‹° íƒ€ì… ëª©ë¡: {', '.join(sorted(entity_types))}")
                
                # DocumentSection ë“± LLM ì—”í‹°í‹° ê°œìˆ˜ í™•ì¸
                llm_entities = [e for e in kg_result.get('entities', []) if e.get('type') in ['DocumentSection', 'DocumentSubsection', 'Author', 'Topic', 'Statistic']]
                logger.info(f"ğŸ¯ LLM íŠ¹í™” ì—”í‹°í‹°: {len(llm_entities)}ê°œ")
                
                # memgraph_service = MemgraphService()
                
                if False and memgraph_service.is_connected():
                    # ê¸°ì¡´ ë°ì´í„° ì™„ì „íˆ ì‚­ì œí•˜ê³  ìƒˆë¡œ ì €ì¥
                    logger.info("ğŸ—‘ï¸ ê¸°ì¡´ Memgraph ë°ì´í„° ì‚­ì œ ë° í–¥ìƒëœ KG ì €ì¥ ì¤‘ (clear_existing=True)...")
                    success = memgraph_service.insert_kg_data(kg_result, clear_existing=True)
                    
                    if success:
                        if "metadata" not in kg_result:
                            kg_result["metadata"] = {}
                        kg_result["metadata"]["memgraph_enhanced_saved"] = True
                        kg_result["metadata"]["memgraph_enhanced_saved_at"] = datetime.now().isoformat()
                        kg_result["metadata"]["memgraph_enhanced_entities"] = len(kg_result.get("entities", []))
                        kg_result["metadata"]["memgraph_enhanced_relationships"] = len(kg_result.get("relationships", []))
                        logger.info(f"âœ… Memgraphì— í–¥ìƒëœ KG ì €ì¥ ì„±ê³µ! (ì—”í‹°í‹°: {len(kg_result.get('entities', []))}, ê´€ê³„: {len(kg_result.get('relationships', []))})")
                    else:
                        if "metadata" not in kg_result:
                            kg_result["metadata"] = {}
                        kg_result["metadata"]["memgraph_enhanced_saved"] = False
                        logger.warning("âš ï¸ Memgraphì— í–¥ìƒëœ KG ì €ì¥ ì‹¤íŒ¨")
                else:
                    if "metadata" not in kg_result:
                        kg_result["metadata"] = {}
                    kg_result["metadata"]["memgraph_enhanced_saved"] = False
                    kg_result["metadata"]["memgraph_error"] = "Connection failed"
                    logger.error("âŒ Memgraph ì—°ê²° ì‹¤íŒ¨")
            except Exception as e:
                import logging
                logger = logging.getLogger(__name__)
                logger.error(f"âŒ Memgraph í–¥ìƒëœ KG ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                if "metadata" not in kg_result:
                    kg_result["metadata"] = {}
                kg_result["metadata"]["memgraph_enhanced_saved"] = False
                kg_result["metadata"]["memgraph_enhanced_error"] = str(e)
        
        # ê³„ì¸µì  KG ê²°ê³¼ì— ì¶”ê°€ ì •ë³´ í¬í•¨
        kg_with_context = {
            "file_info": parsing_results["file_info"],
            "generation_timestamp": datetime.now().isoformat(),
            "source_parser": best_parser,
            "keywords_used": len(analysis_results.get("keywords", {})),
            "llm_structure_integrated": use_llm and "llm_analysis" in structure_results,
            "knowledge_graph": kg_result,
            "statistics": {
                "total_entities": len(kg_result.get("entities", [])),
                "total_relationships": len(kg_result.get("relationships", [])),
                "structural_elements": len(kg_result.get("structural_hierarchy", [])),
                "entity_types": {},
                "relationship_types": {}
            }
        }
        
        # ì—”í‹°í‹° íƒ€ì…ë³„ í†µê³„
        entity_types = {}
        for entity in kg_result.get("entities", []):
            entity_type = entity.get("type", "unknown")
            entity_types[entity_type] = entity_types.get(entity_type, 0) + 1
        kg_with_context["statistics"]["entity_types"] = entity_types
        
        # ê´€ê³„ íƒ€ì…ë³„ í†µê³„
        relationship_types = {}
        for rel in kg_result.get("relationships", []):
            rel_type = rel.get("type", "unknown")
            relationship_types[rel_type] = relationship_types.get(rel_type, 0) + 1
        kg_with_context["statistics"]["relationship_types"] = relationship_types
        
        # ê²°ê³¼ ì €ì¥
        output_dir.mkdir(exist_ok=True)
        with open(kg_result_path, 'w', encoding='utf-8') as f:
            json.dump(kg_with_context, f, ensure_ascii=False, indent=2)
        
        # ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
        saved_files = []
        
        # Knowledge Graph ê²°ê³¼ íŒŒì¼
        saved_files.append({
            "type": "knowledge_graph",
            "path": str(kg_result_path),
            "description": "ê³„ì¸µì  Knowledge Graph ê²°ê³¼"
        })
        
        # êµ¬ì¡° ë¶„ì„ ê²°ê³¼ íŒŒì¼
        structure_result_path = output_dir / ("llm_structure_analysis.json" if use_llm else "structure_analysis.json")
        if structure_result_path.exists():
            saved_files.append({
                "type": "structure_analysis",
                "path": str(structure_result_path),
                "description": "ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ê²°ê³¼"
            })
        
        # í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ íŒŒì¼
        analysis_result_path = output_dir / "keyword_analysis.json"
        if analysis_result_path.exists():
            saved_files.append({
                "type": "keyword_analysis",
                "path": str(analysis_result_path),
                "description": "í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼"
            })
        
        # íŒŒì‹± ê´€ë ¨ íŒŒì¼ë“¤
        if parsing_results.get("parsing_results"):
            for parser_name, parser_result in parsing_results["parsing_results"].items():
                if parser_result.get("success"):
                    parser_dir = output_dir / parser_name
                    
                    # í…ìŠ¤íŠ¸ íŒŒì¼
                    text_file = parser_dir / f"{parser_name}_text.txt"
                    if text_file.exists():
                        saved_files.append({
                            "type": "extracted_text",
                            "parser": parser_name,
                            "path": str(text_file),
                            "description": f"{parser_name} íŒŒì„œë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸"
                        })
                    
                    # ë©”íƒ€ë°ì´í„° íŒŒì¼
                    metadata_file = parser_dir / f"{parser_name}_metadata.json"
                    if metadata_file.exists():
                        saved_files.append({
                            "type": "metadata",
                            "parser": parser_name,
                            "path": str(metadata_file),
                            "description": f"{parser_name} íŒŒì„œ ë©”íƒ€ë°ì´í„°"
                        })
                    
                    # êµ¬ì¡° ì •ë³´ íŒŒì¼
                    structure_file = parser_dir / f"{parser_name}_structure.json"
                    if structure_file.exists():
                        saved_files.append({
                            "type": "parser_structure",
                            "parser": parser_name,
                            "path": str(structure_file),
                            "description": f"{parser_name} íŒŒì„œ êµ¬ì¡° ì •ë³´"
                        })
        
        # íŒŒì‹± ê²°ê³¼ ì¢…í•© íŒŒì¼
        parsing_result_path = parser_service.get_parsing_result_path(file_path_obj, directory_path)
        if parsing_result_path.exists():
            saved_files.append({
                "type": "parsing_summary",
                "path": str(parsing_result_path),
                "description": "íŒŒì‹± ê²°ê³¼ ì¢…í•©"
            })
        
        # Memgraph ì €ì¥ ìƒíƒœ ì •ë³´
        if kg_result.get("memgraph_saved"):
            saved_files.append({
                "type": "memgraph_database",
                "path": "memgraph://localhost:7687",
                "description": "Memgraph ê·¸ë˜í”„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ KG ë°ì´í„°"
            })
        
        # ì‘ë‹µì— íŒŒì¼ ê²½ë¡œ ì •ë³´ ì¶”ê°€
        kg_with_context["saved_files"] = saved_files
        kg_with_context["output_directory"] = str(output_dir)

        # API ì‘ë‹µìš© ë°ì´í„° ìƒì„± (saved_filesë§Œ í¬í•¨)
        api_response = {
            "saved_files": saved_files,
            "output_directory": str(output_dir),
            "generation_timestamp": datetime.now().isoformat(),
            "statistics": {
                "total_saved_files": len(saved_files),
                "file_types": {}
            }
        }

        # íŒŒì¼ íƒ€ì…ë³„ í†µê³„
        file_types = {}
        for file_info in saved_files:
            file_type = file_info.get("type", "unknown")
            file_types[file_type] = file_types.get(file_type, 0) + 1
        api_response["statistics"]["file_types"] = file_types

        # API ì‘ë‹µ ì €ì¥ (saved_files ì¤‘ì‹¬)
        with open(kg_response_path, 'w', encoding='utf-8') as f:
            json.dump(api_response, f, ensure_ascii=False, indent=2)

        return api_response
        
    except HTTPException:
        raise
    except Exception as e:
        from utils.error_handler import log_and_raise_http_exception, collect_context_info
        
        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìˆ˜ì§‘
        context = collect_context_info(locals())
        
        # ìƒì„¸í•œ ì˜¤ë¥˜ ë¡œê¹… ë° HTTPException ë°œìƒ
        log_and_raise_http_exception(
            e, 
            "Knowledge Graph ìƒì„±", 
            context=context,
            logger_name=__name__
        )


@router.get("/knowledge-graph")
async def get_knowledge_graph(
    file_path: str = Query(..., description="Knowledge Graphë¥¼ ì¡°íšŒí•  íŒŒì¼ ê²½ë¡œ"),
    force_reparse: bool = Query(False, description="ì¬íŒŒì‹± ì—¬ë¶€"),
    force_reanalyze: bool = Query(False, description="ì¬ë¶„ì„ ì—¬ë¶€"),
    force_rebuild: bool = Query(False, description="KG ì¬ìƒì„± ì—¬ë¶€"),
    use_llm: bool = Query(True, description="LLM ê¸°ë°˜ êµ¬ì¡° ë¶„ì„ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)"),
    directory: Optional[str] = Query(None, description="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬"),
    dataset_id: Optional[str] = Query(None, description="ë°ì´í„°ì…‹ ID (ì„ íƒì , ëª¨ë“  ë…¸ë“œì— dataset í”„ë¡œí¼í‹° ì¶”ê°€)"),
    db: Session = Depends(get_db)
):
    """
    GET ë°©ì‹ìœ¼ë¡œ Knowledge Graphë¥¼ ìƒì„±í•˜ê³  ì €ì¥ëœ íŒŒì¼ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    - ê¸°ì¡´ ê²°ê³¼ê°€ ìˆê³  force ì˜µì…˜ì´ ì—†ìœ¼ë©´ ì €ì¥ëœ ì‘ë‹µì„ ë°”ë¡œ ë°˜í™˜
    - ì‘ë‹µì€ saved_files ëª©ë¡ê³¼ í†µê³„ ì •ë³´ë§Œ í¬í•¨í•©ë‹ˆë‹¤
    - dataset_idê°€ ì œê³µë˜ë©´ ëª¨ë“  ë…¸ë“œì— dataset í”„ë¡œí¼í‹°ê°€ ì¶”ê°€ë©ë‹ˆë‹¤
    """
    request = {
        "file_path": file_path,
        "force_reparse": force_reparse,
        "force_reanalyze": force_reanalyze,
        "force_rebuild": force_rebuild,
        "use_llm": use_llm,
        "directory": directory,
        "dataset_id": dataset_id
    }
    
    return await generate_knowledge_graph(request, db)


@router.get("/config/root")
async def get_file_root(db: Session = Depends(get_db)):
    """
    í˜„ì¬ ì„¤ì •ëœ íŒŒì¼ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    analyzer = LocalFileAnalyzer(db)
    return {"file_root": analyzer.get_file_root()}


@router.get("/config/current-directory")
async def get_current_directory():
    """
    ë°±ì—”ë“œ ì„œë²„ì˜ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    import os
    from pathlib import Path
    
    current_dir = Path.cwd()
    parent_dir = current_dir.parent
    
    # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    files = []
    directories = []
    
    try:
        for item in current_dir.iterdir():
            item_info = {
                "name": item.name,
                "path": str(item),
                "size": item.stat().st_size if item.is_file() else None,
                "modified": item.stat().st_mtime,
                "is_hidden": item.name.startswith(".")
            }
            
            if item.is_file():
                item_info["extension"] = item.suffix.lower()
                files.append(item_info)
            elif item.is_dir():
                try:
                    # ë””ë ‰í† ë¦¬ ë‚´ í•­ëª© ê°œìˆ˜ ê³„ì‚°
                    item_count = len(list(item.iterdir()))
                    item_info["item_count"] = item_count
                except (PermissionError, OSError):
                    item_info["item_count"] = 0
                directories.append(item_info)
    except (PermissionError, OSError):
        pass  # ê¶Œí•œ ì˜¤ë¥˜ ì‹œ ë¹ˆ ëª©ë¡ ë°˜í™˜
    
    # ì´ë¦„ìˆœ ì •ë ¬
    files.sort(key=lambda x: x["name"].lower())
    directories.sort(key=lambda x: x["name"].lower())
    
    return {
        "current_directory": str(current_dir),
        "parent_directory": str(parent_dir),
        "relative_to_parent": "../",
        "working_directory_info": {
            "name": current_dir.name,
            "exists": current_dir.exists(),
            "is_directory": current_dir.is_dir()
        },
        "contents": {
            "directories": directories,
            "files": files,
            "total_directories": len(directories),
            "total_files": len(files)
        }
    }


@router.post("/config/change-directory")
async def change_directory(
    request: dict,
    db: Session = Depends(get_db)
):
    """
    í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤.
    """
    import os
    from pathlib import Path
    
    new_directory = request.get("directory")
    if not new_directory:
        raise HTTPException(status_code=400, detail="ë””ë ‰í† ë¦¬ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤")
    
    new_path = Path(new_directory)
    
    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    if not new_path.is_absolute():
        new_path = Path.cwd() / new_path
    
    # ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not new_path.exists():
        raise HTTPException(status_code=404, detail=f"ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {new_path}")
    
    if not new_path.is_dir():
        raise HTTPException(status_code=400, detail=f"ê²½ë¡œê°€ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {new_path}")
    
    try:
        # ë””ë ‰í† ë¦¬ ë³€ê²½
        old_directory = str(Path.cwd())
        os.chdir(new_path)
        new_current_directory = str(Path.cwd())
        
        return {
            "success": True,
            "message": "ë””ë ‰í† ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤",
            "old_directory": old_directory,
            "new_directory": new_current_directory
        }
        
    except PermissionError:
        raise HTTPException(status_code=403, detail=f"ë””ë ‰í† ë¦¬ì— ì ‘ê·¼í•  ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {new_path}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë””ë ‰í† ë¦¬ ë³€ê²½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.post("/config/change-directory-and-list") 
async def change_directory_and_list(
    request: dict,
    db: Session = Depends(get_db)
):
    """
    ë””ë ‰í† ë¦¬ë¥¼ ë³€ê²½í•˜ê³  í•´ë‹¹ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ë¨¼ì € ë””ë ‰í† ë¦¬ ë³€ê²½
    change_result = await change_directory(request, db)
    
    if change_result["success"]:
        # ë³€ê²½ëœ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        current_dir_info = await get_current_directory()
        
        return {
            **change_result,
            "contents": current_dir_info["contents"]
        }
    else:
        return change_result


@router.get("/config/extractors")
async def get_available_extractors(db: Session = Depends(get_db)):
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ì¶”ì¶œê¸° ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    from services.config_service import ConfigService
    
    default_extractors = ConfigService.get_json_config(
        db, "DEFAULT_EXTRACTORS", ["llm"]
    )
    
    # ì¶”ì¶œê¸°ë³„ í™œì„±í™” ìƒíƒœ í™•ì¸
    extractor_config = ConfigService.get_extractor_config(db)
    
    available_extractors = []
    extractor_status = {
        "keybert": extractor_config.get("keybert_enabled", True),
        "ner": extractor_config.get("ner_enabled", True),
        "konlpy": extractor_config.get("konlpy_enabled", True),
        "llm": extractor_config.get("llm_enabled", False),
        "metadata": extractor_config.get("metadata_enabled", True),
        "langextract": extractor_config.get("langextract_enabled", False)
    }
    
    for extractor in ["keybert", "ner", "konlpy", "llm", "metadata", "langextract"]:
        if extractor_status.get(extractor, False):
            available_extractors.append(extractor)
    
    return {
        "default_extractors": default_extractors,
        "available_extractors": available_extractors,
        "extractor_status": extractor_status
    }
