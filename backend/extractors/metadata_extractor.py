from typing import List, Dict, Any, Optional
from pathlib import Path
import re
import time
import json
import logging
from datetime import datetime
from .base import KeywordExtractor, Keyword
from utils.text_cleaner import TextCleaner
from utils.position_mapper import PositionMapper
from utils.debug_logger import get_debug_logger
from prompts.templates import get_prompt_template
from prompts.config import PromptConfig
from utils.llm_logger import log_prompt_and_response

# LangChain imports
try:
    from langchain_ollama import OllamaLLM
except ImportError:
    try:
        from langchain_community.llms import Ollama as OllamaLLM
    except ImportError:
        OllamaLLM = None


class MetadataExtractor(KeywordExtractor):
    """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, db_session = None):
        super().__init__("metadata", config)
        self.is_loaded = True  # ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°ëŠ” í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
        
        # í”„ë¡¬í”„íŠ¸ ì„¤ì • ì´ˆê¸°í™”
        self.prompt_config = PromptConfig(config, db_session)
        
        # LangChain Ollama ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
        self.ollama_client = None
        
    def load_model(self) -> bool:
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°ëŠ” ë³„ë„ ëª¨ë¸ ë¡œë“œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."""
        self.is_loaded = True
        return True
    
    def extract(self, text: str, file_path: Optional[Path] = None) -> List[Keyword]:
        """ë¬¸ì„œì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ í‚¤ì›Œë“œë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        logger = logging.getLogger(__name__)
        debug_logger = get_debug_logger()
        start_time = time.time()
        
        logger.info(f"ğŸ” ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œì‘ - í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        # ë””ë²„ê·¸ ë¡œê¹…: ì¶”ì¶œ ì‹œì‘
        debug_logger.start_extraction(
            extractor_name="metadata",
            file_info={"filename": str(file_path) if file_path else "unknown", "id": None},
            text=text,
            config=self.config
        )
        
        # ìœ„ì¹˜ ë§¤í•‘ ìƒì„±
        position_mapper = PositionMapper()
        position_map = position_mapper.create_position_map(text, file_path)
        
        all_metadata_keywords = []
        
        try:
            # LLM ê¸°ë°˜ ë¬¸ì„œ ìš”ì•½ ë©”íƒ€ë°ì´í„° ì¶”ì¶œë§Œ ì‚¬ìš©
            if self.config.get("extract_summary", True):
                summary_keywords = self._extract_summary_metadata(text)
                all_metadata_keywords.extend(summary_keywords)
            else:
                logger.info("ğŸ“ LLM ê¸°ë°˜ ìš”ì•½ì´ ë¹„í™œì„±í™”ë˜ì–´ ë©”íƒ€ë°ì´í„° ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            
            # ë””ë²„ê·¸ ë¡œê¹…: ìµœì¢… ê²°ê³¼
            extraction_time = time.time() - start_time
            debug_logger.log_final_results(
                extractor_name="metadata",
                final_keywords=[{
                    "keyword": kw.text,
                    "score": kw.score,
                    "category": kw.category,
                    "start_position": kw.start_position,
                    "end_position": kw.end_position,
                    "context": kw.context_snippet
                } for kw in all_metadata_keywords],
                extraction_time=extraction_time,
                total_processing_time=time.time() - start_time
            )
            
            logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ - {len(all_metadata_keywords)}ê°œ ë©”íƒ€ë°ì´í„° í‚¤ì›Œë“œ, ì²˜ë¦¬ì‹œê°„: {extraction_time:.2f}ì´ˆ")
            return all_metadata_keywords
            
        except Exception as e:
            logger.error(f"âŒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
        finally:
            # ë””ë²„ê·¸ ì„¸ì…˜ ì €ì¥
            debug_logger.save_debug_session()
    
    def _extract_structure_metadata(self, text: str, position_mapper: PositionMapper, position_map: Dict[str, any]) -> List[Keyword]:
        """ë¬¸ì„œ êµ¬ì¡° ê´€ë ¨ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        logger = logging.getLogger(__name__)
        keywords = []
        
        # ì œëª© ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼)
        heading_patterns = [
            (r'^#{1}\s+(.+)$', 'title_h1', 1.0),          # # ì œëª©
            (r'^#{2}\s+(.+)$', 'title_h2', 0.9),          # ## ì œëª©
            (r'^#{3}\s+(.+)$', 'title_h3', 0.8),          # ### ì œëª©
            (r'^#{4,6}\s+(.+)$', 'title_h4_h6', 0.7),     # #### ~ ###### ì œëª©
        ]
        
        lines = text.split('\n')
        for line_idx, line in enumerate(lines):
            line = line.strip()
            
            for pattern, category, base_score in heading_patterns:
                matches = re.finditer(pattern, line, re.MULTILINE)
                for match in matches:
                    title_text = match.group(1).strip()
                    if len(title_text) >= 2:
                        # ì œëª© ìœ„ì¹˜ ì°¾ê¸°
                        char_pos = text.find(line)
                        if char_pos != -1:
                            page_number, line_number, column_number = position_mapper.get_position_info(char_pos, position_map)
                            
                            keywords.append(Keyword(
                                text=title_text,
                                score=base_score,
                                extractor=self.name,
                                category=category,
                                start_position=char_pos,
                                end_position=char_pos + len(title_text),
                                context_snippet=line,
                                page_number=page_number,
                                line_number=line_number,
                                column_number=column_number
                            ))
                            
                            logger.debug(f"ğŸ“‹ êµ¬ì¡° ë©”íƒ€ë°ì´í„° - {category}: '{title_text}'")
        
        # ëª©ë¡ êµ¬ì¡° ê°ì§€
        list_patterns = [
            (r'^\s*[-*+]\s+(.+)$', 'list_item', 0.5),      # ë¶ˆë¦¿ ëª©ë¡
            (r'^\s*\d+\.\s+(.+)$', 'numbered_item', 0.6),  # ë²ˆí˜¸ ëª©ë¡
        ]
        
        for line in lines:
            line = line.strip()
            for pattern, category, score in list_patterns:
                match = re.match(pattern, line)
                if match:
                    item_text = match.group(1).strip()
                    if len(item_text) >= 3:
                        # ìœ„ì¹˜ëŠ” ëŒ€ëµì ìœ¼ë¡œ ê³„ì‚°
                        char_pos = text.find(line)
                        if char_pos != -1:
                            keywords.append(Keyword(
                                text=item_text,
                                score=score,
                                extractor=self.name,
                                category=category,
                                start_position=char_pos,
                                end_position=char_pos + len(item_text),
                                context_snippet=line,
                                page_number=None,
                                line_number=None,
                                column_number=None
                            ))
        
        return keywords
    
    def _extract_statistical_metadata(self, text: str) -> List[Keyword]:
        """í†µê³„ì  ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        logger = logging.getLogger(__name__)
        keywords = []
        
        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        char_count = len(text)
        word_count = len(text.split())
        sentence_count = len(re.split(r'[.!?]+', text))
        paragraph_count = len([p for p in text.split('\n\n') if p.strip()])
        
        # í‰ê·  í†µê³„
        avg_words_per_sentence = word_count / max(sentence_count, 1)
        avg_chars_per_word = char_count / max(word_count, 1)
        
        # í†µê³„ë¥¼ ë©”íƒ€ë°ì´í„° í‚¤ì›Œë“œë¡œ ë³€í™˜
        statistical_data = [
            (self._categorize_length(char_count), "doc_length", 0.7),
            (self._categorize_word_count(word_count), "word_count", 0.6),
            (self._categorize_sentence_count(sentence_count), "sentence_count", 0.5),
            (self._categorize_paragraph_count(paragraph_count), "paragraph_count", 0.5),
            (self._categorize_sentence_length(avg_words_per_sentence), "sentence_length", 0.4),
            (self._categorize_complexity(avg_chars_per_word), "complexity", 0.6),
        ]
        
        for text_label, category, score in statistical_data:
            keywords.append(Keyword(
                text=text_label,
                score=score,
                extractor=self.name,
                category=category,
                start_position=None,
                end_position=None,
                context_snippet=f"í†µê³„: {text_label}",
                page_number=None,
                line_number=None,
                column_number=None
            ))
            
            logger.debug(f"ğŸ“Š í†µê³„ ë©”íƒ€ë°ì´í„°: {text_label}")
        
        return keywords
    
    def _extract_content_metadata(self, text: str, position_mapper: PositionMapper, position_map: Dict[str, any]) -> List[Keyword]:
        """ì½˜í…ì¸  ê´€ë ¨ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        logger = logging.getLogger(__name__)
        keywords = []
        
        # URL íŒ¨í„´ ì°¾ê¸°
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
        urls = re.finditer(url_pattern, text)
        
        for match in urls:
            url_text = match.group()
            # ë„ë©”ì¸ ì¶”ì¶œ
            domain_match = re.search(r'https?://([^/]+)', url_text)
            if domain_match:
                domain = domain_match.group(1)
                keywords.append(Keyword(
                    text=domain,
                    score=0.5,
                    extractor=self.name,
                    category="url_reference",
                    start_position=match.start(),
                    end_position=match.end(),
                    context_snippet=url_text,
                    page_number=None,
                    line_number=None,
                    column_number=None
                ))
        
        # ì´ë©”ì¼ íŒ¨í„´ ì°¾ê¸°
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.finditer(email_pattern, text)
        
        for match in emails:
            email_text = match.group()
            domain = email_text.split('@')[1]
            keywords.append(Keyword(
                text=domain,
                score=0.6,
                extractor=self.name,
                category="email_reference",
                start_position=match.start(),
                end_position=match.end(),
                context_snippet=email_text,
                page_number=None,
                line_number=None,
                column_number=None
            ))
        
        # ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
        date_patterns = [
            (r'\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼', 'date_korean'),
            (r'\d{4}-\d{2}-\d{2}', 'date_iso'),
            (r'\d{1,2}/\d{1,2}/\d{4}', 'date_us'),
            (r'\d{1,2}\.\d{1,2}\.\d{4}', 'date_eu'),
        ]
        
        for pattern, category in date_patterns:
            dates = re.finditer(pattern, text)
            for match in dates:
                date_text = match.group()
                keywords.append(Keyword(
                    text=date_text,
                    score=0.7,
                    extractor=self.name,
                    category=category,
                    start_position=match.start(),
                    end_position=match.end(),
                    context_snippet=date_text,
                    page_number=None,
                    line_number=None,
                    column_number=None
                ))
        
        # ìˆ«ì íŒ¨í„´ ë¶„ì„
        number_pattern = r'\b\d{1,3}(?:,\d{3})*(?:\.\d+)?\b'
        numbers = re.findall(number_pattern, text)
        
        if numbers:
            # ìˆ«ìì˜ ë¶„í¬ ë¶„ì„
            numeric_values = []
            for num_str in numbers:
                try:
                    numeric_values.append(float(num_str.replace(',', '')))
                except ValueError:
                    continue
            
            if numeric_values:
                avg_number = sum(numeric_values) / len(numeric_values)
                keywords.append(Keyword(
                    text=self._categorize_numbers(avg_number),
                    score=0.4,
                    extractor=self.name,
                    category="numeric_content",
                    start_position=None,
                    end_position=None,
                    context_snippet=f"í‰ê·  ìˆ˜ì¹˜: {avg_number:.2f}",
                    page_number=None,
                    line_number=None,
                    column_number=None
                ))
        
        logger.debug(f"ğŸ“„ ì½˜í…ì¸  ë©”íƒ€ë°ì´í„° {len(keywords)}ê°œ ì¶”ì¶œ")
        return keywords
    
    def _extract_summary_metadata(self, text: str) -> List[Keyword]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ìš”ì•½ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        logger = logging.getLogger(__name__)
        keywords = []
        
        try:
            # í…ìŠ¤íŠ¸ ì •ì œ
            clean_text = TextCleaner.clean_text(text)
            
            # ë¬¸ì„œê°€ ë„ˆë¬´ ì§§ì€ ê²½ìš° ìš”ì•½ ê±´ë„ˆëœ€
            if len(clean_text.strip()) < 100:
                logger.debug("ğŸ“ ë¬¸ì„œê°€ ë„ˆë¬´ ì§§ì•„ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return keywords
            
            # LLM ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            ollama_config = self._get_ollama_config()
            if not ollama_config['enabled']:
                logger.debug("ğŸ“ LLMì´ ë¹„í™œì„±í™”ë˜ì–´ ê·œì¹™ ê¸°ë°˜ ìš”ì•½ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return self._extract_rule_based_summary(clean_text)
            
            logger.info(f"ğŸ¤– LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ìš”ì•½ ìƒì„± ì¤‘... (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(clean_text)}ì)")
            
            # LLMì„ í†µí•œ ìš”ì•½ ìƒì„±
            summary_result = self._generate_llm_summary(clean_text, ollama_config)
            
            if summary_result:
                logger.info(f"âœ… LLM ìš”ì•½ ìƒì„± ì™„ë£Œ")
                keywords.extend(summary_result)
            else:
                logger.warning("âš ï¸ LLM ìš”ì•½ ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´")
                keywords.extend(self._extract_rule_based_summary(clean_text))
            
            return keywords
            
        except Exception as e:
            logger.error(f"âŒ ìš”ì•½ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê·œì¹™ ê¸°ë°˜ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´
            try:
                return self._extract_rule_based_summary(clean_text)
            except:
                return []
    
    def _get_ollama_config(self) -> Dict[str, any]:
        """Ollama LLM ì„¤ì •ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        # LLMì´ í™œì„±í™”ë˜ê³  LLM ìš”ì•½ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ LLM ì‚¬ìš©
        llm_enabled = self.config.get('llm_enabled', False)
        llm_summary_enabled = self.config.get('llm_summary', True)
        
        return {
            'enabled': llm_enabled and llm_summary_enabled,
            'base_url': self.config.get('ollama_base_url', 'http://localhost:11434'),
            'model': self.config.get('ollama_model', 'gemma3n:latest'),
            'timeout': self.config.get('ollama_timeout', 30)
        }
    
    def _generate_llm_summary(self, text: str, ollama_config: Dict[str, any]) -> List[Keyword]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        import requests
        import json
        logger = logging.getLogger(__name__)
        
        try:
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ì²­í‚¹
            max_chunk_size = 4000  # í† í° í•œê³„ ê³ ë ¤
            if len(text) > max_chunk_size:
                # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì²­í‚¹
                sentences = re.split(r'[.!?]+', text)
                chunks = []
                current_chunk = ""
                
                for sentence in sentences:
                    if len(current_chunk + sentence) > max_chunk_size and current_chunk:
                        chunks.append(current_chunk.strip())
                        current_chunk = sentence
                    else:
                        current_chunk += sentence + ". "
                        
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                
                # ê° ì²­í¬ë³„ë¡œ ìš”ì•½ ìƒì„± í›„ í†µí•©
                all_summaries = []
                for i, chunk in enumerate(chunks, 1):
                    logger.info(f"  ğŸ“„ ì²­í¬ {i}/{len(chunks)} ìš”ì•½ ìƒì„± ì¤‘...")
                    chunk_summary = self._call_llm_for_summary(chunk, ollama_config)
                    if chunk_summary:
                        all_summaries.extend(chunk_summary)
                
                return all_summaries
            else:
                return self._call_llm_for_summary(text, ollama_config)
                
        except Exception as e:
            logger.error(f"âŒ LLM ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _call_llm_for_summary(self, text: str, ollama_config: Dict[str, any]) -> List[Keyword]:
        """LangChainì„ ì‚¬ìš©í•˜ì—¬ LLM APIë¥¼ í˜¸ì¶œí•˜ê³  ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        logger = logging.getLogger(__name__)
        
        try:
            # LangChain Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í•„ìš”ì‹œ)
            if not self.ollama_client:
                try:
                    if not OllamaLLM:
                        raise ImportError("LangChain Ollama not available")
                    
                    self.ollama_client = OllamaLLM(
                        base_url=ollama_config['base_url'],
                        model=ollama_config['model'],
                        timeout=ollama_config['timeout']
                    )
                    logger.debug(f"âœ… LangChain Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
                except Exception as e:
                    logger.error(f"âŒ LangChain Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    # í´ë°±: ê¸°ì¡´ requests ë°©ì‹ ì‚¬ìš©
                    return self._call_llm_for_summary_fallback(text, ollama_config)
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
            try:
                template_name = self.prompt_config.get_template_name('document_summary')
                variables = self.prompt_config.get_template_variables('document_summary', text)
                prompt = get_prompt_template('document_summary', template_name, **variables)
                logger.debug(f"ğŸ¯ ë¬¸ì„œ ìš”ì•½ í…œí”Œë¦¿ ì‚¬ìš©: document_summary.{template_name}")
            except Exception as e:
                logger.warning(f"âš ï¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš© ì‹¤íŒ¨, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {e}")
                # í´ë°±: ê¸°ì¡´ ë°©ì‹
                prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ 5ê°€ì§€ ìœ í˜•ì˜ ìš”ì•½ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ê° ìš”ì•½ì€ ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{text}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "intro": "ë¬¸ì„œì˜ ë„ì…ë¶€ë‚˜ ì‹œì‘ ë¶€ë¶„ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
  "conclusion": "ë¬¸ì„œì˜ ê²°ë¡ ì´ë‚˜ ë§ˆë¬´ë¦¬ ë¶€ë¶„ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½", 
  "core": "ë¬¸ì„œì˜ ê°€ì¥ í•µì‹¬ì ì¸ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
  "topics": ["ì£¼ìš”", "í‚¤ì›Œë“œ", "ëª©ë¡", "5ê°œ", "ì´ë‚´"],
  "tone": "ë¬¸ì„œì˜ ì „ë°˜ì ì¸ í†¤ì´ë‚˜ ì„±ê²© (ì˜ˆ: ê³µì‹ì , í•™ìˆ ì , ê¸°ìˆ ì , ì„¤ëª…ì , ì •ë³´ì œê³µì )"
}}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:"""
        
            # LangChainì„ í†µí•´ LLM í˜¸ì¶œ
            logger.debug(f"ğŸš€ LangChain Ollama í˜¸ì¶œ - ëª¨ë¸: {ollama_config['model']}")
            
            start_time = time.time()
            response = self.ollama_client.invoke(prompt)
            call_duration = time.time() - start_time
            
            logger.debug(f"âœ… LangChain LLM ì‘ë‹µ ìˆ˜ì‹  - ê¸¸ì´: {len(response)}ì, ì†Œìš”ì‹œê°„: {call_duration:.2f}ì´ˆ")

            # í”„ë¡¬í”„íŠ¸/ì‘ë‹µ íŒŒì¼ ì €ì¥ ë° ë¡œê·¸ ê¸°ë¡
            log_prompt_and_response(
                label="document_summary",
                provider="ollama",
                model=ollama_config['model'],
                prompt=prompt,
                response=response,
                logger=logger,
                meta={
                    "base_url": ollama_config['base_url'],
                    "timeout": ollama_config['timeout'],
                    "langchain_version": True,
                    "call_duration": call_duration,
                },
            )

            # JSON ì‘ë‹µ íŒŒì‹±
            return self._parse_llm_summary_response(response)
                
        except Exception as e:
            logger.error(f"âŒ LangChain LLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ì¡´ requests ë°©ì‹ ì‚¬ìš©
            logger.info("ê¸°ì¡´ requests ë°©ì‹ìœ¼ë¡œ í´ë°± ì‹œë„...")
            return self._call_llm_for_summary_fallback(text, ollama_config)
    
    def _call_llm_for_summary_fallback(self, text: str, ollama_config: Dict[str, any]) -> List[Keyword]:
        """ê¸°ì¡´ requests ë°©ì‹ì˜ LLM API í˜¸ì¶œ (í´ë°±ìš©)."""
        import requests
        import json
        logger = logging.getLogger(__name__)
        
        try:
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
            try:
                template_name = self.prompt_config.get_template_name('document_summary')
                variables = self.prompt_config.get_template_variables('document_summary', text)
                prompt = get_prompt_template('document_summary', template_name, **variables)
                logger.debug(f"ğŸ¯ ë¬¸ì„œ ìš”ì•½ í…œí”Œë¦¿ ì‚¬ìš© (í´ë°±): document_summary.{template_name}")
            except Exception as e:
                logger.warning(f"âš ï¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš© ì‹¤íŒ¨, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {e}")
                # í´ë°±: ê¸°ì¡´ ë°©ì‹
                prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ 5ê°€ì§€ ìœ í˜•ì˜ ìš”ì•½ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ê° ìš”ì•½ì€ ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{text}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "intro": "ë¬¸ì„œì˜ ë„ì…ë¶€ë‚˜ ì‹œì‘ ë¶€ë¶„ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
  "conclusion": "ë¬¸ì„œì˜ ê²°ë¡ ì´ë‚˜ ë§ˆë¬´ë¦¬ ë¶€ë¶„ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½", 
  "core": "ë¬¸ì„œì˜ ê°€ì¥ í•µì‹¬ì ì¸ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
  "topics": ["ì£¼ìš”", "í‚¤ì›Œë“œ", "ëª©ë¡", "5ê°œ", "ì´ë‚´"],
  "tone": "ë¬¸ì„œì˜ ì „ë°˜ì ì¸ í†¤ì´ë‚˜ ì„±ê²© (ì˜ˆ: ê³µì‹ì , í•™ìˆ ì , ê¸°ìˆ ì , ì„¤ëª…ì , ì •ë³´ì œê³µì )"
}}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:"""
            
            # í”„ë¡¬í”„íŠ¸ ì„¤ì •ì—ì„œ LLM íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
            llm_params = self.prompt_config.get_llm_params('document_summary')
            
            payload = {
                "model": ollama_config['model'],
                "prompt": prompt,
                "stream": False,
                "options": llm_params
            }
            
            logger.debug(f"ğŸš€ Ollama API í´ë°± í˜¸ì¶œ - ëª¨ë¸: {ollama_config['model']}")
            
            response = requests.post(
                f"{ollama_config['base_url']}/api/generate",
                json=payload,
                timeout=ollama_config['timeout']
            )
            
            # í”„ë¡¬í”„íŠ¸/ì‘ë‹µ íŒŒì¼ ì €ì¥ ë° ë¡œê·¸ ê¸°ë¡ (ìƒíƒœ ì½”ë“œ ë¬´ê´€í•˜ê²Œ ì €ì¥)
            resp_text = ""
            if response.status_code == 200:
                resp_text = response.json().get("response", "")
            else:
                resp_text = response.text or ""

            log_prompt_and_response(
                label="document_summary",
                provider="ollama",
                model=ollama_config['model'],
                prompt=prompt,
                response=resp_text,
                logger=logger,
                meta={
                    "base_url": ollama_config['base_url'],
                    "status_code": response.status_code,
                    "timeout": ollama_config['timeout'],
                    "options": llm_params,
                    "fallback_mode": True,
                },
            )

            if response.status_code == 200:
                logger.debug(f"âœ… LLM í´ë°± ì‘ë‹µ ìˆ˜ì‹  - ê¸¸ì´: {len(resp_text)}ì")
                # JSON ì‘ë‹µ íŒŒì‹±
                return self._parse_llm_summary_response(resp_text)
            else:
                logger.error(f"âŒ Ollama API í´ë°± ì˜¤ë¥˜: {response.status_code}")
                return []
                
        except Exception as e:
            logger.error(f"âŒ LLM API í´ë°± í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_llm_summary_response(self, response: str) -> List[Keyword]:
        """LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ í‚¤ì›Œë“œ ëª©ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        logger = logging.getLogger(__name__)
        keywords = []
        
        try:
            # JSON ë¶€ë¶„ ì¶”ì¶œ
            json_text = self._extract_json_from_response(response)
            summary_data = json.loads(json_text)
            
            # ë„ì…ë¶€ ìš”ì•½
            if summary_data.get('intro'):
                intro_text = summary_data['intro'][:150]
                keywords.append(Keyword(
                    text=intro_text,
                    score=0.9,
                    extractor=self.name,
                    category="summary_intro",
                    start_position=None,
                    end_position=None,
                    context_snippet=intro_text,
                    page_number=None,
                    line_number=None,
                    column_number=None
                ))
            
            # ê²°ë¡ ë¶€ ìš”ì•½
            if summary_data.get('conclusion'):
                conclusion_text = summary_data['conclusion'][:150]
                keywords.append(Keyword(
                    text=conclusion_text,
                    score=0.9,
                    extractor=self.name,
                    category="summary_conclusion",
                    start_position=None,
                    end_position=None,
                    context_snippet=conclusion_text,
                    page_number=None,
                    line_number=None,
                    column_number=None
                ))
            
            # í•µì‹¬ ë‚´ìš©
            if summary_data.get('core'):
                core_text = summary_data['core'][:150]
                keywords.append(Keyword(
                    text=core_text,
                    score=1.0,
                    extractor=self.name,
                    category="summary_core",
                    start_position=None,
                    end_position=None,
                    context_snippet=core_text,
                    page_number=None,
                    line_number=None,
                    column_number=None
                ))
            
            # ì£¼ìš” í† í”½ë“¤
            if summary_data.get('topics') and isinstance(summary_data['topics'], list):
                for i, topic in enumerate(summary_data['topics'][:5], 1):
                    keywords.append(Keyword(
                        text=topic,
                        score=0.8,
                        extractor=self.name,
                        category="summary_topic",
                        start_position=None,
                        end_position=None,
                        context_snippet=f"ì£¼ìš” ì£¼ì œ #{i}: {topic}",
                        page_number=None,
                        line_number=None,
                        column_number=None
                    ))
            
            # ë¬¸ì„œ í†¤
            if summary_data.get('tone'):
                tone_text = summary_data['tone'][:50]
                keywords.append(Keyword(
                    text=tone_text,
                    score=0.7,
                    extractor=self.name,
                    category="summary_tone",
                    start_position=None,
                    end_position=None,
                    context_snippet=f"ë¬¸ì„œì˜ ì „ë°˜ì  ì„±ê²©: {tone_text}",
                    page_number=None,
                    line_number=None,
                    column_number=None
                ))
            
            logger.info(f"ğŸ“ LLM ìš”ì•½ íŒŒì‹± ì™„ë£Œ - {len(keywords)}ê°œ ìš”ì•½ í‚¤ì›Œë“œ ìƒì„±")
            return keywords
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            logger.error(f"ì‘ë‹µ ë‚´ìš©: {response[:200]}...")
            return []
        except Exception as e:
            logger.error(f"âŒ LLM ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_json_from_response(self, response: str) -> str:
        """ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        import re
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        response = re.sub(r'```json\s*', '', response, flags=re.IGNORECASE)
        response = re.sub(r'```\s*', '', response)
        
        # JSON ê°ì²´ ì°¾ê¸°
        json_pattern = r'\{[^{}]*"intro"[^{}]*\}'
        match = re.search(json_pattern, response, re.DOTALL)
        if match:
            return match.group(0)
        
        # ë” ê´€ëŒ€í•œ JSON íŒ¨í„´
        broad_pattern = r'\{.*?\}' 
        match = re.search(broad_pattern, response, re.DOTALL)
        if match:
            return match.group(0)
        
        # JSON ë§ˆì»¤ ì´í›„ì˜ ë‚´ìš© ì°¾ê¸°
        json_markers = ['JSON:', '{']
        for marker in json_markers:
            if marker in response:
                json_part = response[response.find(marker):].strip()
                if marker != '{':
                    json_part = json_part[len(marker):].strip()
                
                # ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€ ì¶”ì¶œ
                start_idx = json_part.find('{')
                if start_idx != -1:
                    end_idx = json_part.rfind('}')
                    if end_idx != -1 and end_idx > start_idx:
                        return json_part[start_idx:end_idx + 1]
        
        # ì‘ë‹µ ì „ì²´ê°€ JSONì¼ ìˆ˜ë„ ìˆìŒ
        return response.strip()
    
    def _extract_rule_based_summary(self, text: str) -> List[Keyword]:
        """ê·œì¹™ ê¸°ë°˜ ë¬¸ì„œ ìš”ì•½ (LLM ëŒ€ì²´ìš©)."""
        logger = logging.getLogger(__name__)
        keywords = []
        
        try:
            sentences = re.split(r'[.!?]+', text)
            sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
            
            # ë¬¸ì„œê°€ ë„ˆë¬´ ì§§ì€ ê²½ìš°
            if len(sentences) < 3:
                return keywords
            
            # 1. ì²« ë²ˆì§¸ ë¬¸ì¥ (ë„ì…ë¶€)
            first_sentence = sentences[0][:100] + "..." if len(sentences[0]) > 100 else sentences[0]
            keywords.append(Keyword(
                text=first_sentence,
                score=0.8,
                extractor=self.name,
                category="summary_intro",
                start_position=None,
                end_position=None,
                context_snippet=first_sentence,
                page_number=None,
                line_number=None,
                column_number=None
            ))
            
            # 2. ë§ˆì§€ë§‰ ë¬¸ì¥ (ê²°ë¡ ë¶€)
            if len(sentences) > 1:
                last_sentence = sentences[-1][:100] + "..." if len(sentences[-1]) > 100 else sentences[-1]
                keywords.append(Keyword(
                    text=last_sentence,
                    score=0.8,
                    extractor=self.name,
                    category="summary_conclusion",
                    start_position=None,
                    end_position=None,
                    context_snippet=last_sentence,
                    page_number=None,
                    line_number=None,
                    column_number=None
                ))
            
            # 3. ê°€ì¥ ê¸´ ë¬¸ì¥ (í•µì‹¬ ë‚´ìš©)
            longest_sentence = max(sentences, key=len)
            if len(longest_sentence) > 50:
                core_content = longest_sentence[:120] + "..." if len(longest_sentence) > 120 else longest_sentence
                keywords.append(Keyword(
                    text=core_content,
                    score=0.9,
                    extractor=self.name,
                    category="summary_core",
                    start_position=None,
                    end_position=None,
                    context_snippet=core_content,
                    page_number=None,
                    line_number=None,
                    column_number=None
                ))
            
            # 4. ë¬¸ì„œ ì£¼ì œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)
            word_freq = {}
            for sentence in sentences[:10]:  # ì²˜ìŒ 10ê°œ ë¬¸ì¥ë§Œ ë¶„ì„
                words = re.findall(r'\b[ê°€-í£a-zA-Z]{2,}\b', sentence)
                for word in words:
                    if TextCleaner.is_meaningful_keyword(word):
                        word_freq[word] = word_freq.get(word, 0) + 1
            
            # ìƒìœ„ 3ê°œ ë¹ˆë„ í‚¤ì›Œë“œ
            top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:3]
            for word, freq in top_words:
                if freq >= 2:  # ìµœì†Œ 2ë²ˆ ì´ìƒ ë“±ì¥
                    keywords.append(Keyword(
                        text=word,
                        score=min(0.7, 0.4 + freq * 0.1),  # ë¹ˆë„ì— ë”°ë¥¸ ì ìˆ˜
                        extractor=self.name,
                        category="summary_topic",
                        start_position=None,
                        end_position=None,
                        context_snippet=f"ë¹ˆë„: {freq}íšŒ",
                        page_number=None,
                        line_number=None,
                        column_number=None
                    ))
            
            # 5. ë¬¸ì„œ ë¶„ìœ„ê¸°/í†¤ ë¶„ì„
            tone = self._analyze_document_tone(clean_text)
            if tone:
                keywords.append(Keyword(
                    text=tone,
                    score=0.6,
                    extractor=self.name,
                    category="summary_tone",
                    start_position=None,
                    end_position=None,
                    context_snippet=f"ë¶„ì„ëœ ë¬¸ì„œ ë¶„ìœ„ê¸°: {tone}",
                    page_number=None,
                    line_number=None,
                    column_number=None
                ))
            
            logger.debug(f"ğŸ“ ìš”ì•½ ë©”íƒ€ë°ì´í„° {len(keywords)}ê°œ ì¶”ì¶œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ìš”ì•½ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return keywords
    
    def _analyze_document_tone(self, text: str) -> str:
        """ë¬¸ì„œì˜ í†¤ê³¼ ë¶„ìœ„ê¸°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        # ê°ì • ë¶„ì„ì„ ìœ„í•œ í‚¤ì›Œë“œ íŒ¨í„´
        tone_patterns = {
            'í•™ìˆ ì ': [r'ì—°êµ¬', r'ë¶„ì„', r'ê²°ê³¼', r'ë°©ë²•ë¡ ', r'ì´ë¡ ', r'ê°€ì„¤', r'ì‹¤í—˜', r'ë°ì´í„°'],
            'ê³µì‹ì ': [r'ê³µì§€', r'ì•ˆë‚´', r'ê·œì •', r'ì§€ì¹¨', r'ì ˆì°¨', r'ì •ì±…', r'ê³„íš'],
            'ê¸°ìˆ ì ': [r'ì‹œìŠ¤í…œ', r'êµ¬í˜„', r'ê°œë°œ', r'ì½”ë“œ', r'ì•Œê³ ë¦¬ì¦˜', r'í”„ë¡œê·¸ë˜ë°', r'ì†Œí”„íŠ¸ì›¨ì–´'],
            'ì„¤ëª…ì ': [r'ë°©ë²•', r'ë‹¨ê³„', r'ê³¼ì •', r'ì ˆì°¨', r'ì˜ˆì‹œ', r'ì„¤ëª…', r'ê°€ì´ë“œ'],
            'ë¶„ì„ì ': [r'ë¹„êµ', r'ê²€í† ', r'í‰ê°€', r'ë¶„ì„', r'ê³ ì°°', r'ì¡°ì‚¬', r'ì—°êµ¬'],
            'ê¸ì •ì ': [r'ì„±ê³µ', r'í–¥ìƒ', r'ê°œì„ ', r'íš¨ê³¼', r'ë°œì „', r'ì¢‹ì€', r'ìš°ìˆ˜í•œ'],
            'ë¶€ì •ì ': [r'ë¬¸ì œ', r'ì‹¤íŒ¨', r'ì˜¤ë¥˜', r'ìœ„í—˜', r'ì–´ë ¤ì›€', r'ë¶€ì¡±', r'ë‚˜ìœ']
        }
        
        text_lower = text.lower()
        tone_scores = {}
        
        for tone, patterns in tone_patterns.items():
            score = 0
            for pattern in patterns:
                score += len(re.findall(pattern, text_lower))
            
            if score > 0:
                tone_scores[tone] = score
        
        if tone_scores:
            # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ í†¤ ë°˜í™˜
            dominant_tone = max(tone_scores, key=tone_scores.get)
            return dominant_tone
        
        return "ì¼ë°˜ì "
    
    def _extract_file_metadata(self, file_path: Path, text: str) -> List[Keyword]:
        """íŒŒì¼ ê´€ë ¨ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        logger = logging.getLogger(__name__)
        keywords = []
        
        # íŒŒì¼ í™•ì¥ì
        file_extension = file_path.suffix.lower().lstrip('.')
        if file_extension:
            keywords.append(Keyword(
                text=file_extension,
                score=0.8,
                extractor=self.name,
                category="file_format",
                start_position=None,
                end_position=None,
                context_snippet=f"íŒŒì¼ í˜•ì‹: {file_extension}",
                page_number=None,
                line_number=None,
                column_number=None
            ))
        
        # íŒŒì¼ëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        filename_without_ext = file_path.stem
        filename_keywords = self._extract_filename_keywords(filename_without_ext)
        keywords.extend(filename_keywords)
        
        # íŒŒì¼ í¬ê¸° ì¹´í…Œê³ ë¦¬ (í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ì¶”ì •)
        size_category = self._categorize_file_size(len(text))
        keywords.append(Keyword(
            text=size_category,
            score=0.5,
            extractor=self.name,
            category="file_size",
            start_position=None,
            end_position=None,
            context_snippet=f"íŒŒì¼ í¬ê¸° ì¹´í…Œê³ ë¦¬: {size_category}",
            page_number=None,
            line_number=None,
            column_number=None
        ))
        
        logger.debug(f"ğŸ“ íŒŒì¼ ë©”íƒ€ë°ì´í„° {len(keywords)}ê°œ ì¶”ì¶œ")
        return keywords
    
    def _extract_filename_keywords(self, filename: str) -> List[Keyword]:
        """íŒŒì¼ëª…ì—ì„œ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        keywords = []
        
        # íŒŒì¼ëª…ì„ ë‹¨ì–´ë¡œ ë¶„í•  (ì–¸ë”ìŠ¤ì½”ì–´, í•˜ì´í”ˆ, ê³µë°± ê¸°ì¤€)
        filename_words = re.split(r'[_\-\s]+', filename)
        
        for word in filename_words:
            word = word.strip()
            if len(word) >= 2 and TextCleaner.is_meaningful_keyword(word):
                keywords.append(Keyword(
                    text=word,
                    score=0.6,
                    extractor=self.name,
                    category="filename_keyword",
                    start_position=None,
                    end_position=None,
                    context_snippet=f"íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ: {word}",
                    page_number=None,
                    line_number=None,
                    column_number=None
                ))
        
        return keywords
    
    # ì¹´í…Œê³ ë¦¬í™” í—¬í¼ ë©”ì„œë“œë“¤
    def _categorize_length(self, char_count: int) -> str:
        if char_count < 1000:
            return "ë§¤ìš°ì§§ìŒ"
        elif char_count < 5000:
            return "ì§§ìŒ"
        elif char_count < 20000:
            return "ë³´í†µ"
        elif char_count < 50000:
            return "ê¹€"
        else:
            return "ë§¤ìš°ê¹€"
    
    def _categorize_word_count(self, word_count: int) -> str:
        if word_count < 100:
            return "ì†ŒëŸ‰"
        elif word_count < 500:
            return "ì ìŒ"
        elif word_count < 2000:
            return "ë³´í†µ"
        elif word_count < 5000:
            return "ë§ìŒ"
        else:
            return "ëŒ€ëŸ‰"
    
    def _categorize_sentence_count(self, sentence_count: int) -> str:
        if sentence_count < 10:
            return "ë‹¨ìˆœ"
        elif sentence_count < 50:
            return "ë³´í†µ"
        elif sentence_count < 200:
            return "ë³µì¡"
        else:
            return "ë§¤ìš°ë³µì¡"
    
    def _categorize_paragraph_count(self, paragraph_count: int) -> str:
        if paragraph_count < 5:
            return "ë‹¨ìˆœêµ¬ì¡°"
        elif paragraph_count < 20:
            return "ë³´í†µêµ¬ì¡°"
        else:
            return "ë³µì¡êµ¬ì¡°"
    
    def _categorize_sentence_length(self, avg_words: float) -> str:
        if avg_words < 8:
            return "ì§§ì€ë¬¸ì¥"
        elif avg_words < 15:
            return "ë³´í†µë¬¸ì¥"
        elif avg_words < 25:
            return "ê¸´ë¬¸ì¥"
        else:
            return "ë§¤ìš°ê¸´ë¬¸ì¥"
    
    def _categorize_complexity(self, avg_chars_per_word: float) -> str:
        if avg_chars_per_word < 4:
            return "ë‹¨ìˆœ"
        elif avg_chars_per_word < 6:
            return "ë³´í†µ"
        elif avg_chars_per_word < 8:
            return "ë³µì¡"
        else:
            return "ë§¤ìš°ë³µì¡"
    
    def _categorize_numbers(self, avg_number: float) -> str:
        if avg_number < 10:
            return "ì†Œìˆ˜"
        elif avg_number < 100:
            return "ì¤‘ê°„ìˆ˜"
        elif avg_number < 1000:
            return "í°ìˆ˜"
        else:
            return "ëŒ€í˜•ìˆ˜"
    
    def _categorize_file_size(self, text_length: int) -> str:
        if text_length < 5000:
            return "ì†Œí˜•"
        elif text_length < 50000:
            return "ì¤‘í˜•"
        elif text_length < 200000:
            return "ëŒ€í˜•"
        else:
            return "ì´ˆëŒ€í˜•"
