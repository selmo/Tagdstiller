"""
PDF ì´ë¯¸ì§€ ì¶”ì¶œ ë° ë©€í‹°ëª¨ë‹¬ LLM ë¶„ì„ ì„œë¹„ìŠ¤
"""

from __future__ import annotations

import base64
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import fitz  # PyMuPDF
import requests
from PIL import Image
import io

from services.config_service import ConfigService
from sqlalchemy.orm import Session

# OCR ê´€ë ¨ imports - ì„ íƒì  ì„í¬íŠ¸
try:
    import pytesseract
    import cv2
    import numpy as np
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    pytesseract = None
    cv2 = None
    np = None

logger = logging.getLogger(__name__)


class ImageAnalyzer:
    """PDF ì´ë¯¸ì§€ ì¶”ì¶œ ë° ë©€í‹°ëª¨ë‹¬ LLM ë¶„ì„"""

    def __init__(self, db: Session):
        self.db = db
        self.logger = logging.getLogger(__name__)

    def extract_full_text_from_scanned_pdf(self, file_path: Path, output_dir: Path) -> Dict[str, Any]:
        """ìŠ¤ìº”ëœ PDF ë¬¸ì„œì—ì„œ ì „ì²´ í˜ì´ì§€ OCRì„ í†µí•´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if not OCR_AVAILABLE:
            self.logger.warning("âš ï¸ OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ìŠ¤ìº” ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœ€")
            return {
                "success": False,
                "error": "OCR libraries not available",
                "extracted_text": "",
                "pages_processed": 0
            }

        try:
            output_dir.mkdir(parents=True, exist_ok=True)
            doc = fitz.open(str(file_path))

            full_text = []
            pages_processed = 0
            total_pages = len(doc)

            self.logger.info(f"ğŸ“„ ìŠ¤ìº” ë¬¸ì„œ OCR ì²˜ë¦¬ ì‹œì‘: {total_pages}í˜ì´ì§€")

            for page_num in range(total_pages):
                try:
                    # í˜ì´ì§€ë¥¼ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ë Œë”ë§
                    page = doc.load_page(page_num)

                    # í•´ìƒë„ ì„¤ì • (DPIê°€ ë†’ì„ìˆ˜ë¡ OCR ì •í™•ë„ í–¥ìƒ)
                    zoom = 2  # 2ë°° í™•ëŒ€
                    mat = fitz.Matrix(zoom, zoom)
                    pix = page.get_pixmap(matrix=mat)

                    # PIL Imageë¡œ ë³€í™˜
                    img_data = pix.tobytes("png")
                    pil_image = Image.open(io.BytesIO(img_data))

                    # NumPy ë°°ì—´ë¡œ ë³€í™˜
                    img_array = np.array(pil_image)

                    # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
                    if len(img_array.shape) == 3:
                        img_gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
                    else:
                        img_gray = img_array

                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (OCR ì„±ëŠ¥ í–¥ìƒ)
                    # 1. ë…¸ì´ì¦ˆ ì œê±°
                    denoised = cv2.medianBlur(img_gray, 3)

                    # 2. ëŒ€ë¹„ í–¥ìƒ
                    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
                    enhanced = clahe.apply(denoised)

                    # 3. ì´ì§„í™”
                    _, thresh = cv2.threshold(enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

                    # OCR ì‹¤í–‰ (í•œêµ­ì–´ + ì˜ì–´)
                    page_text = pytesseract.image_to_string(
                        thresh,
                        lang='kor+eng',
                        config='--oem 3 --psm 6'  # ì¢‹ì€ OCR ì„¤ì •
                    )

                    # í˜ì´ì§€ í…ìŠ¤íŠ¸ ì •ë¦¬
                    page_text = page_text.strip()
                    if page_text:
                        full_text.append(f"=== í˜ì´ì§€ {page_num + 1} ===\n{page_text}")
                        pages_processed += 1
                        self.logger.info(f"âœ… í˜ì´ì§€ {page_num + 1} OCR ì™„ë£Œ ({len(page_text)} ë¬¸ì)")
                    else:
                        self.logger.info(f"âšª í˜ì´ì§€ {page_num + 1} OCR ê²°ê³¼ ì—†ìŒ")

                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    pix = None

                except Exception as e:
                    self.logger.warning(f"âš ï¸ í˜ì´ì§€ {page_num + 1} OCR ì‹¤íŒ¨: {e}")
                    continue

            doc.close()

            # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
            extracted_text = "\n\n".join(full_text)

            # OCR ê²°ê³¼ íŒŒì¼ë¡œ ì €ì¥
            ocr_file_path = output_dir / "full_document_ocr.txt"
            ocr_file_path.write_text(extracted_text, encoding='utf-8')

            result = {
                "success": True,
                "extracted_text": extracted_text,
                "pages_processed": pages_processed,
                "total_pages": total_pages,
                "ocr_file_path": str(ocr_file_path),
                "text_length": len(extracted_text)
            }

            self.logger.info(f"ğŸ‰ ìŠ¤ìº” ë¬¸ì„œ OCR ì™„ë£Œ: {pages_processed}/{total_pages}í˜ì´ì§€, {len(extracted_text)}ë¬¸ì ì¶”ì¶œ")
            return result

        except Exception as e:
            self.logger.error(f"âŒ ìŠ¤ìº” ë¬¸ì„œ OCR ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "extracted_text": "",
                "pages_processed": 0
            }

    def extract_images_from_pdf(self, file_path: Path, output_dir: Path) -> List[Dict[str, Any]]:
        """PDFì—ì„œ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•˜ì—¬ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            output_dir.mkdir(parents=True, exist_ok=True)
            images_info = []

            doc = fitz.open(str(file_path))

            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                image_list = page.get_images(full=True)

                for img_index, img in enumerate(image_list):
                    try:
                        # ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
                        xref = img[0]
                        pix = fitz.Pixmap(doc, xref)

                        # ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ìŠ¤í‚µ (ì•„ì´ì½˜ ë“±)
                        if pix.width < 50 or pix.height < 50:
                            pix = None
                            continue

                        # ì´ë¯¸ì§€ íŒŒì¼ëª… ìƒì„±
                        image_filename = f"page_{page_num + 1}_img_{img_index + 1}.png"
                        image_path = output_dir / image_filename

                        # PNGë¡œ ì €ì¥
                        if pix.n - pix.alpha < 4:  # GRAY or RGB
                            pix.save(str(image_path))
                        else:  # CMYK: convert to RGB first
                            pix1 = fitz.Pixmap(fitz.csRGB, pix)
                            pix1.save(str(image_path))
                            pix1 = None

                        # ì´ë¯¸ì§€ ì •ë³´ ì €ì¥
                        image_info = {
                            "filename": image_filename,
                            "path": str(image_path),
                            "page": page_num + 1,
                            "width": pix.width,
                            "height": pix.height,
                            "size_bytes": image_path.stat().st_size if image_path.exists() else 0,
                            "format": "PNG"
                        }

                        images_info.append(image_info)
                        self.logger.info(f"ğŸ“¸ ì´ë¯¸ì§€ ì¶”ì¶œ: {image_filename} ({pix.width}x{pix.height})")

                        pix = None

                    except Exception as e:
                        self.logger.warning(f"âš ï¸ í˜ì´ì§€ {page_num + 1}, ì´ë¯¸ì§€ {img_index + 1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        continue

            doc.close()
            self.logger.info(f"âœ… ì´ {len(images_info)}ê°œ ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ")
            return images_info

        except Exception as e:
            self.logger.error(f"âŒ PDF ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def analyze_image_with_llm(self, image_path: Path, context: str = "", llm_config: Optional[Dict[str, Any]] = None, output_dir: Optional[Path] = None) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ë‚´ìš©ì— ë”°ë¥¸ ì°¨ë³„í™”ëœ ë¶„ì„ ìˆ˜í–‰"""
        if llm_config and llm_config.get("provider"):
            provider = llm_config["provider"]
        else:
            provider = ConfigService.get_config_value(self.db, "LLM_PROVIDER", "gemini")

        self.logger.info(f"ğŸ¤– ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: provider={provider}, ì´ë¯¸ì§€={image_path.name}")

        # 1. ì´ë¯¸ì§€ ì½˜í…ì¸  íƒ€ì… íŒë‹¨ (í…ìŠ¤íŠ¸ ì¤‘ì‹¬ vs ì‹œê°ì  ì½˜í…ì¸ )
        content_type_analysis = self._analyze_image_content_type(image_path)
        is_text_heavy = content_type_analysis["is_text_heavy"]

        self.logger.info(f"ğŸ“Š ì´ë¯¸ì§€ ì½˜í…ì¸  ë¶„ì„: {content_type_analysis['content_type']} "
                        f"(í…ìŠ¤íŠ¸ ë¹„ìœ¨: {content_type_analysis['text_ratio']:.2f}, "
                        f"í…ìŠ¤íŠ¸ ì¤‘ì‹¬: {is_text_heavy})")

        # 2. ì½˜í…ì¸  íƒ€ì…ì— ë”°ë¥¸ ì°¨ë³„í™”ëœ ì²˜ë¦¬
        if is_text_heavy:
            # í…ìŠ¤íŠ¸ ì¤‘ì‹¬ ì´ë¯¸ì§€ -> OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            result = self._extract_text_from_image(image_path, context)
            result["processing_method"] = "ocr_text_extraction"
        else:
            # ì‹œê°ì  ì½˜í…ì¸  -> ë©€í‹°ëª¨ë‹¬ LLMìœ¼ë¡œ ë‚´ìš© ë¶„ì„
            if provider == "gemini":
                result = self._analyze_with_gemini(image_path, context, llm_config, output_dir)
            elif provider == "openai":
                result = self._analyze_with_openai(image_path, context, llm_config)
            else:
                result = {"success": False, "error": f"ë©€í‹°ëª¨ë‹¬ ë¶„ì„ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” provider: {provider}"}
            result["processing_method"] = "multimodal_llm_analysis"

        # 3. ê³µí†µ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        result["content_type_analysis"] = content_type_analysis

        self.logger.info(f"ğŸ¯ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼: success={result.get('success', False)}, "
                        f"ì²˜ë¦¬ë°©ë²•={result.get('processing_method')}")
        return result

    def _analyze_with_gemini(self, image_path: Path, context: str, llm_config: Optional[Dict[str, Any]] = None, output_dir: Optional[Path] = None) -> Dict[str, Any]:
        """Gemini Visionìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ì„"""
        try:
            # LLM ì„¤ì • ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            if llm_config:
                conf = llm_config.copy()
                # ê¸°ë³¸ê°’ìœ¼ë¡œ DB ì„¤ì • ë³´ì™„
                db_conf = ConfigService.get_gemini_config(self.db)
                for key, value in db_conf.items():
                    if key not in conf:
                        conf[key] = value
            else:
                conf = ConfigService.get_gemini_config(self.db)

            api_key = conf.get("api_key")
            model = conf.get("model", "gemini-2.0-flash-exp")

            if not api_key:
                return {"error": "GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}

            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode('utf-8')

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt_text = self._get_image_analysis_prompt(context)

            # Gemini API í˜¸ì¶œìš© payload
            payload = {
                "contents": [{
                    "parts": [
                        {"text": prompt_text},
                        {
                            "inline_data": {
                                "mime_type": "image/png",
                                "data": image_data
                            }
                        }
                    ]
                }],
                "generationConfig": {
                    "temperature": 0.2,
                    "maxOutputTokens": 2000,
                    "responseMimeType": "application/json"
                }
            }

            # ì¬ì‹œë„ ë¡œì§ (503 ì—ëŸ¬ ëŒ€ì‘)
            import time
            max_retries = 3
            retry_delay = 2

            url = f"https://generativelanguage.googleapis.com/v1beta/{model}:generateContent?key={api_key}"

            for attempt in range(max_retries):
                try:
                    response = requests.post(url, json=payload, timeout=60)
                    response.raise_for_status()
                    break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
                except requests.exceptions.HTTPError as e:
                    if e.response.status_code == 503 and attempt < max_retries - 1:
                        self.logger.warning(f"âš ï¸ Gemini Vision API 503 ì—ëŸ¬, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})")
                        time.sleep(retry_delay)
                        retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    else:
                        raise  # ë§ˆì§€ë§‰ ì‹œë„ê±°ë‚˜ ë‹¤ë¥¸ ì—ëŸ¬ë©´ ì˜ˆì™¸ ë°œìƒ

            data = response.json()

            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ ë° ë¡œê¹…
            usage_metadata = data.get("usageMetadata", {})
            prompt_tokens = usage_metadata.get("promptTokenCount", 0)
            response_tokens = usage_metadata.get("candidatesTokenCount", 0)
            total_tokens = usage_metadata.get("totalTokenCount", 0)

            if usage_metadata:
                self.logger.info(
                    f"ğŸ“Š Gemini Vision í† í° ì‚¬ìš©ëŸ‰ - í”„ë¡¬í”„íŠ¸: {prompt_tokens}, ì‘ë‹µ: {response_tokens}, ì´í•©: {total_tokens}"
                )

            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_chunks = []
            for candidate in data.get("candidates", []):
                parts = candidate.get("content", {}).get("parts", [])
                for part in parts:
                    if isinstance(part, dict) and "text" in part:
                        text_chunks.append(part["text"])

            if not text_chunks:
                return {"error": "Gemini ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

            # JSON íŒŒì‹± ì‹œë„
            analysis_text = "".join(text_chunks).strip()

            # í”„ë¡¬í”„íŠ¸ ë° ì‘ë‹µ ë¡œê¹… (output_dirì´ ì œê³µëœ ê²½ìš°)
            if output_dir:
                from utils.llm_logger import log_prompt_and_response
                log_prompt_and_response(
                    label=f"image_analysis_{image_path.stem}",
                    provider="gemini",
                    model=model,
                    prompt=prompt_text,
                    response=analysis_text,
                    logger=self.logger,
                    base_dir=str(output_dir),
                    request_data=payload,
                    response_data=data,
                    meta={
                        "image_file": str(image_path),
                        "image_size_bytes": image_path.stat().st_size,
                        "context": context,
                        "tokens": {
                            "prompt_tokens": prompt_tokens,
                            "response_tokens": response_tokens,
                            "total_tokens": total_tokens
                        }
                    }
                )

            try:
                analysis_json = json.loads(analysis_text)
                return {
                    "success": True,
                    "analysis": analysis_json,
                    "raw_response": analysis_text
                }
            except json.JSONDecodeError:
                return {
                    "success": False,
                    "error": "LLM ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "raw_response": analysis_text
                }

        except Exception as e:
            self.logger.error(f"âŒ Gemini ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _analyze_with_openai(self, image_path: Path, context: str, llm_config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """OpenAI Visionìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ì„"""
        try:
            # LLM ì„¤ì • ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            if llm_config:
                conf = llm_config.copy()
                # ê¸°ë³¸ê°’ìœ¼ë¡œ DB ì„¤ì • ë³´ì™„
                db_conf = ConfigService.get_openai_config(self.db)
                for key, value in db_conf.items():
                    if key not in conf:
                        conf[key] = value
            else:
                conf = ConfigService.get_openai_config(self.db)

            api_key = conf.get("api_key")
            model = conf.get("model", "gpt-4-turbo")  # Vision ì§€ì› ëª¨ë¸ ì‚¬ìš©

            if not api_key:
                return {"error": "OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}

            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode('utf-8')

            # OpenAI API í˜¸ì¶œìš© payload
            payload = {
                "model": model,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": self._get_image_analysis_prompt(context)},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{image_data}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                "max_tokens": 2000,
                "temperature": 0.2
            }

            response = requests.post(
                f"{conf.get('base_url', 'https://api.openai.com/v1')}/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json=payload,
                timeout=60
            )
            response.raise_for_status()

            data = response.json()
            content = data["choices"][0]["message"]["content"]

            # JSON íŒŒì‹± ì‹œë„
            try:
                analysis_json = json.loads(content)
                return {
                    "success": True,
                    "analysis": analysis_json,
                    "raw_response": content
                }
            except json.JSONDecodeError:
                return {
                    "success": False,
                    "error": "LLM ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "raw_response": content
                }

        except Exception as e:
            self.logger.error(f"âŒ OpenAI ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def _get_image_analysis_prompt(self, context: str) -> str:
        """ì‹œê°ì  ì½˜í…ì¸  ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± (í…ìŠ¤íŠ¸ ì¶”ì¶œ ì œì™¸)"""
        prompt = """
**ì¤‘ìš”: ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.**

ì´ ì´ë¯¸ì§€ì˜ ì‹œê°ì  ë‚´ìš©ì„ ìƒì„¸íˆ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”.
(ì°¸ê³ : í…ìŠ¤íŠ¸ ì¶”ì¶œì€ ë³„ë„ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ, ì‹œê°ì  ìš”ì†Œì™€ ë‚´ìš© ë¶„ì„ì— ì§‘ì¤‘í•´ì£¼ì„¸ìš”)

ë¶„ì„í•´ì•¼ í•  ìš”ì†Œ:
1. ì´ë¯¸ì§€ íƒ€ì… (chart, diagram, photo, graph, flowchart, map, infographic ë“±)
2. ì‹œê°ì  êµ¬ì¡°ì™€ ë ˆì´ì•„ì›ƒ
3. ì£¼ìš” ì‹œê°ì  ê°ì²´ ë° ìš”ì†Œë“¤
4. ìƒ‰ìƒ, íŒ¨í„´, ìŠ¤íƒ€ì¼
5. ë°ì´í„° ì‹œê°í™” ìš”ì†Œ (ì°¨íŠ¸, ê·¸ë˜í”„, í‘œ ë“±)
6. ë¬¸ì„œì—ì„œì˜ ì—­í• ê³¼ ì¤‘ìš”ë„
7. ì‹œê°ì ìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” ì£¼ìš” ë©”ì‹œì§€ë‚˜ ê°œë…

JSON ì‘ë‹µ í˜•ì‹:
{
  "image_type": "ì°¨íŠ¸/ë‹¤ì´ì–´ê·¸ë¨/ì‚¬ì§„/ê·¸ë˜í”„/í”Œë¡œìš°ì°¨íŠ¸ ë“±",
  "visual_description": "ì´ë¯¸ì§€ì˜ ì‹œê°ì  ë‚´ìš©ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…",
  "visual_elements": {
    "colors": ["ì£¼ìš” ìƒ‰ìƒë“¤"],
    "shapes": ["ë„í˜•ì´ë‚˜ êµ¬ì¡°ì  ìš”ì†Œë“¤"],
    "layout": "ë ˆì´ì•„ì›ƒ ë° êµ¬ì„± ì„¤ëª…",
    "patterns": ["íŒ¨í„´ì´ë‚˜ ë°˜ë³µ ìš”ì†Œë“¤"]
  },
  "content_analysis": {
    "main_message": "ì´ë¯¸ì§€ê°€ ì „ë‹¬í•˜ëŠ” ì£¼ìš” ë©”ì‹œì§€",
    "key_objects": ["ì£¼ìš” ì‹œê°ì  ê°ì²´ë“¤"],
    "relationships": ["ê°ì²´ë“¤ ê°„ì˜ ê´€ê³„ë‚˜ íë¦„"],
    "significance": "ë¬¸ì„œì—ì„œì˜ ì—­í• ê³¼ ì¤‘ìš”ì„±"
  },
  "data_visualization": {
    "chart_type": "ì°¨íŠ¸ ìœ í˜• (í•´ë‹¹í•˜ëŠ” ê²½ìš°)",
    "data_insights": ["ë°ì´í„°ì—ì„œ ë„ì¶œí•  ìˆ˜ ìˆëŠ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸"],
    "trends": ["ë³´ì´ëŠ” íŠ¸ë Œë“œë‚˜ íŒ¨í„´"]
  },
  "keywords": ["ì´ë¯¸ì§€ì—ì„œ ì‹œê°ì ìœ¼ë¡œ ì‹ë³„í•  ìˆ˜ ìˆëŠ” ì£¼ìš” ê°œë…ë“¤"],
  "confidence_score": 0.0-1.0
}
"""

        if context.strip():
            prompt += f"\n\në¬¸ì„œ ë§¥ë½:\n{context}\n\nìœ„ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì´ë¯¸ì§€ì˜ ì‹œê°ì  ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."

        return prompt

    def analyze_document_with_images(self, file_path: Path, text_content: str, output_dir: Path, llm_config: Optional[Dict[str, Any]] = None, filter_config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ë¬¸ì„œ ì „ì²´ë¥¼ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ì—¬ ì¢…í•© ë¶„ì„"""
        try:
            # 1. PDF ë¬¸ì„œ íƒ€ì… íŒë‹¨ (í…ìŠ¤íŠ¸ ê¸°ë°˜ vs ì´ë¯¸ì§€ ê¸°ë°˜)
            doc_type_analysis = self._analyze_pdf_type(file_path, text_content)
            self.logger.info(f"ğŸ“„ PDF ë¬¸ì„œ íƒ€ì… ë¶„ì„: {doc_type_analysis['document_type']} "
                           f"(í…ìŠ¤íŠ¸ ë°€ë„: {doc_type_analysis['text_density']:.3f}, "
                           f"ì´ë¯¸ì§€ ë¹„ìœ¨: {doc_type_analysis['image_ratio']:.3f})")

            # 2. ì´ë¯¸ì§€ ì¶”ì¶œ
            images_info = self.extract_images_from_pdf(file_path, output_dir / "images")

            if not images_info:
                return {
                    "success": False,
                    "message": "ì¶”ì¶œëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤",
                    "images_count": 0,
                    "document_type_analysis": doc_type_analysis
                }

            # 2-1. ì´ë¯¸ì§€ í•„í„°ë§ (ì„¤ì •ì´ ìˆëŠ” ê²½ìš°)
            original_count = len(images_info)
            if filter_config:
                images_info = self._filter_images(images_info, filter_config)
                filtered_count = original_count - len(images_info)
                if filtered_count > 0:
                    self.logger.info(f"ğŸ” ì´ë¯¸ì§€ í•„í„°ë§: {original_count}ê°œ â†’ {len(images_info)}ê°œ ({filtered_count}ê°œ ì œì™¸)")

            # 2-2. ê° ì´ë¯¸ì§€ ë¶„ì„
            image_analyses = []
            for img_info in images_info:
                image_path = Path(img_info["path"])
                if image_path.exists():
                    # ì´ë¯¸ì§€ ì£¼ë³€ í…ìŠ¤íŠ¸ ë§¥ë½ ì œê³µ (í˜ì´ì§€ ì •ë³´ ê¸°ë°˜)
                    page_context = f"ë¬¸ì„œ: {file_path.name}, í˜ì´ì§€: {img_info['page']}"

                    self.logger.info(f"ğŸ” ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {img_info['filename']}")
                    analysis = self.analyze_image_with_llm(image_path, page_context, llm_config, output_dir)
                    analysis["image_info"] = img_info
                    image_analyses.append(analysis)

                    if analysis.get("success"):
                        self.logger.info(f"âœ… ì´ë¯¸ì§€ ë¶„ì„ ì„±ê³µ: {img_info['filename']}")
                    else:
                        error_msg = analysis.get("error", "Unknown error")
                        self.logger.error(f"âŒ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {img_info['filename']} - {error_msg}")

            # 3. ì¢…í•© ê²°ê³¼ ìƒì„±
            result = {
                "success": True,
                "document_path": str(file_path),
                "document_type_analysis": doc_type_analysis,
                "images_count": len(images_info),
                "successful_analyses": len([a for a in image_analyses if a.get("success")]),
                "images_analysis": image_analyses,
                "summary": self._generate_image_summary(image_analyses),
                "extracted_keywords": self._extract_keywords_from_images(image_analyses),
                "processing_strategy": self._get_processing_strategy(doc_type_analysis, image_analyses)
            }

            # 4. ê²°ê³¼ ì €ì¥
            result_file = output_dir / "image_analysis.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            self.logger.info(f"ğŸ“Š ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {len(images_info)}ê°œ ì¶”ì¶œ, {result['successful_analyses']}ê°œ ë¶„ì„ ì„±ê³µ")
            return result

        except Exception as e:
            self.logger.error(f"âŒ ë¬¸ì„œ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "images_count": 0
            }

    def _analyze_pdf_type(self, file_path: Path, text_content: str) -> Dict[str, Any]:
        """PDF ë¬¸ì„œ íƒ€ì… ë¶„ì„ (í…ìŠ¤íŠ¸ ê¸°ë°˜ vs ì´ë¯¸ì§€ ê¸°ë°˜)"""
        try:
            doc = fitz.open(str(file_path))

            total_pages = len(doc)
            total_text_chars = len(text_content.strip())
            total_images = 0
            pages_with_images = 0
            pages_with_text = 0

            # ê° í˜ì´ì§€ë³„ë¡œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ë¶„ì„
            for page_num in range(total_pages):
                page = doc.load_page(page_num)

                # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                page_text = page.get_text().strip()
                if len(page_text) > 50:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ê°€ ìˆëŠ” í˜ì´ì§€
                    pages_with_text += 1

                # í˜ì´ì§€ë³„ ì´ë¯¸ì§€ ê°œìˆ˜
                page_images = page.get_images(full=True)
                if page_images:
                    pages_with_images += 1
                    total_images += len(page_images)

            doc.close()

            # í…ìŠ¤íŠ¸ ë°€ë„ ê³„ì‚° (í˜ì´ì§€ë‹¹ í‰ê·  ë¬¸ì ìˆ˜)
            text_density = total_text_chars / total_pages if total_pages > 0 else 0

            # ì´ë¯¸ì§€ ë¹„ìœ¨ ê³„ì‚°
            image_ratio = pages_with_images / total_pages if total_pages > 0 else 0

            # ë¬¸ì„œ íƒ€ì… íŒë‹¨
            document_type = self._determine_document_type(text_density, image_ratio, pages_with_text, total_pages)

            return {
                "document_type": document_type,
                "text_density": text_density,
                "image_ratio": image_ratio,
                "total_pages": total_pages,
                "pages_with_text": pages_with_text,
                "pages_with_images": pages_with_images,
                "total_images": total_images,
                "total_text_chars": total_text_chars,
                "analysis": self._get_document_type_analysis(document_type)
            }

        except Exception as e:
            self.logger.error(f"âŒ PDF íƒ€ì… ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "document_type": "unknown",
                "error": str(e),
                "text_density": 0,
                "image_ratio": 0
            }

    def _determine_document_type(self, text_density: float, image_ratio: float, pages_with_text: int, total_pages: int) -> str:
        """ë¬¸ì„œ íƒ€ì… ê²°ì • ë¡œì§"""

        # í…ìŠ¤íŠ¸ê°€ ì „í˜€ ì—†ëŠ” ê²½ìš° -> ìˆœìˆ˜ ì´ë¯¸ì§€ ë¬¸ì„œ
        if text_density < 10 and pages_with_text == 0:
            return "pure_image"

        # í…ìŠ¤íŠ¸ê°€ ë§¤ìš° ì ê³  ì´ë¯¸ì§€ê°€ ë§ì€ ê²½ìš° -> ìŠ¤ìº” ë¬¸ì„œ
        elif text_density < 100 and image_ratio > 0.7:
            return "scanned_document"

        # í…ìŠ¤íŠ¸ê°€ ì ë‹¹í•˜ì§€ë§Œ ì´ë¯¸ì§€ ë¹„ìœ¨ì´ ë†’ì€ ê²½ìš° -> ì´ë¯¸ì§€ ì¤‘ì‹¬ ë¬¸ì„œ
        elif text_density < 300 and image_ratio > 0.5:
            return "image_dominant"

        # í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„í•˜ê³  ì´ë¯¸ì§€ë„ ìˆëŠ” ê²½ìš° -> í•˜ì´ë¸Œë¦¬ë“œ ë¬¸ì„œ
        elif text_density > 300 and image_ratio > 0.3:
            return "hybrid_document"

        # í…ìŠ¤íŠ¸ê°€ ë§ê³  ì´ë¯¸ì§€ê°€ ì ì€ ê²½ìš° -> í…ìŠ¤íŠ¸ ì¤‘ì‹¬ ë¬¸ì„œ
        elif text_density > 300 and image_ratio < 0.3:
            return "text_dominant"

        # ê¸°íƒ€ ê²½ìš°
        else:
            return "mixed_content"

    def _get_document_type_analysis(self, document_type: str) -> Dict[str, Any]:
        """ë¬¸ì„œ íƒ€ì…ë³„ ë¶„ì„ ì „ëµ ë°˜í™˜"""

        strategies = {
            "pure_image": {
                "description": "ìˆœìˆ˜ ì´ë¯¸ì§€ ë¬¸ì„œ (ìŠ¤ìº”ë³¸)",
                "primary_strategy": "ocr_first",
                "image_analysis_priority": "high",
                "text_extraction_method": "ocr_based",
                "recommended_approach": "ëª¨ë“  ì´ë¯¸ì§€ì— ëŒ€í•´ OCR ìˆ˜í–‰ í›„ ë©€í‹°ëª¨ë‹¬ LLM ë¶„ì„"
            },
            "scanned_document": {
                "description": "ìŠ¤ìº”ëœ ë¬¸ì„œ",
                "primary_strategy": "ocr_enhanced",
                "image_analysis_priority": "high",
                "text_extraction_method": "ocr_based",
                "recommended_approach": "OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ ì´ë¯¸ì§€ êµ¬ì¡° ë¶„ì„"
            },
            "image_dominant": {
                "description": "ì´ë¯¸ì§€ ì¤‘ì‹¬ ë¬¸ì„œ",
                "primary_strategy": "image_first",
                "image_analysis_priority": "high",
                "text_extraction_method": "hybrid",
                "recommended_approach": "ì´ë¯¸ì§€ ë¶„ì„ì„ ìš°ì„ í•˜ê³  í…ìŠ¤íŠ¸ë¡œ ë³´ì™„"
            },
            "hybrid_document": {
                "description": "í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê· í˜• ë¬¸ì„œ",
                "primary_strategy": "balanced",
                "image_analysis_priority": "medium",
                "text_extraction_method": "native_plus_ocr",
                "recommended_approach": "í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ë¶„ì„ì„ ë™ë“±í•˜ê²Œ ìˆ˜í–‰"
            },
            "text_dominant": {
                "description": "í…ìŠ¤íŠ¸ ì¤‘ì‹¬ ë¬¸ì„œ",
                "primary_strategy": "text_first",
                "image_analysis_priority": "low",
                "text_extraction_method": "native",
                "recommended_approach": "í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ì‹¬, í•µì‹¬ ì´ë¯¸ì§€ë§Œ ì„ ë³„ ë¶„ì„"
            },
            "mixed_content": {
                "description": "í˜¼í•© ì½˜í…ì¸  ë¬¸ì„œ",
                "primary_strategy": "adaptive",
                "image_analysis_priority": "medium",
                "text_extraction_method": "hybrid",
                "recommended_approach": "í˜ì´ì§€ë³„ ì ì‘ì  ë¶„ì„"
            }
        }

        return strategies.get(document_type, {
            "description": "ì•Œ ìˆ˜ ì—†ëŠ” ë¬¸ì„œ íƒ€ì…",
            "primary_strategy": "default",
            "image_analysis_priority": "medium",
            "text_extraction_method": "native",
            "recommended_approach": "ê¸°ë³¸ ë¶„ì„ ìˆ˜í–‰"
        })

    def _filter_images(self, images_info: List[Dict[str, Any]], filter_config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì´ë¯¸ì§€ í•„í„°ë§ - ì‘ì€ ë¡œê³ /ì•„ì´ì½˜ ì œì™¸, ì¤‘ë³µ í¬ê¸° ì´ë¯¸ì§€ ì œê±°"""
        min_width = filter_config.get("min_width", 100)
        min_height = filter_config.get("min_height", 100)
        skip_duplicates = filter_config.get("skip_duplicates", False)

        filtered = []
        seen_sizes = set()  # ì¤‘ë³µ ê°ì§€ìš© (width, height)

        for img_info in images_info:
            width = img_info.get("width", 0)
            height = img_info.get("height", 0)

            # 1. ìµœì†Œ í¬ê¸° í•„í„°
            if width < min_width or height < min_height:
                self.logger.debug(f"â­ï¸ ì‘ì€ ì´ë¯¸ì§€ ì œì™¸: {img_info['filename']} ({width}x{height})")
                continue

            # 2. ì¤‘ë³µ í¬ê¸° í•„í„° (ê°™ì€ í¬ê¸° ì´ë¯¸ì§€ëŠ” ë°˜ë³µë˜ëŠ” ë¡œê³ /í—¤ë”ì¼ ê°€ëŠ¥ì„± ë†’ìŒ)
            if skip_duplicates:
                size_key = (width, height)
                if size_key in seen_sizes:
                    self.logger.debug(f"â­ï¸ ì¤‘ë³µ í¬ê¸° ì´ë¯¸ì§€ ì œì™¸: {img_info['filename']} ({width}x{height})")
                    continue
                seen_sizes.add(size_key)

            filtered.append(img_info)

        return filtered

    def _get_processing_strategy(self, doc_type_analysis: Dict[str, Any], image_analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë¬¸ì„œ íƒ€ì…ê³¼ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì²˜ë¦¬ ì „ëµ ì œì•ˆ"""

        document_type = doc_type_analysis.get("document_type", "unknown")
        analysis_strategy = doc_type_analysis.get("analysis", {})

        successful_analyses = len([a for a in image_analyses if a.get("success")])
        total_images = len(image_analyses)

        # ì´ë¯¸ì§€ ë¶„ì„ ì„±ê³µë¥ 
        success_rate = successful_analyses / total_images if total_images > 0 else 0

        # ì „ëµë³„ ê¶Œì¥ì‚¬í•­
        recommendations = []

        if document_type == "pure_image" or document_type == "scanned_document":
            recommendations.append("ğŸ“„ ìŠ¤ìº” ë¬¸ì„œë¡œ íŒë‹¨ë¨ - OCR ê¸°ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê¶Œì¥")
            recommendations.append("ğŸ–¼ï¸ ëª¨ë“  ì´ë¯¸ì§€ì— ëŒ€í•´ ë©€í‹°ëª¨ë‹¬ LLM ë¶„ì„ ìˆ˜í–‰")
            if success_rate < 0.8:
                recommendations.append("âš ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì„±ê³µë¥ ì´ ë‚®ìŒ - OCR ì „ìš© ë„êµ¬ ì¶”ê°€ ê³ ë ¤")

        elif document_type == "image_dominant":
            recommendations.append("ğŸ¨ ì´ë¯¸ì§€ ì¤‘ì‹¬ ë¬¸ì„œ - ì‹œê°ì  ì •ë³´ ìš°ì„  ë¶„ì„")
            recommendations.append("ğŸ“Š ì°¨íŠ¸, ë‹¤ì´ì–´ê·¸ë¨, í‘œ ë“±ì˜ êµ¬ì¡°ì  ìš”ì†Œ ì§‘ì¤‘ ë¶„ì„")

        elif document_type == "hybrid_document":
            recommendations.append("ğŸ”„ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê· í˜• ë¬¸ì„œ - í†µí•© ë¶„ì„ ìˆ˜í–‰")
            recommendations.append("ğŸ”— í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê°„ì˜ ìƒê´€ê´€ê³„ ë¶„ì„")

        elif document_type == "text_dominant":
            recommendations.append("ğŸ“ í…ìŠ¤íŠ¸ ì¤‘ì‹¬ ë¬¸ì„œ - í•µì‹¬ ì´ë¯¸ì§€ë§Œ ì„ ë³„ ë¶„ì„")
            recommendations.append("ğŸ¯ ë³´ì¡° ìë£Œë¡œì„œì˜ ì´ë¯¸ì§€ ì—­í•  ë¶„ì„")

        # ì„±ê³µë¥  ê¸°ë°˜ ì¶”ê°€ ê¶Œì¥ì‚¬í•­
        if success_rate > 0.9:
            recommendations.append("âœ… ì´ë¯¸ì§€ ë¶„ì„ ì„±ê³µë¥  ë§¤ìš° ë†’ìŒ - í˜„ì¬ ì„¤ì • ìœ ì§€ ê¶Œì¥")
        elif success_rate > 0.7:
            recommendations.append("âœ… ì´ë¯¸ì§€ ë¶„ì„ ì„±ê³µë¥  ì–‘í˜¸ - í˜„ì¬ ì„¤ì • ìœ ì§€")
        elif success_rate > 0.5:
            recommendations.append("âš ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì„±ê³µë¥  ë³´í†µ - LLM ëª¨ë¸ ë³€ê²½ ê³ ë ¤")
        else:
            recommendations.append("âŒ ì´ë¯¸ì§€ ë¶„ì„ ì„±ê³µë¥  ë‚®ìŒ - ì„¤ì • ê²€í†  í•„ìš”")

        return {
            "document_type": document_type,
            "primary_strategy": analysis_strategy.get("primary_strategy", "default"),
            "image_analysis_priority": analysis_strategy.get("image_analysis_priority", "medium"),
            "success_rate": success_rate,
            "recommendations": recommendations,
            "next_steps": self._get_next_steps(document_type, success_rate)
        }

    def _get_next_steps(self, document_type: str, success_rate: float) -> List[str]:
        """ë¬¸ì„œ íƒ€ì…ê³¼ ì„±ê³µë¥ ì— ë”°ë¥¸ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ"""

        next_steps = []

        if document_type in ["pure_image", "scanned_document"]:
            next_steps.extend([
                "1. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ì™„ì „í•œ ë¬¸ì„œ ì¬êµ¬ì„±",
                "2. OCR ì •í™•ë„ ê²€ì¦ ë° ìˆ˜ë™ ë³´ì • í•„ìš” ì—¬ë¶€ í™•ì¸",
                "3. êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜ (í…Œì´ë¸”, ëª©ë¡ ë“±)"
            ])

        elif document_type == "image_dominant":
            next_steps.extend([
                "1. ì‹œê°ì  ìš”ì†Œë“¤ ê°„ì˜ ê´€ê³„ ë¶„ì„ ë° í”Œë¡œìš° ì°¨íŠ¸ ìƒì„±",
                "2. ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ í‚¤ì›Œë“œì™€ í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ í†µí•©",
                "3. ì‹œê°ì  ì •ë³´ì˜ ê³„ì¸µ êµ¬ì¡° íŒŒì•…"
            ])

        elif document_type == "hybrid_document":
            next_steps.extend([
                "1. í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ê°„ ì°¸ì¡° ê´€ê³„ ë§¤í•‘",
                "2. ë¬¸ì„œì˜ ì „ì²´ì ì¸ ì •ë³´ íë¦„ ë¶„ì„",
                "3. í†µí•©ëœ ì§€ì‹ ê·¸ë˜í”„ ìƒì„±"
            ])

        else:  # text_dominant, mixed_content
            next_steps.extend([
                "1. í•µì‹¬ ì´ë¯¸ì§€ì™€ ê´€ë ¨ëœ í…ìŠ¤íŠ¸ ì„¹ì…˜ ì‹ë³„",
                "2. ì´ë¯¸ì§€ê°€ ë³´ì™„í•˜ëŠ” ì •ë³´ì˜ ì¤‘ìš”ë„ í‰ê°€",
                "3. í…ìŠ¤íŠ¸ ì¤‘ì‹¬ì˜ ìš”ì•½ì— ì‹œê°ì  ì •ë³´ í†µí•©"
            ])

        # ì„±ê³µë¥ ì´ ë‚®ì„ ë•Œì˜ ì¶”ê°€ ë‹¨ê³„
        if success_rate < 0.5:
            next_steps.insert(0, "0. ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨ ì›ì¸ ì¡°ì‚¬ ë° ëŒ€ì•ˆ ë°©ë²• ëª¨ìƒ‰")

        return next_steps

    def _generate_image_summary(self, analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ìš”ì•½ (OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì‹œê°ì  ë¶„ì„ í¬í•¨)"""
        successful = [a for a in analyses if a.get("success")]

        if not successful:
            return {"message": "ì„±ê³µí•œ ì´ë¯¸ì§€ ë¶„ì„ì´ ì—†ìŠµë‹ˆë‹¤"}

        # ì²˜ë¦¬ ë°©ë²•ë³„ í†µê³„
        processing_methods = {}
        image_types = {}
        all_keywords = []
        total_extracted_text_length = 0
        ocr_analyses = []
        visual_analyses = []

        for analysis in successful:
            # ì²˜ë¦¬ ë°©ë²• í†µê³„
            method = analysis.get("processing_method", "unknown")
            processing_methods[method] = processing_methods.get(method, 0) + 1

            # OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼
            if method == "ocr_text_extraction":
                ocr_analyses.append(analysis)
                extracted_text = analysis.get("extracted_text", "")
                total_extracted_text_length += len(extracted_text)

                # OCR í‚¤ì›Œë“œ
                if "text_analysis" in analysis:
                    ocr_keywords = analysis["text_analysis"].get("top_keywords", [])
                    all_keywords.extend(ocr_keywords)

            # ë©€í‹°ëª¨ë‹¬ LLM ì‹œê°ì  ë¶„ì„ ê²°ê³¼
            elif method == "multimodal_llm_analysis":
                visual_analyses.append(analysis)
                if "analysis" in analysis:
                    img_type = analysis["analysis"].get("image_type", "unknown")
                    image_types[img_type] = image_types.get(img_type, 0) + 1

                    keywords = analysis["analysis"].get("keywords", [])
                    all_keywords.extend(keywords)

        return {
            "total_analyzed": len(successful),
            "processing_summary": {
                "methods_used": processing_methods,
                "ocr_extractions": len(ocr_analyses),
                "visual_analyses": len(visual_analyses)
            },
            "ocr_results": {
                "total_text_length": total_extracted_text_length,
                "average_text_per_image": total_extracted_text_length / len(ocr_analyses) if ocr_analyses else 0
            },
            "visual_analysis": {
                "image_types": image_types,
                "average_confidence": sum(
                    a.get("analysis", {}).get("confidence_score", 0) for a in visual_analyses
                ) / len(visual_analyses) if visual_analyses else 0
            },
            "combined_keywords": list(set(all_keywords))[:30],  # ìƒìœ„ 30ê°œ
            "keyword_sources": {
                "from_ocr": len([kw for analysis in ocr_analyses for kw in analysis.get("text_analysis", {}).get("top_keywords", [])]),
                "from_visual": len([kw for analysis in visual_analyses for kw in analysis.get("analysis", {}).get("keywords", [])])
            }
        }

    def _analyze_image_content_type(self, image_path: Path) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ì˜ ì½˜í…ì¸  íƒ€ì… ë¶„ì„ (í…ìŠ¤íŠ¸ ì¤‘ì‹¬ vs ì‹œê°ì  ì½˜í…ì¸ )"""
        try:
            # OCR ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ visual ì²˜ë¦¬
            if not OCR_AVAILABLE:
                self.logger.warning("âš ï¸ OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì‹œê°ì  ì½˜í…ì¸ ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤")
                with Image.open(image_path) as img:
                    width, height = img.size
                return {
                    "content_type": "mostly_visual",
                    "is_text_heavy": False,
                    "text_length": 0,
                    "text_density": 0,
                    "text_ratio": 0,
                    "image_size": {"width": width, "height": height},
                    "extracted_text_preview": "",
                    "ocr_available": False
                }

            # OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œí•˜ì—¬ í…ìŠ¤íŠ¸ ë¹„ìœ¨ ì¸¡ì •
            text_content = self._extract_text_with_ocr(image_path)
            text_length = len(text_content.strip())

            # ì´ë¯¸ì§€ í¬ê¸° ì •ë³´ ì–»ê¸°
            with Image.open(image_path) as img:
                width, height = img.size
                total_pixels = width * height

            # í…ìŠ¤íŠ¸ ë°€ë„ ê³„ì‚° (í”½ì…€ë‹¹ í…ìŠ¤íŠ¸ ë¬¸ì ìˆ˜)
            text_density = text_length / total_pixels if total_pixels > 0 else 0

            # í…ìŠ¤íŠ¸ ë¹„ìœ¨ ì¶”ì • (ì„ì˜ ê¸°ì¤€ì  ì‚¬ìš©)
            text_ratio = min(text_length / 100, 1.0)  # 100ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”

            # í…ìŠ¤íŠ¸ ì¤‘ì‹¬ ì´ë¯¸ì§€ íŒë‹¨ ê¸°ì¤€
            is_text_heavy = (
                text_length > 50 and  # ìµœì†Œ 50ì ì´ìƒ
                (text_density > 0.001 or text_ratio > 0.3)  # ë°€ë„ê°€ ë†’ê±°ë‚˜ í…ìŠ¤íŠ¸ ë¹„ìœ¨ì´ ë†’ìŒ
            )

            # ì½˜í…ì¸  íƒ€ì… ê²°ì •
            if text_length == 0:
                content_type = "pure_visual"
            elif is_text_heavy:
                content_type = "text_dominant"
            elif text_length > 10:
                content_type = "mixed_content"
            else:
                content_type = "mostly_visual"

            return {
                "content_type": content_type,
                "is_text_heavy": is_text_heavy,
                "text_length": text_length,
                "text_density": text_density,
                "text_ratio": text_ratio,
                "image_size": {"width": width, "height": height},
                "extracted_text_preview": text_content[:100] + "..." if len(text_content) > 100 else text_content
            }

        except Exception as e:
            self.logger.error(f"âŒ ì´ë¯¸ì§€ ì½˜í…ì¸  íƒ€ì… ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "content_type": "unknown",
                "is_text_heavy": False,
                "error": str(e),
                "text_length": 0,
                "text_density": 0,
                "text_ratio": 0
            }

    def _extract_text_with_ocr(self, image_path: Path) -> str:
        """OCRì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if not OCR_AVAILABLE:
            self.logger.warning("âš ï¸ OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤")
            return ""

        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (OCR ì •í™•ë„ í–¥ìƒì„ ìœ„í•´)
            image = cv2.imread(str(image_path))
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

            # ë…¸ì´ì¦ˆ ì œê±° ë° ì´ë¯¸ì§€ í–¥ìƒ
            denoised = cv2.fastNlMeansDenoising(gray)

            # OCR ì‹¤í–‰ (í•œêµ­ì–´ + ì˜ì–´ ì§€ì›)
            custom_config = r'--oem 3 --psm 6 -l kor+eng'
            text = pytesseract.image_to_string(denoised, config=custom_config)

            return text.strip()

        except Exception as e:
            self.logger.warning(f"âš ï¸ OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # OCR ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ pytesseract ì‹œë„
            try:
                text = pytesseract.image_to_string(Image.open(image_path), lang='kor+eng')
                return text.strip()
            except Exception as e2:
                self.logger.error(f"âŒ ê¸°ë³¸ OCRë„ ì‹¤íŒ¨: {e2}")
                return ""

    def _extract_text_from_image(self, image_path: Path, context: str = "") -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì¤‘ì‹¬ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜"""
        try:
            if not OCR_AVAILABLE:
                return {
                    "success": False,
                    "error": "OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "extracted_text": "",
                    "ocr_available": False
                }

            # OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extracted_text = self._extract_text_with_ocr(image_path)

            if not extracted_text.strip():
                return {
                    "success": False,
                    "error": "ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "extracted_text": "",
                    "ocr_available": True
                }

            # í…ìŠ¤íŠ¸ ì •ë¦¬ ë° êµ¬ì¡°í™”
            cleaned_text = self._clean_extracted_text(extracted_text)

            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„ì„
            text_analysis = self._analyze_extracted_text(cleaned_text)

            result = {
                "success": True,
                "method": "ocr_extraction",
                "extracted_text": cleaned_text,
                "raw_text": extracted_text,
                "text_analysis": text_analysis,
                "context": context
            }

            self.logger.info(f"âœ… OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(cleaned_text)}ì ì¶”ì¶œ")
            return result

        except Exception as e:
            self.logger.error(f"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "extracted_text": ""
            }

    def _clean_extracted_text(self, text: str) -> str:
        """OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        import re
        cleaned = re.sub(r'\s+', ' ', text)

        # ì¤„ë°”ê¿ˆ ì •ë¦¬
        cleaned = re.sub(r'\n\s*\n', '\n\n', cleaned)

        # ì•ë’¤ ê³µë°± ì œê±°
        cleaned = cleaned.strip()

        return cleaned

    def _analyze_extracted_text(self, text: str) -> Dict[str, Any]:
        """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì˜ ê°„ë‹¨í•œ ë¶„ì„"""
        words = text.split()
        lines = text.split('\n')

        # ê¸°ë³¸ í†µê³„
        stats = {
            "total_characters": len(text),
            "total_words": len(words),
            "total_lines": len(lines),
            "avg_words_per_line": len(words) / len(lines) if lines else 0
        }

        # ì–¸ì–´ ê°ì§€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        korean_chars = len([c for c in text if 'ê°€' <= c <= 'í£'])
        english_chars = len([c for c in text if c.isalpha() and c.isascii()])

        if korean_chars > english_chars:
            primary_language = "korean"
        elif english_chars > 0:
            primary_language = "english"
        else:
            primary_language = "unknown"

        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)
        word_freq = {}
        for word in words:
            if len(word) > 2:  # 2ì ì´ìƒë§Œ
                word_freq[word] = word_freq.get(word, 0) + 1

        top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]

        return {
            "statistics": stats,
            "primary_language": primary_language,
            "language_distribution": {
                "korean_chars": korean_chars,
                "english_chars": english_chars
            },
            "top_keywords": [word for word, freq in top_keywords],
            "keyword_frequencies": dict(top_keywords)
        }

    def _extract_keywords_from_images(self, analyses: List[Dict[str, Any]]) -> List[str]:
        """ì´ë¯¸ì§€ ë¶„ì„ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (OCR ê²°ê³¼ í¬í•¨)"""
        all_keywords = []

        for analysis in analyses:
            if analysis.get("success"):
                # ë©€í‹°ëª¨ë‹¬ LLM ë¶„ì„ ê²°ê³¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                if "analysis" in analysis:
                    keywords = analysis["analysis"].get("keywords", [])
                    all_keywords.extend(keywords)

                # OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                if "text_analysis" in analysis:
                    ocr_keywords = analysis["text_analysis"].get("top_keywords", [])
                    all_keywords.extend(ocr_keywords)

        # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ ê¸°ë°˜ ì •ë ¬
        keyword_freq = {}
        for kw in all_keywords:
            keyword_freq[kw] = keyword_freq.get(kw, 0) + 1

        return sorted(keyword_freq.keys(), key=lambda x: keyword_freq[x], reverse=True)