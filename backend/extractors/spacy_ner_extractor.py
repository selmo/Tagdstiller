from typing import List, Dict, Any, Optional
from pathlib import Path
import time
from .base import KeywordExtractor, Keyword
from utils.text_cleaner import TextCleaner
from utils.position_mapper import PositionMapper
from utils.debug_logger import get_debug_logger

class SpaCyNERExtractor(KeywordExtractor):
    """spaCy NER ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("spacy_ner", config)
        self.nlp = None
        self.model_name = config.get('model', 'ko_core_news_sm') if config else 'ko_core_news_sm'
        self.auto_download = config.get('auto_download', True) if config else True
    
    def load_model(self) -> bool:
        """spaCy NER ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            import spacy
            import logging
            
            logger = logging.getLogger(__name__)
            logger.info(f"ğŸ“¦ spaCy NER ëª¨ë¸ '{self.model_name}' ë¡œë“œ ì‹œì‘...")
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í™•ì¸
            available_models = ['ko_core_news_sm', 'ko_core_news_md', 'ko_core_news_lg', 'en_core_web_sm', 'en_core_web_md', 'en_core_web_lg']
            
            # ìš”ì²­ëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ fallback ìˆœì„œë¡œ ì‹œë„
            models_to_try = [self.model_name]
            if self.model_name not in models_to_try:
                # í•œêµ­ì–´ ë¬¸ì„œë¼ë©´ í•œêµ­ì–´ ëª¨ë¸ ìš°ì„ 
                if 'ko' in self.model_name or 'korean' in self.model_name.lower():
                    models_to_try.extend(['ko_core_news_sm', 'ko_core_news_md', 'ko_core_news_lg'])
                else:
                    models_to_try.extend(['en_core_web_sm', 'en_core_web_md', 'en_core_web_lg'])
                    
                # ë§ˆì§€ë§‰ìœ¼ë¡œ í•œêµ­ì–´ ëª¨ë¸ë“¤ë„ ì‹œë„
                models_to_try.extend(['ko_core_news_sm', 'en_core_web_sm'])
            
            for model_name in models_to_try:
                try:
                    logger.info(f"  ğŸ”„ '{model_name}' ëª¨ë¸ ë¡œë“œ ì‹œë„...")
                    self.nlp = spacy.load(model_name)
                    self.actual_model_name = model_name
                    self.is_loaded = True
                    logger.info(f"âœ… spaCy NER ëª¨ë¸ '{model_name}' ë¡œë“œ ì„±ê³µ")
                    return True
                except OSError:
                    logger.warning(f"âš ï¸ ëª¨ë¸ '{model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ìë™ ë‹¤ìš´ë¡œë“œ ì‹œë„...")
                    
                    # ìë™ ë‹¤ìš´ë¡œë“œê°€ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ë‹¤ìš´ë¡œë“œ ì‹œë„
                    if self.auto_download and self._download_model(model_name, logger):
                        try:
                            logger.info(f"  ğŸ”„ ë‹¤ìš´ë¡œë“œí•œ '{model_name}' ëª¨ë¸ ë¡œë“œ ì‹œë„...")
                            self.nlp = spacy.load(model_name)
                            self.actual_model_name = model_name
                            self.is_loaded = True
                            logger.info(f"âœ… spaCy NER ëª¨ë¸ '{model_name}' ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ì„±ê³µ")
                            return True
                        except Exception as load_error:
                            logger.error(f"âŒ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ '{model_name}' ë¡œë“œ ì‹¤íŒ¨: {load_error}")
                            continue
                    else:
                        logger.warning(f"âš ï¸ ëª¨ë¸ '{model_name}' ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ë‹¤ìŒ ëª¨ë¸ ì‹œë„...")
                        continue
                except Exception as model_error:
                    logger.warning(f"âš ï¸ ëª¨ë¸ '{model_name}' ë¡œë“œ ì‹¤íŒ¨: {model_error}")
                    continue
            
            # ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨
            logger.error(f"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ spaCy ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸ í•„ìš”")
            logger.info(f"ğŸ’¡ ëª¨ë¸ ì„¤ì¹˜ ëª…ë ¹ì–´: python -m spacy download ko_core_news_sm")
            self.is_loaded = False
            return False
            
        except ImportError:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"âŒ spaCy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì„¤ì¹˜ í•„ìš”: pip install spacy")
            self.is_loaded = False
            return False
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"âŒ spaCy NER ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            return False
    
    def _download_model(self, model_name: str, logger) -> bool:
        """spaCy ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            import subprocess
            import sys
            
            logger.info(f"ğŸ“¥ spaCy ëª¨ë¸ '{model_name}' ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            
            # spacy download ëª…ë ¹ì–´ ì‹¤í–‰
            result = subprocess.run(
                [sys.executable, "-m", "spacy", "download", model_name],
                capture_output=True,
                text=True,
                timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            if result.returncode == 0:
                logger.info(f"âœ… spaCy ëª¨ë¸ '{model_name}' ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                return True
            else:
                logger.error(f"âŒ spaCy ëª¨ë¸ '{model_name}' ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.error(f"âŒ spaCy ëª¨ë¸ '{model_name}' ë‹¤ìš´ë¡œë“œ íƒ€ì„ì•„ì›ƒ (5ë¶„ ì´ˆê³¼)")
            return False
        except Exception as e:
            logger.error(f"âŒ spaCy ëª¨ë¸ '{model_name}' ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def is_available(self) -> bool:
        """spaCy NERëŠ” ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œì—ë„ íŒ¨í„´ ê¸°ë°˜ ì¶”ì¶œì´ ê°€ëŠ¥í•˜ë¯€ë¡œ í•­ìƒ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."""
        return True
    
    def extract(self, text: str, file_path: Optional[Path] = None) -> List[Keyword]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê°œì²´ëª…ì„ í‚¤ì›Œë“œë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        import logging
        logger = logging.getLogger(__name__)
        debug_logger = get_debug_logger()
        start_time = time.time()
        
        logger.info(f"ğŸ” spaCy NER í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘ - ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        # ë””ë²„ê·¸ ë¡œê¹…: ì¶”ì¶œ ì‹œì‘
        debug_logger.start_extraction(
            extractor_name="spacy_ner",
            file_info={"filename": str(file_path) if file_path else "unknown", "id": None},
            text=text,
            config=self.config
        )
        
        # ìœ„ì¹˜ ë§¤í•‘ ìƒì„±
        position_mapper = PositionMapper()
        position_map = position_mapper.create_position_map(text, file_path)
        logger.info(f"ğŸ“ ìœ„ì¹˜ ë§¤í•‘ ìƒì„± ì™„ë£Œ - ì´ {position_map['total_pages']}í˜ì´ì§€, {position_map['total_lines']}ì¤„")
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        original_text_copy = text  # ì›ë³¸ ë³´ê´€
        cleaned_text = TextCleaner.clean_text(text)
        logger.info(f"ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ - ì •ì œëœ ê¸¸ì´: {len(cleaned_text)} ë¬¸ì")
        
        # ë””ë²„ê·¸ ë¡œê¹…: ì „ì²˜ë¦¬ ê²°ê³¼
        debug_logger.log_preprocessing(
            extractor_name="spacy_ner",
            original_text=original_text_copy,
            preprocessed_text=cleaned_text,
            preprocessing_steps=["clean_text", "normalize_whitespace", "spacy_preprocessing"]
        )
        
        if not self.is_loaded:
            logger.info("ğŸ“¦ spaCy NER ëª¨ë¸ ë¡œë“œ ì‹œë„...")
            self.load_model()
        
        if not self.is_loaded:
            logger.warning("âš ï¸ spaCy NER ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, íŒ¨í„´ ê¸°ë°˜ ì¶”ì¶œë¡œ fallback")
            return self._extract_entities_simple(cleaned_text, text, position_mapper, position_map)
        
        try:
            logger.info(f"ğŸ¯ spaCy ëª¨ë¸ '{getattr(self, 'actual_model_name', self.model_name)}'ìœ¼ë¡œ NER ì¶”ì¶œ ì¤‘...")
            # ì‹¤ì œ spaCy NER ì‚¬ìš©
            entities = self._extract_entities_spacy(cleaned_text, text, position_mapper, position_map)
            
            # ìµœì¢… ê²°ê³¼ ë¡œê¹…
            extraction_time = time.time() - start_time
            debug_logger.log_final_results(
                extractor_name="spacy_ner",
                final_keywords=[{
                    "keyword": ent.text,
                    "score": getattr(ent, 'score', 1.0),
                    "category": ent.category,
                    "start_position": ent.start_position,
                    "end_position": ent.end_position,
                    "page_number": ent.page_number,
                    "line_number": ent.line_number,
                    "context": ent.context_snippet
                } for ent in entities],
                extraction_time=extraction_time,
                total_processing_time=time.time() - start_time
            )
            
            return entities
        except Exception as e:
            logger.error(f"âŒ spaCy NER ì¶”ì¶œ ì‹¤íŒ¨, íŒ¨í„´ ê¸°ë°˜ ì¶”ì¶œë¡œ fallback: {e}")
            fallback_entities = self._extract_entities_simple(cleaned_text, text, position_mapper, position_map)
            
            # í´ë°± ê²°ê³¼ ë¡œê¹…
            extraction_time = time.time() - start_time
            debug_logger.log_final_results(
                extractor_name="spacy_ner",
                final_keywords=[{
                    "keyword": ent.text,
                    "score": getattr(ent, 'score', 1.0),
                    "category": ent.category,
                    "start_position": ent.start_position,
                    "end_position": ent.end_position,
                    "context": ent.context_snippet
                } for ent in fallback_entities],
                extraction_time=extraction_time,
                total_processing_time=time.time() - start_time
            )
            
            return fallback_entities
        finally:
            # ë””ë²„ê·¸ ì„¸ì…˜ ì €ì¥
            debug_logger.save_debug_session()
    
    def _extract_entities_spacy(self, text: str, original_text: str, position_mapper: PositionMapper, position_map: Dict[str, any]) -> List[Keyword]:
        """ì‹¤ì œ spaCy NERì„ ì‚¬ìš©í•œ ê°œì²´ëª… ì¶”ì¶œ"""
        import logging
        import time
        logger = logging.getLogger(__name__)
        debug_logger = get_debug_logger()
        
        try:
            # NER ì„¤ì • ë¡œê¹…
            max_keywords = self.config.get('max_keywords', 15) if self.config else 15
            actual_model_name = getattr(self, 'actual_model_name', self.model_name)
            logger.info(f"âš™ï¸ spaCy NER ì„¤ì • - ëª¨ë¸: {actual_model_name}, ìµœëŒ€ê°œì²´ëª…: {max_keywords}")
            
            # ë””ë²„ê·¸ ë¡œê¹…: ëª¨ë¸ ì •ë³´
            debug_logger.log_embeddings(
                extractor_name="spacy_ner",
                model_name=actual_model_name
            )
            
            # spaCy ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘
            start_time = time.time()
            logger.info(f"ğŸ” spaCy ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘ (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì)...")
            
            doc = self.nlp(text)
            processing_time = time.time() - start_time
            logger.info(f"âš¡ spaCy ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {processing_time:.3f}ì´ˆ)")
            
            results = []
            
            # ë°œê²¬ëœ ê°œì²´ëª…ë“¤ ë¡œê¹…
            entities_found = []
            raw_entities = list(doc.ents)  # Convert to list for indexing
            
            for ent in raw_entities:
                entities_found.append(f"{ent.text}({ent.label_})")
            
            if entities_found:
                logger.info(f"ğŸ” spaCy NER ë°œê²¬ëœ ê°œì²´ëª… ({len(entities_found)}ê°œ): {', '.join(entities_found[:10])}{'...' if len(entities_found) > 10 else ''}")
                
                # ë””ë²„ê·¸ ë¡œê¹…: ë°œê²¬ëœ ê°œì²´ëª… í›„ë³´ë“¤
                entity_candidates = [ent.text for ent in raw_entities]
                debug_logger.log_candidate_generation(
                    extractor_name="spacy_ner",
                    candidates=entity_candidates,
                    generation_method="spacy_named_entity_recognition",
                    params={
                        "model": actual_model_name,
                        "processing_time": processing_time,
                        "total_entities_found": len(raw_entities)
                    }
                )
            else:
                logger.warning("âš ï¸ spaCy NERì—ì„œ ê°œì²´ëª…ì„ ì°¾ì§€ ëª»í•¨")
                debug_logger.log_candidate_generation(
                    extractor_name="spacy_ner",
                    candidates=[],
                    generation_method="spacy_named_entity_recognition",
                    params={"model": actual_model_name, "processing_time": processing_time}
                )
                return []
            
            # ê°œì²´ëª… ì²˜ë¦¬ ì‹œì‘
            logger.info(f"ğŸ”„ ê°œì²´ëª… í‚¤ì›Œë“œ ë³€í™˜ ì‹œì‘ ({len(raw_entities)}ê°œ ì²˜ë¦¬ ì˜ˆì •)...")
            
            # ê°œì²´ëª…ì„ í‚¤ì›Œë“œë¡œ ë³€í™˜ (ì§„í–‰ë¥  í‘œì‹œ)
            total_entities = len(raw_entities)
            processed_entities = 0
            valid_entities = 0
            
            for i, ent in enumerate(raw_entities, 1):
                processed_entities += 1
                
                # ì§„í–‰ë¥  ë¡œê¹… (10%ë§ˆë‹¤ ë˜ëŠ” ìƒìœ„ 3ê°œ)
                progress_percent = int((processed_entities / total_entities) * 100)
                if progress_percent % 25 == 0 or i <= 3:
                    logger.info(f"ğŸ“Š ê°œì²´ëª… ì²˜ë¦¬ ì§„í–‰ë¥ : {processed_entities}/{total_entities} ({progress_percent}%)")
                
                # ê°œì²´ëª… ìœ íš¨ì„± ê²€ì‚¬ (TextCleaner ì‚¬ìš©)
                entity_text = ent.text.strip()
                if not TextCleaner.is_meaningful_keyword(entity_text):
                    logger.debug(f"â© ê±´ë„ˆëœ€ (ìœ íš¨í•˜ì§€ ì•Šì€ í‚¤ì›Œë“œ): '{entity_text}'")
                    continue
                
                # ê°œì²´ëª… ì •ê·œí™”
                normalized_entity = TextCleaner.normalize_keyword(entity_text)
                if not normalized_entity:
                    logger.debug(f"â© ê±´ë„ˆëœ€ (ì •ê·œí™” ì‹¤íŒ¨): '{entity_text}'")
                    continue
                
                # ê°œì²´ëª… íƒ€ì…ë³„ ì‹ ë¢°ë„ ì„¤ì •
                confidence = self._get_entity_confidence(ent.label_)
                
                # ê°œì²´ëª… ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
                context = self._extract_context(text, ent.start_char, ent.end_char)
                
                # í˜ì´ì§€/ì¤„/ì»¬ëŸ¼ ë²ˆí˜¸ ê³„ì‚°
                page_number, line_number, column_number = position_mapper.get_position_info(ent.start_char, position_map)
                
                keyword = Keyword(
                    text=normalized_entity,
                    score=confidence,
                    extractor=self.name,
                    category=ent.label_,  # PERSON, ORG, LOC, MISC ë“±
                    start_position=ent.start_char,
                    end_position=ent.end_char,
                    context_snippet=context,
                    page_number=page_number,
                    line_number=line_number,
                    column_number=column_number
                )
                results.append(keyword)
                valid_entities += 1
                
                # ê°œë³„ ê°œì²´ëª… ì •ë³´ ë¡œê¹… (ìƒìœ„ 5ê°œë§Œ)
                if valid_entities <= 5:
                    logger.info(f"  ğŸ“ [{i}/{total_entities}] '{ent.text}' ({ent.label_}) - ìœ„ì¹˜: {ent.start_char}-{ent.end_char}, ì‹ ë¢°ë„: {confidence:.3f}")
            
            # í›„ì²˜ë¦¬ ë‹¨ê³„ ì‹œì‘
            logger.info(f"ğŸ”§ í›„ì²˜ë¦¬ ì‹œì‘ - ì¤‘ë³µ ì œê±° ë° ì •ë ¬ ({len(results)}ê°œ â†’ ìµœëŒ€ {max_keywords}ê°œ)")
            
            # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ìˆœ ì •ë ¬ (ì •ê·œí™”ëœ í‚¤ì›Œë“œ ê¸°ì¤€ìœ¼ë¡œ)
            unique_results = {}
            duplicates_removed = 0
            
            for result in results:
                # í‚¤ì›Œë“œë¥¼ ë‹¤ì‹œ í•œë²ˆ ì •ê·œí™”í•˜ì—¬ ì¤‘ë³µ ì²´í¬
                normalized_text = TextCleaner.normalize_keyword(result.text)
                key = (normalized_text.lower(), result.category)
                
                if key not in unique_results or unique_results[key].score < result.score:
                    if key in unique_results:
                        duplicates_removed += 1
                        logger.debug(f"ì¤‘ë³µ ì œê±°: '{unique_results[key].text}' -> '{result.text}' (ì •ê·œí™”: '{normalized_text}')")
                    
                    # ê²°ê³¼ì— ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ ì ìš©
                    result.text = normalized_text
                    unique_results[key] = result
            
            if duplicates_removed > 0:
                logger.info(f"ğŸ§¹ ì¤‘ë³µ ì œê±°: {duplicates_removed}ê°œ ê°œì²´ëª… ì œê±°")
            
            final_results = list(unique_results.values())
            final_results.sort(key=lambda x: x.score, reverse=True)
            
            # ìƒìœ„ Nê°œë§Œ ë°˜í™˜
            before_limit = len(final_results)
            final_results = final_results[:max_keywords]
            
            if before_limit > max_keywords:
                logger.info(f"ğŸ“Š ìƒìœ„ í‚¤ì›Œë“œ ì œí•œ: {before_limit}ê°œ â†’ {len(final_results)}ê°œ")
            
            # ê°œì²´ëª… íƒ€ì…ë³„ í†µê³„
            category_stats = {}
            for result in final_results:
                category = result.category
                if category not in category_stats:
                    category_stats[category] = 0
                category_stats[category] += 1
            
            # í†µê³„ ë¡œê¹…
            stats_text = [f"{cat}:{count}ê°œ" for cat, count in sorted(category_stats.items())]
            logger.info(f"ğŸ“ˆ ê°œì²´ëª… íƒ€ì…ë³„ ë¶„í¬: {', '.join(stats_text)}")
            
            # ìµœì¢… ê²°ê³¼ ë¡œê¹…
            if final_results:
                top_entities = [f"{kw.text}({kw.category},{kw.score:.3f})" for kw in final_results[:5]]
                logger.info(f"âœ… spaCy NER ì¶”ì¶œ ì™„ë£Œ - ì´ {len(final_results)}ê°œ ê°œì²´ëª… (ì²˜ë¦¬: {processed_entities}, ìœ íš¨: {valid_entities})")
                logger.info(f"ğŸ† ìƒìœ„ ê°œì²´ëª…: {', '.join(top_entities)}")
            else:
                logger.warning("âš ï¸ spaCy NER ì²˜ë¦¬ í›„ ìœ íš¨í•œ ê°œì²´ëª…ì´ ì—†ìŒ")
            
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ spaCy NER ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    def _extract_entities_simple(self, text: str, original_text: str, position_mapper: PositionMapper, position_map: Dict[str, any]) -> List[Keyword]:
        """ê°„ë‹¨í•œ ê°œì²´ëª… ì¶”ì¶œ (íŒ¨í„´ ê¸°ë°˜)"""
        import re
        import logging
        
        logger = logging.getLogger(__name__)
        logger.info(f"ğŸ”§ spaCy fallback - íŒ¨í„´ ê¸°ë°˜ ê°œì²´ëª… ì¶”ì¶œ ì‹œì‘ (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì)")
        logger.info(f"âš™ï¸ íŒ¨í„´ ê¸°ë°˜ ì„¤ì • - ìµœëŒ€ê°œì²´ëª…: {self.config.get('max_keywords', 10) if self.config else 10}")
        
        results = []
        patterns_applied = 0
        
        # íŒ¨í„´ 1: ìˆ«ì + ë…„ë„ íŒ¨í„´ (DATE)
        patterns_applied += 1
        logger.info(f"ğŸ” íŒ¨í„´ {patterns_applied}: ë…„ë„ íŒ¨í„´ ì ìš© ì¤‘...")
        year_pattern = r'\d{4}ë…„'
        years = list(re.finditer(year_pattern, text))
        year_count = 0
        for match in years:
            matched_text = match.group()
            if TextCleaner.is_meaningful_keyword(matched_text):
                normalized_text = TextCleaner.normalize_keyword(matched_text)
                if normalized_text:
                    context = self._extract_context(text, match.start(), match.end())
                    page_number, line_number, column_number = position_mapper.get_position_info(match.start(), position_map)
                    results.append(Keyword(
                        text=normalized_text,
                        score=0.8,
                        extractor=self.name,
                        category="DATE",
                        start_position=match.start(),
                        end_position=match.end(),
                        context_snippet=context,
                        page_number=page_number,
                        line_number=line_number,
                        column_number=column_number
                    ))
                    year_count += 1
        if year_count > 0:
            logger.info(f"  ğŸ“… ë…„ë„ íŒ¨í„´: {year_count}ê°œ ë°œê²¬")
        
        # íŒ¨í„´ 2: ìˆ«ì + ì› íŒ¨í„´ (MONEY)
        patterns_applied += 1
        logger.info(f"ğŸ” íŒ¨í„´ {patterns_applied}: ê¸ˆì•¡ íŒ¨í„´ ì ìš© ì¤‘...")
        money_pattern = r'\d+[ì–µë§Œì²œ]?ì›'
        money_matches = list(re.finditer(money_pattern, text))
        money_count = 0
        for match in money_matches:
            matched_text = match.group()
            if TextCleaner.is_meaningful_keyword(matched_text):
                normalized_text = TextCleaner.normalize_keyword(matched_text)
                if normalized_text:
                    context = self._extract_context(text, match.start(), match.end())
                    page_number, line_number, column_number = position_mapper.get_position_info(match.start(), position_map)
                    results.append(Keyword(
                        text=normalized_text,
                        score=0.85,
                        extractor=self.name,
                        category="MONEY",
                        start_position=match.start(),
                        end_position=match.end(),
                        context_snippet=context,
                        page_number=page_number,
                        line_number=line_number,
                        column_number=column_number
                    ))
                    money_count += 1
        if money_count > 0:
            logger.info(f"  ğŸ’° ê¸ˆì•¡ íŒ¨í„´: {money_count}ê°œ ë°œê²¬")
        
        # íŒ¨í„´ 3: í•œêµ­ì–´ íšŒì‚¬ëª… íŒ¨í„´ (ORG)
        patterns_applied += 1
        logger.info(f"ğŸ” íŒ¨í„´ {patterns_applied}: ê¸°ê´€ëª… íŒ¨í„´ ì ìš© ì¤‘...")
        org_pattern = r'[ê°€-í£]{2,}(?:ì „ì|ê·¸ë£¹|íšŒì‚¬|ê¸°ì—…|ì‚°ì—…|ì½”í¼ë ˆì´ì…˜|Corporation|Inc|Ltd)'
        orgs = list(re.finditer(org_pattern, text))
        org_count = 0
        for match in orgs:
            context = self._extract_context(text, match.start(), match.end())
            page_number, line_number, column_number = position_mapper.get_position_info(match.start(), position_map)
            results.append(Keyword(
                text=match.group(),
                score=0.75,
                extractor=self.name,
                category="ORG",
                start_position=match.start(),
                end_position=match.end(),
                context_snippet=context,
                page_number=page_number,
                line_number=line_number,
                column_number=column_number
            ))
            org_count += 1
        if org_count > 0:
            logger.info(f"  ğŸ¢ ê¸°ê´€ëª… íŒ¨í„´: {org_count}ê°œ ë°œê²¬")
        
        # íŒ¨í„´ 4: ì§€ì—­ëª… íŒ¨í„´ (LOC)
        patterns_applied += 1
        logger.info(f"ğŸ” íŒ¨í„´ {patterns_applied}: ì§€ì—­ëª… íŒ¨í„´ ì ìš© ì¤‘...")
        loc_pattern = r'[ê°€-í£]{2,}(?:ì‹œ|ë„|êµ¬|êµ°|ë™|ë©´|ì|íŠ¹ë³„ì‹œ|ê´‘ì—­ì‹œ)'
        locs = list(re.finditer(loc_pattern, text))
        loc_count = 0
        for match in locs:
            context = self._extract_context(text, match.start(), match.end())
            page_number, line_number, column_number = position_mapper.get_position_info(match.start(), position_map)
            results.append(Keyword(
                text=match.group(),
                score=0.7,
                extractor=self.name,
                category="LOC",
                start_position=match.start(),
                end_position=match.end(),
                context_snippet=context,
                page_number=page_number,
                line_number=line_number,
                column_number=column_number
            ))
            loc_count += 1
        if loc_count > 0:
            logger.info(f"  ğŸŒ ì§€ì—­ëª… íŒ¨í„´: {loc_count}ê°œ ë°œê²¬")
        
        # íŒ¨í„´ 5: ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ì˜ì–´ ê°œì²´ëª… (MISC)
        patterns_applied += 1
        logger.info(f"ğŸ” íŒ¨í„´ {patterns_applied}: ì˜ì–´ ê°œì²´ëª… íŒ¨í„´ ì ìš© ì¤‘...")
        english_entity_pattern = r'\b[A-Z][A-Za-z]{2,}\b'
        english_entities = list(re.finditer(english_entity_pattern, text))
        english_count = 0
        for match in english_entities:
            # ì¼ë°˜ì ì¸ ì˜ì–´ ë‹¨ì–´ëŠ” ì œì™¸
            word = match.group()
            if word.lower() not in {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'this', 'that', 'with', 'from', 'they', 'have', 'more', 'will', 'been', 'were', 'said', 'each', 'which', 'their', 'time', 'would', 'there', 'what', 'about', 'when', 'after', 'first', 'other', 'many', 'some', 'very', 'come', 'could', 'make', 'know', 'just', 'into', 'over', 'think', 'also', 'back', 'only', 'good', 'work', 'life', 'way', 'even', 'new', 'want', 'because', 'any', 'these', 'give', 'day', 'most', 'us'}:
                context = self._extract_context(text, match.start(), match.end())
                page_number, line_number, column_number = position_mapper.get_position_info(match.start(), position_map)
                results.append(Keyword(
                    text=word,
                    score=0.6,
                    extractor=self.name,
                    category="MISC",
                    start_position=match.start(),
                    end_position=match.end(),
                    context_snippet=context,
                    page_number=page_number,
                    line_number=line_number,
                    column_number=column_number
                ))
                english_count += 1
        if english_count > 0:
            logger.info(f"  ğŸ”¤ ì˜ì–´ ê°œì²´ëª… íŒ¨í„´: {english_count}ê°œ ë°œê²¬")
        
        logger.info(f"ğŸ“Š íŒ¨í„´ ë§¤ì¹­ ì™„ë£Œ - {patterns_applied}ê°œ íŒ¨í„´ ì ìš©, ì´ {len(results)}ê°œ í›„ë³´ ë°œê²¬")
        
        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ìˆœ ì •ë ¬ (ì •ê·œí™”ëœ í‚¤ì›Œë“œ ê¸°ì¤€ìœ¼ë¡œ)
        unique_results = {}
        duplicates_removed = 0
        
        for result in results:
            # í‚¤ì›Œë“œë¥¼ ì •ê·œí™”í•˜ì—¬ ì¤‘ë³µ ì²´í¬
            normalized_text = TextCleaner.normalize_keyword(result.text)
            key = (normalized_text.lower(), result.category)
            
            if key not in unique_results or unique_results[key].score < result.score:
                if key in unique_results:
                    duplicates_removed += 1
                    logger.debug(f"íŒ¨í„´ ê¸°ë°˜ ì¤‘ë³µ ì œê±°: '{unique_results[key].text}' -> '{result.text}' (ì •ê·œí™”: '{normalized_text}')")
                
                # ê²°ê³¼ì— ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ ì ìš©
                result.text = normalized_text
                unique_results[key] = result
        
        if duplicates_removed > 0:
            logger.info(f"ğŸ§¹ íŒ¨í„´ ê¸°ë°˜ ì¤‘ë³µ ì œê±°: {duplicates_removed}ê°œ ê°œì²´ëª… ì œê±°")
        
        final_results = list(unique_results.values())
        final_results.sort(key=lambda x: x.score, reverse=True)
        
        # ìƒìœ„ Nê°œë§Œ ë°˜í™˜
        max_keywords = self.config.get('max_keywords', 10)
        final_results = final_results[:max_keywords]
        
        # íŒ¨í„´ ê¸°ë°˜ ì¶”ì¶œ ê²°ê³¼ ë¡œê¹…
        if final_results:
            pattern_entities = [f"{kw.text}({kw.category},{kw.score:.3f})" for kw in final_results[:5]]
            logger.info(f"ğŸ”§ íŒ¨í„´ ê¸°ë°˜ ì¶”ì¶œ ì™„ë£Œ - {len(final_results)}ê°œ ê°œì²´ëª…, ìƒìœ„: {', '.join(pattern_entities)}")
        else:
            logger.warning("âš ï¸ íŒ¨í„´ ê¸°ë°˜ ì¶”ì¶œì—ì„œë„ ê°œì²´ëª…ì„ ì°¾ì§€ ëª»í•¨")
        
        return final_results
    
    def _find_entity_positions(self, text: str, entity: str) -> List[tuple]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê°œì²´ëª… ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        positions = []
        start = 0
        while True:
            pos = text.find(entity, start)
            if pos == -1:
                break
            positions.append((pos, pos + len(entity)))
            start = pos + 1
        return positions
    
    def _get_entity_confidence(self, label: str) -> float:
        """ê°œì²´ëª… íƒ€ì…ë³„ ì‹ ë¢°ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        confidence_map = {
            # ì˜ì–´ spaCy ëª¨ë¸ ë ˆì´ë¸”
            "PERSON": 0.9,
            "ORG": 0.85,
            "LOC": 0.8,
            "DATE": 0.75,
            "TIME": 0.7,
            "MONEY": 0.85,
            "PERCENT": 0.8,
            "MISC": 0.6,
            # í•œêµ­ì–´ spaCy ëª¨ë¸ ë ˆì´ë¸” (ko_core_news_sm)
            "PS": 0.85,  # Person (ì¸ëª…)
            "LC": 0.8,   # Location (ì§€ëª…)
            "OG": 0.85,  # Organization (ê¸°ê´€ëª…)
            "DT": 0.75,  # Date/Time (ë‚ ì§œ/ì‹œê°„)
            "TI": 0.7,   # Time (ì‹œê°„)
            "QT": 0.8,   # Quantity (ìˆ˜ëŸ‰)
            "CV": 0.75,  # Civilization (ë¬¸ëª…)
            "AM": 0.7,   # Animal (ë™ë¬¼)
            "PT": 0.7,   # Plant (ì‹ë¬¼)
            "MT": 0.7,   # Material (ë¬¼ì§ˆ)
            "TR": 0.7,   # Term (ìš©ì–´)
            "EV": 0.75,  # Event (ì‚¬ê±´)
            "AF": 0.7,   # Artifact (ì¸ê³µë¬¼)
            "FD": 0.75,  # Field (ë¶„ì•¼)
            "TM": 0.7    # Theory/Method (ì´ë¡ /ë°©ë²•)
        }
        return confidence_map.get(label, 0.65)
    
    def _extract_context(self, text: str, start_pos: int, end_pos: int, context_size: int = 50) -> str:
        """ê°œì²´ëª… ì£¼ë³€ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        context_start = max(0, start_pos - context_size)
        context_end = min(len(text), end_pos + context_size)
        
        context = text[context_start:context_end]
        
        # ì•ë’¤ì— ìƒëµ í‘œì‹œ ì¶”ê°€
        if context_start > 0:
            context = "..." + context
        if context_end < len(text):
            context = context + "..."
            
        return context