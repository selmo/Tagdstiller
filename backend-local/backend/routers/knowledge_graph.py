"""Knowledge Graph ìƒì„± ì—”ë“œí¬ì¸íŠ¸ - ì²­í¬ ê¸°ë°˜ ë¶„ì„ í†µí•©."""
from __future__ import annotations

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List

from fastapi import APIRouter, Depends, HTTPException, Query
from pydantic import BaseModel
from sqlalchemy.orm import Session

from dependencies import get_db
from services.document_parser_service import DocumentParserService
from services.local_file_analyzer import LocalFileAnalyzer
from services.chunk_analyzer import ChunkAnalyzer
from services.image_analyzer import ImageAnalyzer
from services.knowledge_graph_builder import KnowledgeGraphBuilder

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/local-analysis", tags=["local-analysis"])


class StructureAnalysisRequest(BaseModel):
    file_path: str
    directory: Optional[str] = None
    force_reparse: bool = False
    force_reanalyze: bool = False
    force_rebuild: bool = False
    llm: Optional[Dict[str, Any]] = None

    # ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì˜µì…˜ ì¶”ê°€
    use_chunking: bool = False  # ì²­í‚¹ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False, ëª…ì‹œì ìœ¼ë¡œ í™œì„±í™” í•„ìš”)
    chunk_threshold: int = 30000  # ì²­í‚¹ ë¬¸ì ìˆ˜ ì„ê³„ê°’ (ê¸°ë³¸ 30,000ì)
    max_chunk_size: int = 50000
    extractors: List[str] = ["KeyBERT", "spaCy NER", "LLM"]
    analysis_types: List[str] = ["keywords", "summary", "structure", "knowledge_graph"]

    # ì´ë¯¸ì§€ ë¶„ì„ ì˜µì…˜ ì¶”ê°€
    analyze_images: bool = False
    extract_images: bool = True

    # ë‹¤ë‹¨ê³„ ëŒ€í™” ì˜µì…˜ ì¶”ê°€ (í† í° ì œí•œ íšŒí”¼)
    use_multistep: bool = False  # ë‹¤ë‹¨ê³„ ëŒ€í™” ë°©ì‹ ì‚¬ìš© ì—¬ë¶€


def _ensure_absolute(path: Path) -> Path:
    return path if path.is_absolute() else (Path.cwd() / path).resolve()


def _validate_file(file_path: Path) -> None:
    if not file_path.exists():
        raise HTTPException(status_code=404, detail=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    if file_path.is_dir():
        raise HTTPException(status_code=400, detail="ë””ë ‰í† ë¦¬ê°€ ì•„ë‹Œ íŒŒì¼ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
    if file_path.stat().st_size == 0:
        raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
    supported_extensions = {".pdf", ".docx", ".doc", ".txt", ".md", ".html", ".xml", ".hwp"}
    if file_path.suffix.lower() not in supported_extensions:
        raise HTTPException(status_code=400, detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_path.suffix}")


@router.post("/knowledge-graph")
async def generate_knowledge_graph(req: StructureAnalysisRequest, db: Session = Depends(get_db)):
    file_path = _ensure_absolute(Path(req.file_path))
    _validate_file(file_path)

    directory_path = None
    if req.directory:
        directory_path = _ensure_absolute(Path(req.directory))
        directory_path.mkdir(parents=True, exist_ok=True)

    parser_service = DocumentParserService()
    analyzer = LocalFileAnalyzer(db)

    output_dir = parser_service.get_output_directory(file_path, directory_path)
    response_path = output_dir / "llm_structure_response.json"

    if not any([req.force_reparse, req.force_reanalyze, req.force_rebuild]) and response_path.exists():
        with response_path.open('r', encoding='utf-8') as f:
            return json.load(f)

    # 1. íŒŒì‹± ìˆ˜í–‰ (í•­ìƒ ìµœê³  í’ˆì§ˆ íŒŒì„œë¥¼ ì‚¬ìš©)
    try:
        if req.force_reparse or not parser_service.has_parsing_results(file_path, directory_path):
            parsing_results = parser_service.parse_document_comprehensive(
                file_path=file_path,
                force_reparse=req.force_reparse,
                directory=directory_path,
            )
        else:
            parsing_results = parser_service.load_existing_parsing_results(file_path, directory_path)
    except Exception as parsing_error:
        logger.error("âŒ ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨: {parsing_error}")

    # 2. ë¬¸ì„œ í¬ê¸° í™•ì¸ ë° ì²­í‚¹ ê²°ì •
    best_parser = parsing_results.get("summary", {}).get("best_parser")
    document_text = ""
    if best_parser and best_parser in parsing_results.get("parsing_results", {}):
        parser_dir = parser_service.get_output_directory(file_path, directory_path) / best_parser
        text_file = parser_dir / f"{best_parser}_text.txt"
        if text_file.exists():
            document_text = text_file.read_text(encoding='utf-8')

    # 3. ìŠ¤ìº” ë¬¸ì„œ ê°ì§€ ë° OCR ì²˜ë¦¬
    document_size = len(document_text)
    min_text_threshold = 500  # ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ ì„ê³„ê°’

    # ìŠ¤ìº”ëœ ë¬¸ì„œì¸ì§€ ê°ì§€ (í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì´ë¯¸ì§€ ê¸°ë°˜ ë¬¸ì„œë¡œ íŒë‹¨)
    is_scanned_document = document_size < min_text_threshold and file_path.suffix.lower() == ".pdf"

    if is_scanned_document and req.analyze_images:
        logger.info(f"ğŸ“¸ ìŠ¤ìº” ë¬¸ì„œ ê°ì§€ (í…ìŠ¤íŠ¸ {document_size}ì < {min_text_threshold}ì), OCR ì²˜ë¦¬ ì‹œì‘: {file_path}")

        try:
            image_analyzer = ImageAnalyzer(db)
            ocr_result = image_analyzer.extract_full_text_from_scanned_pdf(file_path, output_dir)

            if ocr_result.get("success") and ocr_result.get("extracted_text"):
                # OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¥¼ ê¸°ì¡´ í…ìŠ¤íŠ¸ì™€ ê²°í•©
                ocr_text = ocr_result["extracted_text"]
                document_text = f"{document_text}\n\n=== OCR ì¶”ì¶œ í…ìŠ¤íŠ¸ ===\n{ocr_text}"
                document_size = len(document_text)

                logger.info(f"âœ… OCR ì„±ê³µ: {ocr_result['pages_processed']}/{ocr_result['total_pages']}í˜ì´ì§€, {ocr_result['text_length']}ì ì¶”ì¶œ")
                logger.info(f"ğŸ“Š ì´ í…ìŠ¤íŠ¸ í¬ê¸°: {document_size:,}ì (ê¸°ì¡´ + OCR ê²°í•©)")

                # OCR ê²°ê³¼ë¥¼ íŒŒì‹± ê²°ê³¼ì— ì¶”ê°€
                if "ocr_results" not in parsing_results:
                    parsing_results["ocr_results"] = {}
                parsing_results["ocr_results"]["full_document_ocr"] = ocr_result

            else:
                logger.warning(f"âš ï¸ OCR ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ ì—†ìŒ: {ocr_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

        except Exception as ocr_error:
            logger.error(f"âŒ OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {ocr_error}", exc_info=True)
            # OCR ì‹¤íŒ¨í•´ë„ ê¸°ì¡´ í…ìŠ¤íŠ¸ë¡œ ê³„ì† ì§„í–‰

    # ì²­í‚¹ ì‚¬ìš© ì—¬ë¶€ ê²°ì • (ëª…ì‹œì  ìš”ì²­ ì‹œì—ë§Œ)
    if req.use_chunking:
        if document_size > req.chunk_threshold:
            logger.info(f"ğŸ§© ì²­í¬ ê¸°ë°˜ ë¶„ì„ ëª¨ë“œ (ë¬¸ì„œ í¬ê¸°: {document_size:,}ì > ì„ê³„ê°’: {req.chunk_threshold:,}ì)")
            return await _generate_chunk_based_knowledge_graph(req, db, file_path, directory_path)
        else:
            logger.info(f"â„¹ï¸ ì²­í‚¹ ìš”ì²­ë˜ì—ˆìœ¼ë‚˜ ë¬¸ì„œê°€ ì‘ì•„ ì „ì²´ ë¶„ì„ ì§„í–‰ (í¬ê¸°: {document_size:,}ì â‰¤ ì„ê³„ê°’: {req.chunk_threshold:,}ì)")

    # ê¸°ì¡´ ë°©ì‹: ì „ì²´ ë¬¸ì„œ ë¶„ì„
    logger.info(f"ğŸ“„ ì „ì²´ ë¬¸ì„œ ë¶„ì„ ëª¨ë“œë¡œ ì§€ì‹ ê·¸ë˜í”„ ìƒì„± (ë¬¸ì„œ í¬ê¸°: {document_size:,}ì)")

    # 2. LLM ê¸°ë°˜ êµ¬ì¡° ë¶„ì„ (ê¸°ì¡´ ê²°ê³¼ ì¬ì‚¬ìš© ê°€ëŠ¥)
    structure_result_path = output_dir / "llm_structure_analysis.json"
    llm_overrides = req.llm.copy() if req.llm else {}
    llm_overrides.setdefault("enabled", True)

    if req.force_reanalyze or req.force_rebuild or not structure_result_path.exists():
        structure_results = analyzer.analyze_document_structure_with_llm(
            text=document_text,
            file_path=str(file_path),
            file_extension=file_path.suffix.lower(),
            overrides=llm_overrides,
            use_multistep=req.use_multistep,  # ë‹¤ë‹¨ê³„ ëŒ€í™” ì˜µì…˜ ì „ë‹¬
        )

        structure_results["file_info"] = parsing_results["file_info"]
        structure_results["analysis_timestamp"] = datetime.now().isoformat()
        structure_results["source_parser"] = best_parser

        with structure_result_path.open('w', encoding='utf-8') as f:
            json.dump(structure_results, f, ensure_ascii=False, indent=2)
    else:
        with structure_result_path.open('r', encoding='utf-8') as f:
            structure_results = json.load(f)

    if not structure_results.get("llm_success"):
        raise HTTPException(
            status_code=502,
            detail={
                "message": structure_results.get("llm_error", "LLM êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨"),
                "raw_response": structure_results.get("llm_raw_response", ""),
                "analysis_file": str(structure_result_path),
            },
        )

    llm_analysis = structure_results.get("llm_analysis")
    if not llm_analysis or not llm_analysis.get("structureAnalysis"):
        raise HTTPException(
            status_code=502,
            detail={
                "message": "LLM êµ¬ì¡° ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤",
                "analysis_file": str(structure_result_path),
            },
        )

    # 3. ì´ë¯¸ì§€ ë¶„ì„ (PDF íŒŒì¼ì¸ ê²½ìš°, ì˜µì…˜ í™œì„±í™” ì‹œ)
    image_analysis_result = None
    if file_path.suffix.lower() == ".pdf" and (req.analyze_images or req.extract_images):
        image_analyzer = ImageAnalyzer(db)

        if req.analyze_images:
            logger.info(f"ğŸ–¼ï¸ PDF ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {file_path}")
            # LLM ì„¤ì • ì „ë‹¬
            llm_config = req.llm.copy() if req.llm else {}

            # ì´ë¯¸ì§€ í•„í„°ë§ ì„¤ì • ì¶”ê°€
            filter_config = {
                "min_width": 150,       # ìµœì†Œ ë„ˆë¹„ (ì‘ì€ ë¡œê³ /ì•„ì´ì½˜ ì œì™¸)
                "min_height": 150,      # ìµœì†Œ ë†’ì´
                "skip_duplicates": True # ì¤‘ë³µ ì´ë¯¸ì§€ ìŠ¤í‚µ (ê°™ì€ í¬ê¸° = ë¡œê³ ì¼ ê°€ëŠ¥ì„± ë†’ìŒ)
            }

            image_analysis_result = image_analyzer.analyze_document_with_images(
                file_path=file_path,
                text_content=document_text,
                output_dir=output_dir,
                llm_config=llm_config,
                filter_config=filter_config
            )
            logger.info(f"âœ… ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {image_analysis_result.get('images_count', 0)}ê°œ ì¶”ì¶œ, "
                       f"{image_analysis_result.get('successful_analyses', 0)}ê°œ ë¶„ì„ ì„±ê³µ")
        elif req.extract_images:
            # ì´ë¯¸ì§€ ì¶”ì¶œë§Œ (ë¶„ì„ ì—†ìŒ)
            logger.info(f"ğŸ“¸ PDF ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œì‘: {file_path}")
            images_info = image_analyzer.extract_images_from_pdf(
                file_path=file_path,
                output_dir=output_dir / "images"
            )
            image_analysis_result = {
                "success": True,
                "images_count": len(images_info),
                "extraction_only": True,
                "images_info": images_info
            }

    # 3. ê²°ê³¼ ì €ì¥ ë° ì‘ë‹µ êµ¬ì„±
    output_dir.mkdir(exist_ok=True)

    best_parser = parsing_results.get("summary", {}).get("best_parser")

    saved_files = []
    if structure_result_path.exists():
        saved_files.append({
            "type": "structure_analysis",
            "path": str(structure_result_path),
            "description": "LLM ê¸°ë°˜ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ê²°ê³¼",
        })

    parsing_result_path = parser_service.get_parsing_result_path(file_path, directory_path)
    if parsing_result_path.exists():
        saved_files.append({
            "type": "parsing_summary",
            "path": str(parsing_result_path),
            "description": "íŒŒì‹± ê²°ê³¼ ì¢…í•©",
        })

    if parsing_results.get("parsing_results"):
        for parser_name, parser_result in parsing_results["parsing_results"].items():
            if not parser_result.get("success"):
                continue
            parser_dir = output_dir / parser_name
            text_file = parser_dir / f"{parser_name}_text.txt"
            if text_file.exists():
                saved_files.append({
                    "type": "extracted_text",
                    "parser": parser_name,
                    "path": str(text_file),
                    "description": f"{parser_name} íŒŒì„œë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸",
                })
            metadata_file = parser_dir / f"{parser_name}_metadata.json"
            if metadata_file.exists():
                saved_files.append({
                    "type": "metadata",
                    "parser": parser_name,
                    "path": str(metadata_file),
                    "description": f"{parser_name} íŒŒì„œ ë©”íƒ€ë°ì´í„°",
                })
            structure_file = parser_dir / f"{parser_name}_structure.json"
            if structure_file.exists():
                saved_files.append({
                    "type": "parser_structure",
                    "parser": parser_name,
                    "path": str(structure_file),
                    "description": f"{parser_name} íŒŒì„œ êµ¬ì¡° ì •ë³´",
                })

    # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ íŒŒì¼ ì¶”ê°€
    if image_analysis_result and image_analysis_result.get("success"):
        # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ JSON íŒŒì¼
        image_result_file = output_dir / "image_analysis.json"
        if image_result_file.exists():
            saved_files.append({
                "type": "image_analysis",
                "path": str(image_result_file),
                "description": f"ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ({image_analysis_result.get('images_count', 0)}ê°œ ì´ë¯¸ì§€)",
            })

        # ì¶”ì¶œëœ ì´ë¯¸ì§€ íŒŒì¼ë“¤
        images_dir = output_dir / "images"
        if images_dir.exists() and any(images_dir.glob("*.png")):
            image_files = list(images_dir.glob("*.png"))
            saved_files.append({
                "type": "extracted_images",
                "path": str(images_dir),
                "description": f"ì¶”ì¶œëœ ì´ë¯¸ì§€ íŒŒì¼ë“¤ ({len(image_files)}ê°œ)",
                "count": len(image_files)
            })

    api_response = {
        "saved_files": saved_files,
        "output_directory": str(output_dir),
        "generation_timestamp": datetime.now().isoformat(),
        "source_parser": best_parser,
        "statistics": {
            "total_saved_files": len(saved_files),
            "file_types": {},
        },
    }

    for file_info in saved_files:
        file_type = file_info.get("type", "unknown")
        api_response["statistics"]["file_types"][file_type] = api_response["statistics"]["file_types"].get(file_type, 0) + 1

    with response_path.open('w', encoding='utf-8') as f:
        json.dump(api_response, f, ensure_ascii=False, indent=2)

    return api_response


async def _generate_chunk_based_knowledge_graph(
    req: StructureAnalysisRequest,
    db: Session,
    file_path: Path,
    directory_path: Optional[Path]
):
    """ì²­í¬ ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„±"""

    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if directory_path:
            output_dir = directory_path
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = file_path.parent / f"chunk_kg_{file_path.stem}_{timestamp}"

        output_dir.mkdir(parents=True, exist_ok=True)

        # ì²­í¬ ë¶„ì„ê¸° ì´ˆê¸°í™”
        chunk_analyzer = ChunkAnalyzer(db)

        # ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì‹¤í–‰
        start_time = datetime.now()

        integrated_result = chunk_analyzer.analyze_document_with_chunking(
            file_path=str(file_path),
            output_directory=str(output_dir),
            max_chunk_size=req.max_chunk_size,
            use_llm=req.llm is not None and req.llm.get("enabled", True),
            extractors=req.extractors,
            analysis_types=req.analysis_types
        )

        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()

        # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        chunk_files = []

        # ì£¼ìš” ê²°ê³¼ íŒŒì¼ë“¤
        main_files = [
            ("integrated_analysis_result.json", "í†µí•© ë¶„ì„ ê²°ê³¼"),
            ("chunks_detailed_results.json", "ì²­í¬ë³„ ìƒì„¸ ê²°ê³¼"),
            ("chunks_info.json", "ì²­í¬ ì •ë³´"),
            ("document_structure.json", "ë¬¸ì„œ êµ¬ì¡°")
        ]

        for filename, description in main_files:
            file_path_obj = output_dir / filename
            if file_path_obj.exists():
                chunk_files.append({
                    "type": "analysis_result",
                    "path": str(file_path_obj),
                    "description": description
                })

        # ì²­í¬ë³„ íŒŒì¼ë“¤
        chunks_text_dir = output_dir / "chunks_text"
        if chunks_text_dir.exists():
            for chunk_file in chunks_text_dir.glob("*.txt"):
                chunk_files.append({
                    "type": "chunk_text",
                    "path": str(chunk_file),
                    "description": f"ì²­í¬ í…ìŠ¤íŠ¸: {chunk_file.stem}"
                })

        # í”„ë¡¬í”„íŠ¸ íŒŒì¼ë“¤
        prompts_dir = output_dir / "chunk_prompts"
        if prompts_dir.exists():
            prompt_count = len(list(prompts_dir.glob("*.txt")))
            chunk_files.append({
                "type": "prompts_directory",
                "path": str(prompts_dir),
                "description": f"ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ({prompt_count}ê°œ)"
            })

        # ì‹¤í–‰ ê²°ê³¼ íŒŒì¼ë“¤
        results_dir = output_dir / "chunk_results"
        if results_dir.exists():
            result_count = len(list(results_dir.glob("*")))
            chunk_files.append({
                "type": "results_directory",
                "path": str(results_dir),
                "description": f"ì‹¤í–‰ ê²°ê³¼ ({result_count}ê°œ)"
            })

        response_data = {
            "analysis_method": "chunk_based_knowledge_graph",
            "success": True,
            "chunks_analyzed": integrated_result.total_chunks,
            "total_content_length": integrated_result.total_content_length,
            "processing_time_seconds": processing_time,
            "integrated_keywords_count": len(integrated_result.integrated_keywords),
            "saved_files": chunk_files,
            "output_directory": str(output_dir),
            "generation_timestamp": end_time.isoformat(),
            "chunk_summary": [
                {
                    "chunk_id": chunk_result.chunk_id,
                    "level": chunk_result.level,
                    "content_length": chunk_result.content_length,
                    "keywords_count": len(chunk_result.keywords),
                    "has_knowledge_graph": bool(chunk_result.knowledge_graph)
                }
                for chunk_result in integrated_result.chunk_results
            ],
            "statistics": {
                "total_saved_files": len(chunk_files),
                "file_types": {},
            }
        }

        # íŒŒì¼ íƒ€ì…ë³„ í†µê³„
        for file_info in chunk_files:
            file_type = file_info.get("type", "unknown")
            response_data["statistics"]["file_types"][file_type] = response_data["statistics"]["file_types"].get(file_type, 0) + 1

        # ì‘ë‹µ ì €ì¥
        response_path = output_dir / "chunk_knowledge_graph_response.json"
        with response_path.open('w', encoding='utf-8') as f:
            json.dump(response_data, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… ì²­í¬ ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ)")

        return response_data

    except Exception as e:
        logger.error(f"âŒ ì²­í¬ ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")


@router.get("/knowledge-graph")
async def get_knowledge_graph(
    file_path: str,
    directory: Optional[str] = None,
    force_reparse: bool = False,
    force_reanalyze: bool = False,
    force_rebuild: bool = False,
    use_chunking: bool = Query(False, description="ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì‚¬ìš© ì—¬ë¶€"),
    max_chunk_size: int = Query(50000, description="ìµœëŒ€ ì²­í¬ í¬ê¸°"),
    extractors: str = Query("KeyBERT,spaCy NER,LLM", description="ì¶”ì¶œê¸° ëª©ë¡ (ì½¤ë§ˆ êµ¬ë¶„)"),
    analysis_types: str = Query("keywords,summary,structure,knowledge_graph", description="ë¶„ì„ ìœ í˜• (ì½¤ë§ˆ êµ¬ë¶„)"),
    analyze_images: bool = Query(False, description="ì´ë¯¸ì§€ ë¶„ì„ í™œì„±í™” (ë©€í‹°ëª¨ë‹¬ LLM ì‚¬ìš©)"),
    extract_images: bool = Query(True, description="ì´ë¯¸ì§€ ì¶”ì¶œ í™œì„±í™”"),
    db: Session = Depends(get_db),
):
    """GET ë°©ì‹ìœ¼ë¡œ ì§€ì‹ ê·¸ë˜í”„ ìƒì„± (ì²­í¬ ê¸°ë°˜ ë¶„ì„ ì§€ì›)"""

    # ë¬¸ìì—´ íŒŒë¼ë¯¸í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    extractors_list = [ext.strip() for ext in extractors.split(",")]
    analysis_types_list = [analysis_type.strip() for analysis_type in analysis_types.split(",")]

    request = StructureAnalysisRequest(
        file_path=file_path,
        directory=directory,
        force_reparse=force_reparse,
        force_reanalyze=force_reanalyze,
        force_rebuild=force_rebuild,
        use_chunking=use_chunking,
        max_chunk_size=max_chunk_size,
        extractors=extractors_list,
        analysis_types=analysis_types_list,
        analyze_images=analyze_images,
        extract_images=extract_images
    )
    return await generate_knowledge_graph(request, db)


# ============================================================================
# Full Knowledge Graph API - ë¬¸ì„œ ì „ì²´ë¥¼ Knowledge Graphë¡œ ë³€í™˜
# ============================================================================


class FullKnowledgeGraphRequest(BaseModel):
    """ì „ì²´ Knowledge Graph ìƒì„± ìš”ì²­"""
    file_path: str
    directory: Optional[str] = None
    domain: str = "general"  # general, technical, academic, business, legal
    force_reparse: bool = False
    include_structure: bool = True  # êµ¬ì¡° ë¶„ì„ ì •ë³´ í¬í•¨ ì—¬ë¶€
    save_format: str = "json"  # json, cypher, graphml, all
    llm: Optional[Dict[str, Any]] = None


@router.post("/full-knowledge-graph")
async def generate_full_knowledge_graph(req: FullKnowledgeGraphRequest, db: Session = Depends(get_db)):
    """
    ë¬¸ì„œ ì „ì²´ë¥¼ Knowledge Graphë¡œ ë³€í™˜í•˜ëŠ” ì „ìš© API

    ë©”íƒ€ì •ë³´ê°€ ì•„ë‹Œ ë¬¸ì„œ ë‚´ìš© ì „ì²´ë¥¼ ì—”í‹°í‹°ì™€ ê´€ê³„ë¡œ ì¶”ì¶œí•˜ì—¬ ê·¸ë˜í”„ êµ¬ì¡°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Features:
    - ë„ë©”ì¸ë³„ ë§ì¶¤ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ (general, technical, academic, business, legal)
    - ë‹¤ì–‘í•œ ì¶œë ¥ í˜•ì‹ ì§€ì› (JSON, Cypher, GraphML)
    - êµ¬ì¡° ì •ë³´ í†µí•© ë¶„ì„ (ì„ íƒ)
    - LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ì¶”ì¶œ

    Args:
        req: Knowledge Graph ìƒì„± ìš”ì²­
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜

    Returns:
        Knowledge Graph ê²°ê³¼ (nodes, edges, stats, metadata)
    """
    try:
        file_path = _ensure_absolute(Path(req.file_path))
        _validate_file(file_path)

        directory_path = None
        if req.directory:
            directory_path = _ensure_absolute(Path(req.directory))
            directory_path.mkdir(parents=True, exist_ok=True)

        logger.info(f"ğŸ” Full Knowledge Graph ìƒì„± ì‹œì‘: {file_path.name} (ë„ë©”ì¸: {req.domain})")

        # 1. ë¬¸ì„œ íŒŒì‹±
        parser_service = DocumentParserService()
        output_dir = parser_service.get_output_directory(file_path, directory_path)

        parsing_results = None
        if req.force_reparse or not parser_service.has_parsing_results(file_path, directory_path):
            parsing_results = parser_service.parse_document_comprehensive(
                file_path=file_path,
                force_reparse=req.force_reparse,
                directory=directory_path,
            )
        else:
            parsing_results = parser_service.load_existing_parsing_results(file_path, directory_path)

        # 2. ìµœìƒì˜ íŒŒì„œ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        best_parser = parsing_results.get("summary", {}).get("best_parser")
        document_text = ""
        if best_parser and best_parser in parsing_results.get("parsing_results", {}):
            parser_dir = output_dir / best_parser
            text_file = parser_dir / f"{best_parser}_text.txt"
            if text_file.exists():
                document_text = text_file.read_text(encoding='utf-8')
            else:
                raise HTTPException(status_code=500, detail=f"íŒŒì‹±ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {text_file}")
        else:
            raise HTTPException(status_code=500, detail="ë¬¸ì„œ íŒŒì‹± ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        if not document_text or len(document_text) < 100:
            raise HTTPException(status_code=400, detail="ë¬¸ì„œ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        logger.info(f"ğŸ“„ ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(document_text):,}ì")

        # 3. êµ¬ì¡° ì •ë³´ ì¶”ì¶œ (ì„ íƒ)
        structure_info = None
        if req.include_structure:
            structure_response_path = output_dir / "llm_structure_analysis.json"
            if structure_response_path.exists():
                with structure_response_path.open('r', encoding='utf-8') as f:
                    structure_info = json.load(f)
                logger.info("ğŸ“Š ê¸°ì¡´ êµ¬ì¡° ë¶„ì„ ì •ë³´ ë¡œë“œ ì™„ë£Œ")
            else:
                # êµ¬ì¡° ë¶„ì„ì´ ì—†ìœ¼ë©´ ê°„ë‹¨íˆ ì‹¤í–‰
                analyzer = LocalFileAnalyzer(db)
                structure_result = analyzer.analyze_document_structure_with_llm(
                    text=document_text[:50000],  # êµ¬ì¡° ë¶„ì„ì€ ì•ë¶€ë¶„ë§Œ
                    file_path=str(file_path),
                    file_extension=file_path.suffix,
                    overrides=req.llm or {}
                )
                if structure_result.get("success"):
                    structure_info = structure_result.get("analysis", {})
                    logger.info("ğŸ“Š êµ¬ì¡° ë¶„ì„ ì‹¤í–‰ ì™„ë£Œ")

        # 4. Knowledge Graph ìƒì„±
        kg_builder = KnowledgeGraphBuilder(db)
        kg_result = kg_builder.build_knowledge_graph(
            text=document_text,
            file_path=str(file_path),
            domain=req.domain,
            structure_info=structure_info,
            llm_config=req.llm or {}
        )

        if not kg_result.get("success"):
            raise HTTPException(
                status_code=500,
                detail=f"Knowledge Graph ìƒì„± ì‹¤íŒ¨: {kg_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
            )

        # 5. Knowledge Graph ì €ì¥
        saved_files = kg_builder.save_knowledge_graph(
            kg_result=kg_result,
            output_dir=output_dir,
            format=req.save_format
        )

        # 6. ìµœì¢… ì‘ë‹µ êµ¬ì„±
        response = {
            "success": True,
            "file_path": str(file_path),
            "domain": req.domain,
            "graph": kg_result.get("graph", {}),
            "stats": kg_result.get("stats", {}),
            "metadata": kg_result.get("metadata", {}),
            "saved_files": saved_files,
            "extraction_date": kg_result.get("extraction_date"),
        }

        logger.info(
            f"âœ… Full Knowledge Graph ìƒì„± ì™„ë£Œ: "
            f"{response['stats'].get('entity_count', 0)}ê°œ ì—”í‹°í‹°, "
            f"{response['stats'].get('relationship_count', 0)}ê°œ ê´€ê³„"
        )

        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Full Knowledge Graph ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Knowledge Graph ìƒì„± ì‹¤íŒ¨: {str(e)}")


@router.get("/full-knowledge-graph")
async def get_full_knowledge_graph(
    file_path: str,
    directory: Optional[str] = None,
    domain: str = Query("general", description="ë¬¸ì„œ ë„ë©”ì¸ (general/technical/academic/business/legal)"),
    force_reparse: bool = Query(False, description="ê°•ì œ ì¬íŒŒì‹±"),
    include_structure: bool = Query(True, description="êµ¬ì¡° ë¶„ì„ ì •ë³´ í¬í•¨"),
    save_format: str = Query("json", description="ì €ì¥ í˜•ì‹ (json/cypher/graphml/all)"),
    db: Session = Depends(get_db),
):
    """GET ë°©ì‹ìœ¼ë¡œ ì „ì²´ Knowledge Graph ìƒì„±"""

    request = FullKnowledgeGraphRequest(
        file_path=file_path,
        directory=directory,
        domain=domain,
        force_reparse=force_reparse,
        include_structure=include_structure,
        save_format=save_format
    )
    return await generate_full_knowledge_graph(request, db)
