from typing import List, Dict, Any, Optional
from pathlib import Path
import re
import time
import numpy as np
from .base import KeywordExtractor, Keyword
from utils.text_cleaner import TextCleaner
from utils.position_mapper import PositionMapper
from utils.debug_logger import get_debug_logger

class KeyBERTExtractor(KeywordExtractor):
    """KeyBERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, db_session = None):
        super().__init__("keybert", config)
        self.model = None
        self.model_name = config.get('model', 'all-MiniLM-L6-v2') if config else 'all-MiniLM-L6-v2'
    
    def load_model(self) -> bool:
        """KeyBERT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            from keybert import KeyBERT
            import logging
            
            logger = logging.getLogger(__name__)
            
            # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ì´ ìˆì§€ë§Œ ë‹¤ë¥¸ ëª¨ë¸ì„ ìš”ì²­í•˜ëŠ” ê²½ìš° ì¬ë¡œë“œ
            if self.model is not None and hasattr(self.model, '_model_name'):
                if self.model._model_name != self.model_name:
                    logger.info(f"ğŸ”„ KeyBERT ëª¨ë¸ ë³€ê²½: {self.model._model_name} -> {self.model_name}")
                    self.model = None
                    self.is_loaded = False
            
            if not self.is_loaded or self.model is None:
                logger.info(f"ğŸ“¥ KeyBERT ëª¨ë¸ '{self.model_name}' ë¡œë“œ ì‹œì‘...")
                self.model = KeyBERT(model=self.model_name)
                # ë¡œë“œëœ ëª¨ë¸ ì´ë¦„ì„ ì €ì¥ (ì°¨í›„ ë¹„êµìš©)
                self.model._model_name = self.model_name
                self.is_loaded = True
                logger.info(f"âœ… KeyBERT ëª¨ë¸ '{self.model_name}' ë¡œë“œ ì„±ê³µ")
            
            return True
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"âŒ KeyBERT ëª¨ë¸ '{self.model_name}' ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            return False
    
    def extract(self, text: str, file_path: Optional[Path] = None) -> List[Keyword]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        import logging
        logger = logging.getLogger(__name__)
        debug_logger = get_debug_logger()
        start_time = time.time()
        
        logger.info(f"ğŸ” KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘ - ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        # ë””ë²„ê·¸ ë¡œê¹…: ì¶”ì¶œ ì‹œì‘
        debug_logger.start_extraction(
            extractor_name="keybert",
            file_info={"filename": str(file_path) if file_path else "unknown", "id": None},
            text=text,
            config=self.config
        )
        
        # ìœ„ì¹˜ ë§¤í•‘ ìƒì„±
        position_mapper = PositionMapper()
        position_map = position_mapper.create_position_map(text, file_path)
        logger.info(f"ğŸ“ ìœ„ì¹˜ ë§¤í•‘ ìƒì„± ì™„ë£Œ - ì´ {position_map['total_pages']}í˜ì´ì§€, {position_map['total_lines']}ì¤„")
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        original_text_copy = text  # ì›ë³¸ ë³´ê´€
        cleaned_text = TextCleaner.clean_text(text)
        logger.info(f"ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ - ì •ì œëœ ê¸¸ì´: {len(cleaned_text)} ë¬¸ì")
        
        # ë””ë²„ê·¸ ë¡œê¹…: ì „ì²˜ë¦¬ ê²°ê³¼
        debug_logger.log_preprocessing(
            extractor_name="keybert",
            original_text=original_text_copy,
            preprocessed_text=cleaned_text,
            preprocessing_steps=["clean_text", "remove_extra_whitespace", "normalize_unicode"]
        )
        
        if not self.is_loaded:
            logger.info("ğŸ“¦ KeyBERT ëª¨ë¸ ë¡œë“œ ì‹œë„...")
            self.load_model()
        
        if not self.is_loaded:
            logger.warning("âš ï¸ KeyBERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê°„ë‹¨í•œ ì¶”ì¶œë¡œ fallback")
            # KeyBERT ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ ì¶”ì¶œë¡œ fallback
            return self._extract_keywords_simple(cleaned_text)
        
        try:
            logger.info(f"ğŸ¯ KeyBERT ëª¨ë¸ '{self.model_name}'ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
            # ì‹¤ì œ KeyBERTë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords_keybert(cleaned_text, text, position_mapper, position_map)
            
            # ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¡œê¹…
            extraction_time = time.time() - start_time
            if keywords:
                top_keywords = sorted(keywords, key=lambda x: x.score, reverse=True)[:5]
                keyword_summary = [f"{kw.text}({kw.score:.3f})" for kw in top_keywords]
                logger.info(f"âœ… KeyBERT ì¶”ì¶œ ì™„ë£Œ - {len(keywords)}ê°œ í‚¤ì›Œë“œ, ìƒìœ„: {', '.join(keyword_summary)}")
            else:
                logger.warning("âš ï¸ KeyBERTì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í•¨")
            
            # ë””ë²„ê·¸ ë¡œê¹…: ìµœì¢… ê²°ê³¼
            debug_logger.log_final_results(
                extractor_name="keybert",
                final_keywords=[{
                    "keyword": kw.text,
                    "score": kw.score,
                    "category": kw.category,
                    "start_position": kw.start_position,
                    "end_position": kw.end_position,
                    "page_number": kw.page_number,
                    "line_number": kw.line_number,
                    "context": kw.context_snippet
                } for kw in keywords],
                extraction_time=extraction_time,
                total_processing_time=time.time() - start_time
            )
            
            return keywords
        except Exception as e:
            logger.error(f"âŒ KeyBERT ì¶”ì¶œ ì‹¤íŒ¨, ê°„ë‹¨í•œ ì¶”ì¶œë¡œ fallback: {e}")
            return self._extract_keywords_simple(cleaned_text)
    
    def is_available(self) -> bool:
        """KeyBERTëŠ” ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œì—ë„ fallback ì¶”ì¶œì´ ê°€ëŠ¥í•˜ë¯€ë¡œ í•­ìƒ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."""
        return True
    
    def _extract_keywords_simple(self, text: str) -> List[Keyword]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œ í…ìŠ¤íŠ¸ ê¸°ë°˜)"""
        import re
        import logging
        from collections import Counter
        
        logger = logging.getLogger(__name__)
        logger.info(f"ğŸ”§ KeyBERT fallback - ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘ (í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì)")
        
        # í•œêµ­ì–´ ëª…ì‚¬ íŒ¨í„´ê³¼ ì˜ì–´ ë‹¨ì–´ íŒ¨í„´ì„ ê²°í•©
        korean_pattern = r'[ê°€-í£]{2,}'  # 2ê¸€ì ì´ìƒ í•œê¸€
        english_pattern = r'[A-Za-z]{3,}'  # 3ê¸€ì ì´ìƒ ì˜ì–´
        
        # ìœ íš¨í•œ ë‹¨ì–´ ì¶”ì¶œ (TextCleaner ì‚¬ìš©)
        valid_words = TextCleaner.extract_valid_words(text, min_length=2)
        
        # ë¹ˆë„ ê³„ì‚°
        word_freq = Counter(valid_words)
        
        # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
        max_keywords = self.config.get('max_keywords', 10)
        top_words = word_freq.most_common(max_keywords)
        
        # ê°„ë‹¨í•œ ì¶”ì¶œ ê²°ê³¼ ë¡œê¹…
        if top_words:
            simple_keywords = [f"{word}({freq})" for word, freq in top_words[:5]]
            logger.info(f"ğŸ”§ ê°„ë‹¨í•œ ì¶”ì¶œ ê²°ê³¼ ({len(top_words)}ê°œ): {', '.join(simple_keywords)}")
        else:
            logger.warning("âš ï¸ ê°„ë‹¨í•œ ì¶”ì¶œì—ì„œë„ í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í•¨")
        
        results = []
        for word, freq in top_words:
            # ì ìˆ˜ ê³„ì‚° (ë¹ˆë„ ê¸°ë°˜, 0-1 ì •ê·œí™”)
            score = min(1.0, freq / max(1, len(word_freq)) * 10)  # ë¹ˆë„ë¥¼ 0-1ë¡œ ì •ê·œí™”
            
            # í…ìŠ¤íŠ¸ì—ì„œ ìœ„ì¹˜ ì°¾ê¸°
            positions = self._find_keyword_positions(text, word)
            if positions:
                start_pos, end_pos = positions[0]  # ì²« ë²ˆì§¸ ìœ„ì¹˜
                context = self._extract_context(text, start_pos, end_pos)
                
                results.append(Keyword(
                    text=word,
                    score=score,
                    extractor=self.name,
                    category="keyword",
                    start_position=start_pos,
                    end_position=end_pos,
                    context_snippet=context,
                    page_number=None,
                    line_number=None,
                    column_number=None
                ))
        
        logger.info(f"âœ… ê°„ë‹¨í•œ ì¶”ì¶œ ì™„ë£Œ - {len(results)}ê°œ í‚¤ì›Œë“œ ë°˜í™˜")
        return results
    
    def _extract_keywords_keybert(self, text: str, original_text: str, position_mapper: PositionMapper, position_map: Dict[str, any]) -> List[Keyword]:
        """ì‹¤ì œ KeyBERTë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        import logging
        logger = logging.getLogger(__name__)
        debug_logger = get_debug_logger()
        
        try:
            # KeyBERT ì„¤ì • (ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ìš°ì„ , config fallback)
            max_keywords = self.config.get('max_keywords', 10) if self.config else 10
            keyphrase_ngram_range = tuple(self.config.get('keyphrase_ngram_range', [1, 2])) if self.config else (1, 2)
            stop_words = self.config.get('stop_words', 'english') if self.config else 'english'
            use_maxsum = self.config.get('use_maxsum', False) if self.config else False
            use_mmr = self.config.get('use_mmr', True) if self.config else True
            diversity = self.config.get('diversity', 0.5) if self.config else 0.5
            
            # ì¶”ì¶œ ì„¤ì • ë¡œê¹…
            algorithm = "MMR" if use_mmr else "MaxSum" if use_maxsum else "CosineSim"
            logger.info(f"âš™ï¸ KeyBERT ì„¤ì • - ì•Œê³ ë¦¬ì¦˜: {algorithm}, ìµœëŒ€í‚¤ì›Œë“œ: {max_keywords}, n-gram: {keyphrase_ngram_range}, ë‹¤ì–‘ì„±: {diversity if use_mmr else 'N/A'}")
            
            # N-gram í›„ë³´ ìƒì„± (ë””ë²„ê¹…ìš©)
            from sklearn.feature_extraction.text import CountVectorizer
            try:
                vectorizer = CountVectorizer(
                    ngram_range=keyphrase_ngram_range,
                    stop_words=stop_words if stop_words != 'korean' else None,
                    max_features=1000  # í›„ë³´ ì œí•œ
                )
                vectorizer.fit([text])
                candidates = vectorizer.get_feature_names_out().tolist()
                
                # ë””ë²„ê·¸ ë¡œê¹…: í›„ë³´ ìƒì„±
                debug_logger.log_candidate_generation(
                    extractor_name="keybert",
                    candidates=candidates,
                    generation_method=f"CountVectorizer_ngram_{keyphrase_ngram_range}",
                    params={
                        "ngram_range": keyphrase_ngram_range,
                        "stop_words": stop_words,
                        "max_features": 1000
                    }
                )
            except Exception as e:
                logger.debug(f"í›„ë³´ ìƒì„± ë””ë²„ê¹… ì‹¤íŒ¨: {e}")
                candidates = []
            
            # ì„ë² ë”© ë¡œê¹… (KeyBERT ë‚´ë¶€ ëª¨ë¸ ì •ë³´)
            debug_logger.log_embeddings(
                extractor_name="keybert",
                model_name=self.model_name
            )
            
            # KeyBERTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ (ìƒˆ API ì‚¬ìš©)
            if use_mmr:
                # MMR (Maximal Marginal Relevance) ì‚¬ìš© - ë‹¤ì–‘ì„± í™•ë³´
                logger.info(f"ğŸ§  MMR ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ (ë‹¤ì–‘ì„±: {diversity})...")
                keywords_scores = self.model.extract_keywords(
                    text, 
                    keyphrase_ngram_range=keyphrase_ngram_range,
                    stop_words=stop_words,
                    use_mmr=True,
                    diversity=diversity
                )[:max_keywords]
            elif use_maxsum:
                # Max Sum Similarity ì‚¬ìš©
                logger.info(f"ğŸ“Š MaxSum ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ (í›„ë³´: {max_keywords * 2})...")
                keywords_scores = self.model.extract_keywords(
                    text,
                    keyphrase_ngram_range=keyphrase_ngram_range,
                    stop_words=stop_words,
                    use_maxsum=True,
                    nr_candidates=max_keywords * 2
                )[:max_keywords]
            else:
                # ê¸°ë³¸ cosine similarity ì‚¬ìš©
                logger.info("ğŸ¯ Cosine Similarity ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
                keywords_scores = self.model.extract_keywords(
                    text,
                    keyphrase_ngram_range=keyphrase_ngram_range,
                    stop_words=stop_words
                )[:max_keywords]
            
            # ì›ì‹œ KeyBERT ê²°ê³¼ ë¡œê¹…
            if keywords_scores:
                raw_keywords = [f"{kw}({score:.3f})" for kw, score in keywords_scores[:5]]
                logger.info(f"ğŸ” KeyBERT ì›ì‹œ ê²°ê³¼ ({len(keywords_scores)}ê°œ): {', '.join(raw_keywords)}")
                
                # ë””ë²„ê·¸ ë¡œê¹…: ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼
                similarities = np.array([score for _, score in keywords_scores])
                candidates_only = [kw for kw, _ in keywords_scores]
                debug_logger.log_similarity_calculation(
                    extractor_name="keybert",
                    similarities=similarities,
                    candidates=candidates_only,
                    method=algorithm
                )
                
                # ë””ë²„ê·¸ ë¡œê¹…: ì•Œê³ ë¦¬ì¦˜ ì ìš© ê²°ê³¼
                debug_logger.log_algorithm_application(
                    extractor_name="keybert",
                    algorithm=algorithm,
                    input_candidates=keywords_scores,
                    output_keywords=keywords_scores,  # KeyBERTëŠ” í•œë²ˆì— ìµœì¢… ê²°ê³¼ ë°˜í™˜
                    algorithm_params={
                        "use_mmr": use_mmr,
                        "use_maxsum": use_maxsum,
                        "diversity": diversity if use_mmr else None,
                        "nr_candidates": max_keywords * 2 if use_maxsum else None
                    }
                )
            else:
                logger.warning("âš ï¸ KeyBERTì—ì„œ í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í•¨")
            
            results = []
            positioned_keywords = 0
            abstract_keywords = 0
            
            for keyword, score in keywords_scores:
                # í‚¤ì›Œë“œ ìœ íš¨ì„± ê²€ì‚¬
                if not TextCleaner.is_meaningful_keyword(keyword):
                    logger.debug(f"Invalid keyword filtered: {repr(keyword)}")
                    continue
                
                # í‚¤ì›Œë“œ ì •ê·œí™”
                normalized_keyword = TextCleaner.normalize_keyword(keyword)
                if not normalized_keyword:
                    continue
                
                # í…ìŠ¤íŠ¸ì—ì„œ ìœ„ì¹˜ ì°¾ê¸°
                positions = self._find_keyword_positions(text, normalized_keyword)
                
                if positions:
                    positioned_keywords += 1
                    # ì²« ë²ˆì§¸ ìœ„ì¹˜ ì‚¬ìš©
                    start_pos, end_pos = positions[0]
                    context = self._extract_context(text, start_pos, end_pos)
                    
                    # í˜ì´ì§€/ì¤„/ì»¬ëŸ¼ ë²ˆí˜¸ ê³„ì‚°
                    page_number, line_number, column_number = position_mapper.get_position_info(start_pos, position_map)
                    logger.debug(f"ğŸ“ '{keyword}' ìœ„ì¹˜ ê³„ì‚°: ë¬¸ì {start_pos}-{end_pos} -> í˜ì´ì§€ {page_number}, ì¤„ {line_number}, ì»¬ëŸ¼ {column_number}")
                    
                    results.append(Keyword(
                        text=normalized_keyword,
                        score=float(score),
                        extractor=self.name,
                        category="keybert_keyword",
                        start_position=start_pos,
                        end_position=end_pos,
                        context_snippet=context,
                        page_number=page_number,
                        line_number=line_number,
                        column_number=column_number
                    ))
                    
                    # ê°œë³„ í‚¤ì›Œë“œ ìœ„ì¹˜ ë¡œê¹… (ìƒìœ„ 3ê°œë§Œ)
                    if len(results) <= 3:
                        logger.info(f"  ğŸ“ '{keyword}' (ì ìˆ˜: {score:.3f}) - ìœ„ì¹˜: {start_pos}-{end_pos}, ì»¨í…ìŠ¤íŠ¸: '{context[:50]}{'...' if len(context) > 50 else ''}'")
                else:
                    abstract_keywords += 1
                    # ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° (ë³€í˜•ëœ í‚¤ì›Œë“œì¼ ìˆ˜ ìˆìŒ)
                    results.append(Keyword(
                        text=normalized_keyword,
                        score=float(score),
                        extractor=self.name,
                        category="keybert_keyword",
                        start_position=None,
                        end_position=None,
                        context_snippet=text[:100] + "..." if len(text) > 100 else text,
                        page_number=None,
                        line_number=None,
                        column_number=None
                    ))
                    
                    # ì¶”ìƒ í‚¤ì›Œë“œ ë¡œê¹… (ìƒìœ„ 3ê°œë§Œ)
                    if abstract_keywords <= 3:
                        logger.info(f"  ğŸ”® '{keyword}' (ì ìˆ˜: {score:.3f}) - ì¶”ìƒ í‚¤ì›Œë“œ (í…ìŠ¤íŠ¸ì—ì„œ ì •í™•í•œ ìœ„ì¹˜ ì—†ìŒ)")
            
            # ìµœì¢… í‚¤ì›Œë“œ í†µê³„ ë¡œê¹…
            logger.info(f"ğŸ“‹ KeyBERT í‚¤ì›Œë“œ ì²˜ë¦¬ ì™„ë£Œ - ì´ {len(results)}ê°œ (ìœ„ì¹˜ìˆìŒ: {positioned_keywords}, ì¶”ìƒ: {abstract_keywords})")
            
            # ë””ë²„ê·¸ ë¡œê¹…: ìœ„ì¹˜ ë¶„ì„ ê²°ê³¼
            keywords_with_positions = []
            for kw in results:
                kw_data = {
                    "keyword": kw.text,
                    "score": kw.score,
                    "positions": []
                }
                if kw.start_position is not None:
                    kw_data["positions"].append({
                        "start": kw.start_position,
                        "end": kw.end_position,
                        "page": kw.page_number,
                        "line": kw.line_number,
                        "context": kw.context_snippet
                    })
                keywords_with_positions.append(kw_data)
            
            debug_logger.log_position_analysis(
                extractor_name="keybert",
                keywords_with_positions=keywords_with_positions,
                text=text,
                analysis_method="simple_text_search"
            )
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ KeyBERT ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
        finally:
            # ë””ë²„ê·¸ ì„¸ì…˜ ì €ì¥
            debug_logger.save_debug_session()
    
    def _find_keyword_positions(self, text: str, keyword: str) -> List[tuple]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        positions = []
        start = 0
        while True:
            pos = text.find(keyword.lower(), start)
            if pos == -1:
                break
            positions.append((pos, pos + len(keyword)))
            start = pos + 1
        return positions
    
    def _extract_context(self, text: str, start_pos: int, end_pos: int, context_size: int = 50) -> str:
        """í‚¤ì›Œë“œ ì£¼ë³€ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        context_start = max(0, start_pos - context_size)
        context_end = min(len(text), end_pos + context_size)
        
        context = text[context_start:context_end]
        
        # ì•ë’¤ì— ìƒëµ í‘œì‹œ ì¶”ê°€
        if context_start > 0:
            context = "..." + context
        if context_end < len(text):
            context = context + "..."
            
        return context