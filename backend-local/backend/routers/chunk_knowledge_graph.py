"""
ì²­í¬ ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„± ì—”ë“œí¬ì¸íŠ¸

ê¸°ì¡´ knowledge-graph ì—”ë“œí¬ì¸íŠ¸ë¥¼ í™•ì¥í•˜ì—¬ ë¬¸ì„œ ë¶„í•  ë¶„ì„ì„ ì ìš©í•©ë‹ˆë‹¤.
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List

from fastapi import APIRouter, Depends, HTTPException, Query
from pydantic import BaseModel
from sqlalchemy.orm import Session

from dependencies import get_db
from services.chunk_analyzer import ChunkAnalyzer
from services.document_parser_service import DocumentParserService

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/chunk-analysis", tags=["chunk-analysis"])


class ChunkKnowledgeGraphRequest(BaseModel):
    file_path: str
    output_directory: Optional[str] = None
    max_chunk_size: int = 50000
    use_llm: bool = True
    extractors: List[str] = ["KeyBERT", "spaCy NER", "LLM"]
    analysis_types: List[str] = ["keywords", "summary", "structure", "knowledge_graph"]
    force_reparse: bool = False
    force_reanalyze: bool = False
    llm_config: Optional[Dict[str, Any]] = None


class ChunkAnalysisResponse(BaseModel):
    success: bool
    total_chunks: int
    total_content_length: int
    processing_time_seconds: float
    output_directory: str
    saved_files: List[Dict[str, str]]
    chunk_summary: List[Dict[str, Any]]
    integrated_keywords_count: int
    analysis_timestamp: str
    error_message: Optional[str] = None


def _ensure_absolute(path: Path) -> Path:
    return path if path.is_absolute() else (Path.cwd() / path).resolve()


def _validate_file(file_path: Path) -> None:
    if not file_path.exists():
        raise HTTPException(status_code=404, detail=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    if file_path.is_dir():
        raise HTTPException(status_code=400, detail="ë””ë ‰í† ë¦¬ê°€ ì•„ë‹Œ íŒŒì¼ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
    if file_path.stat().st_size == 0:
        raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

    supported_extensions = {".pdf", ".docx", ".doc", ".txt", ".md", ".html", ".xml", ".hwp"}
    if file_path.suffix.lower() not in supported_extensions:
        raise HTTPException(status_code=400, detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_path.suffix}")


@router.post("/knowledge-graph", response_model=ChunkAnalysisResponse)
async def generate_chunk_knowledge_graph(
    request: ChunkKnowledgeGraphRequest,
    db: Session = Depends(get_db)
):
    """ì²­í¬ ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„±"""

    file_path = _ensure_absolute(Path(request.file_path))
    _validate_file(file_path)

    logger.info(f"ğŸš€ ì²­í¬ ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„± ì‹œì‘: {file_path}")

    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if request.output_directory:
            output_dir = _ensure_absolute(Path(request.output_directory))
        else:
            # ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬: íŒŒì¼ ê²½ë¡œ ê¸°ë°˜
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = file_path.parent / f"chunk_analysis_{file_path.stem}_{timestamp}"

        output_dir.mkdir(parents=True, exist_ok=True)

        # ì²­í¬ ë¶„ì„ê¸° ì´ˆê¸°í™” ë° ì‹¤í–‰
        analyzer = ChunkAnalyzer(db)

        start_time = datetime.now()

        integrated_result = analyzer.analyze_document_with_chunking(
            file_path=str(file_path),
            output_directory=str(output_dir),
            max_chunk_size=request.max_chunk_size,
            use_llm=request.use_llm,
            extractors=request.extractors,
            analysis_types=request.analysis_types
        )

        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()

        # ì €ì¥ëœ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
        saved_files = _collect_saved_files(output_dir)

        # ì²­í¬ ìš”ì•½ ì •ë³´ ìƒì„±
        chunk_summary = []
        for chunk_result in integrated_result.chunk_results:
            chunk_summary.append({
                "chunk_id": chunk_result.chunk_id,
                "level": chunk_result.level,
                "content_length": chunk_result.content_length,
                "keywords_count": len(chunk_result.keywords),
                "has_summary": bool(chunk_result.summary),
                "has_structure_analysis": bool(chunk_result.structure_analysis),
                "has_knowledge_graph": bool(chunk_result.knowledge_graph)
            })

        logger.info(f"âœ… ì²­í¬ ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ)")

        return ChunkAnalysisResponse(
            success=True,
            total_chunks=integrated_result.total_chunks,
            total_content_length=integrated_result.total_content_length,
            processing_time_seconds=processing_time,
            output_directory=str(output_dir),
            saved_files=saved_files,
            chunk_summary=chunk_summary,
            integrated_keywords_count=len(integrated_result.integrated_keywords),
            analysis_timestamp=end_time.isoformat()
        )

    except Exception as e:
        logger.error(f"âŒ ì²­í¬ ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ì²­í¬ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
        )


@router.get("/knowledge-graph", response_model=ChunkAnalysisResponse)
async def get_chunk_knowledge_graph(
    file_path: str = Query(..., description="ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ"),
    output_directory: Optional[str] = Query(None, description="ì¶œë ¥ ë””ë ‰í† ë¦¬"),
    max_chunk_size: int = Query(50000, description="ìµœëŒ€ ì²­í¬ í¬ê¸°"),
    use_llm: bool = Query(True, description="LLM ì‚¬ìš© ì—¬ë¶€"),
    extractors: str = Query("KeyBERT,spaCy NER,LLM", description="ì¶”ì¶œê¸° ëª©ë¡ (ì½¤ë§ˆ êµ¬ë¶„)"),
    analysis_types: str = Query("keywords,summary,structure,knowledge_graph", description="ë¶„ì„ ìœ í˜• (ì½¤ë§ˆ êµ¬ë¶„)"),
    force_reparse: bool = Query(False, description="ê°•ì œ ì¬íŒŒì‹±"),
    force_reanalyze: bool = Query(False, description="ê°•ì œ ì¬ë¶„ì„"),
    db: Session = Depends(get_db)
):
    """GET ë°©ì‹ìœ¼ë¡œ ì²­í¬ ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„±"""

    # ë¬¸ìì—´ íŒŒë¼ë¯¸í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    extractors_list = [ext.strip() for ext in extractors.split(",")]
    analysis_types_list = [analysis_type.strip() for analysis_type in analysis_types.split(",")]

    request = ChunkKnowledgeGraphRequest(
        file_path=file_path,
        output_directory=output_directory,
        max_chunk_size=max_chunk_size,
        use_llm=use_llm,
        extractors=extractors_list,
        analysis_types=analysis_types_list,
        force_reparse=force_reparse,
        force_reanalyze=force_reanalyze
    )

    return await generate_chunk_knowledge_graph(request, db)


@router.get("/chunk-status/{chunk_id}")
async def get_chunk_status(
    chunk_id: str,
    output_directory: str = Query(..., description="ë¶„ì„ ê²°ê³¼ ë””ë ‰í† ë¦¬"),
    db: Session = Depends(get_db)
):
    """íŠ¹ì • ì²­í¬ì˜ ë¶„ì„ ìƒíƒœ ì¡°íšŒ"""

    try:
        output_path = Path(output_directory)

        if not output_path.exists():
            raise HTTPException(status_code=404, detail="ì¶œë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ì²­í¬ ë¶„ì„ê¸° ì´ˆê¸°í™” (ìƒíƒœ ì¡°íšŒìš©)
        analyzer = ChunkAnalyzer(db)
        analyzer.prompt_manager = analyzer.ChunkPromptManager(str(output_path))

        # ì²­í¬ í”„ë¡¬í”„íŠ¸ ìš”ì•½ ì¡°íšŒ
        chunk_summary = analyzer.prompt_manager.get_chunk_prompt_summary(chunk_id)

        return {
            "chunk_id": chunk_id,
            "status": "completed" if chunk_summary["total_executions"] > 0 else "pending",
            **chunk_summary
        }

    except Exception as e:
        logger.error(f"âŒ ì²­í¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì²­í¬ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@router.get("/analysis-report")
async def get_analysis_report(
    output_directory: str = Query(..., description="ë¶„ì„ ê²°ê³¼ ë””ë ‰í† ë¦¬"),
    format: str = Query("json", description="ë³´ê³ ì„œ í˜•ì‹ (json/markdown)")
):
    """ì „ì²´ ë¶„ì„ ë³´ê³ ì„œ ì¡°íšŒ"""

    try:
        output_path = Path(output_directory)

        if not output_path.exists():
            raise HTTPException(status_code=404, detail="ì¶œë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # í†µí•© ë¶„ì„ ê²°ê³¼ íŒŒì¼ ì¡°íšŒ
        integrated_result_file = output_path / "integrated_analysis_result.json"
        if not integrated_result_file.exists():
            raise HTTPException(status_code=404, detail="í†µí•© ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        with open(integrated_result_file, 'r', encoding='utf-8') as f:
            integrated_data = json.load(f)

        if format.lower() == "markdown":
            # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë³´ê³ ì„œ ìƒì„±
            report_lines = [
                f"# ë¬¸ì„œ ë¶„ì„ ë³´ê³ ì„œ",
                f"ìƒì„±ì¼ì‹œ: {datetime.now().isoformat()}",
                f"",
                f"## ë¶„ì„ ê°œìš”",
                f"- ì´ ì²­í¬ ìˆ˜: {integrated_data.get('total_chunks', 0)}",
                f"- ì´ ì½˜í…ì¸  ê¸¸ì´: {integrated_data.get('total_content_length', 0):,}ì",
                f"- í†µí•© í‚¤ì›Œë“œ ìˆ˜: {len(integrated_data.get('integrated_keywords', []))}",
                f"",
                f"## ê³„ì¸µì  ìš”ì•½",
                f"```json",
                json.dumps(integrated_data.get('hierarchical_summary', {}), ensure_ascii=False, indent=2),
                f"```",
                f"",
                f"## ìƒìœ„ í‚¤ì›Œë“œ",
            ]

            for i, keyword in enumerate(integrated_data.get('integrated_keywords', [])[:10], 1):
                keyword_text = keyword.get('keyword', 'ì•Œ ìˆ˜ ì—†ìŒ')
                frequency = keyword.get('frequency', 1)
                sources = len(keyword.get('sources', []))
                report_lines.append(f"{i}. **{keyword_text}** (ë¹ˆë„: {frequency}, ì¶œì²˜: {sources}ê°œ ì²­í¬)")

            return {
                "format": "markdown",
                "content": "\n".join(report_lines)
            }

        else:
            # JSON í˜•ì‹ ë°˜í™˜
            return {
                "format": "json",
                "content": integrated_data
            }

    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ë³´ê³ ì„œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ë³´ê³ ì„œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


def _collect_saved_files(output_dir: Path) -> List[Dict[str, str]]:
    """ì¶œë ¥ ë””ë ‰í† ë¦¬ì—ì„œ ì €ì¥ëœ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘"""

    saved_files = []

    # ì£¼ìš” ê²°ê³¼ íŒŒì¼ë“¤
    main_files = {
        "integrated_analysis_result.json": "í†µí•© ë¶„ì„ ê²°ê³¼",
        "chunks_detailed_results.json": "ì²­í¬ë³„ ìƒì„¸ ê²°ê³¼",
        "chunks_info.json": "ì²­í¬ ì •ë³´",
        "document_structure.json": "ë¬¸ì„œ êµ¬ì¡°"
    }

    for filename, description in main_files.items():
        file_path = output_dir / filename
        if file_path.exists():
            saved_files.append({
                "type": "main_result",
                "path": str(file_path),
                "description": description,
                "size": str(file_path.stat().st_size)
            })

    # ì²­í¬ í…ìŠ¤íŠ¸ íŒŒì¼ë“¤
    chunks_text_dir = output_dir / "chunks_text"
    if chunks_text_dir.exists():
        for chunk_file in chunks_text_dir.glob("*.txt"):
            saved_files.append({
                "type": "chunk_text",
                "path": str(chunk_file),
                "description": f"ì²­í¬ í…ìŠ¤íŠ¸: {chunk_file.stem}",
                "size": str(chunk_file.stat().st_size)
            })

    # í”„ë¡¬í”„íŠ¸ íŒŒì¼ë“¤
    prompts_dir = output_dir / "chunk_prompts"
    if prompts_dir.exists():
        for prompt_file in prompts_dir.glob("*.txt"):
            saved_files.append({
                "type": "prompt",
                "path": str(prompt_file),
                "description": f"í”„ë¡¬í”„íŠ¸: {prompt_file.stem}",
                "size": str(prompt_file.stat().st_size)
            })

    # ê²°ê³¼ íŒŒì¼ë“¤
    results_dir = output_dir / "chunk_results"
    if results_dir.exists():
        for result_file in results_dir.glob("*"):
            if result_file.is_file():
                saved_files.append({
                    "type": "chunk_result",
                    "path": str(result_file),
                    "description": f"ì²­í¬ ê²°ê³¼: {result_file.stem}",
                    "size": str(result_file.stat().st_size)
                })

    return saved_files


__all__ = ["router"]