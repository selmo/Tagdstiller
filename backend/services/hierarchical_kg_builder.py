"""
Hierarchical Knowledge Graph Builder

ë¬¸ì„œ êµ¬ì¡° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì¸µì  KGë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
ë¬¸ì„œ â†’ ì„¹ì…˜ â†’ í•˜ìœ„êµ¬ì¡°(í…Œì´ë¸”, ë‹¨ë½, ì´ë¯¸ì§€) â†’ ì—”í‹°í‹° ê´€ê³„ë¥¼ ëª…ì‹œí•©ë‹ˆë‹¤.
"""

import hashlib
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple
from dataclasses import dataclass

from .kg_schema_manager import KGSchemaManager, DocumentDomain
from .memgraph_service import MemgraphService


def _hash(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()[:16]


@dataclass
class StructuralElement:
    """ë¬¸ì„œ êµ¬ì¡° ìš”ì†Œ"""
    id: str
    type: str  # Section, Table, Paragraph, Image, List, etc.
    properties: Dict[str, Any]
    parent_id: Optional[str] = None
    children: List[str] = None
    content: str = ""
    position: Dict[str, Any] = None  # page, line, bbox ë“±


@dataclass
class HierarchicalEntity:
    """ê³„ì¸µì  ì—”í‹°í‹° (êµ¬ì¡° ìš”ì†Œì™€ ì—°ê²°ëœ ì—”í‹°í‹°)"""
    id: str
    type: str
    properties: Dict[str, Any]
    source_structure: str  # ì´ ì—”í‹°í‹°ê°€ ë°œê²¬ëœ êµ¬ì¡° ìš”ì†Œ ID
    extraction_context: Dict[str, Any]  # ì¶”ì¶œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´


class HierarchicalKGBuilder:
    """ê³„ì¸µì  Knowledge Graph ë¹Œë”"""
    
    def __init__(self, memgraph_config: Dict[str, Any] = None, auto_save_to_memgraph: bool = True, db_session = None):
        self.schema_manager = KGSchemaManager()
        self.auto_save_to_memgraph = auto_save_to_memgraph
        self.logger = logging.getLogger(__name__)
        self.db_session = db_session
        
        # LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ê´€ê³„ ì¶”ì¶œìš©)
        self.llm_service = None
        if db_session:
            try:
                from extractors.llm_extractor import LLMExtractor
                from services.config_service import ConfigService
                
                # LLM ì„¤ì • ë¡œë“œ
                llm_config = {
                    'provider': 'ollama',
                    'model': ConfigService.get_config_value(db_session, 'OLLAMA_MODEL', 'llama3.2'),
                    'base_url': ConfigService.get_config_value(db_session, 'OLLAMA_BASE_URL', 'http://localhost:11434'),
                    'timeout': ConfigService.get_int_config(db_session, 'OLLAMA_TIMEOUT', 30)
                }
                
                self.llm_service = LLMExtractor(config=llm_config, db_session=db_session)
                if self.llm_service.load_model():
                    self.logger.info("âœ… ê³„ì¸µì  KG Builder: LLM ì„œë¹„ìŠ¤ í™œì„±í™”")
                else:
                    self.logger.warning("âš ï¸ ê³„ì¸µì  KG Builder: LLM ì„œë¹„ìŠ¤ ë¡œë“œ ì‹¤íŒ¨")
                    self.llm_service = None
            except Exception as e:
                self.logger.warning(f"âš ï¸ ê³„ì¸µì  KG Builder: LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨ ({e})")
                self.llm_service = None
        
        # Memgraph ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.memgraph_service = None
        if auto_save_to_memgraph:
            try:
                self.memgraph_service = MemgraphService(
                    uri=memgraph_config.get("uri", "bolt://localhost:7687") if memgraph_config else "bolt://localhost:7687",
                    username=memgraph_config.get("username", "") if memgraph_config else "",
                    password=memgraph_config.get("password", "") if memgraph_config else ""
                )
                
                if self.memgraph_service.is_connected():
                    self.logger.info("âœ… ê³„ì¸µì  KG Builder: Memgraph ìë™ ì €ì¥ í™œì„±í™”")
                else:
                    self.logger.warning("âš ï¸ ê³„ì¸µì  KG Builder: Memgraph ì—°ê²° ì‹¤íŒ¨")
                    self.memgraph_service = None
            except Exception as e:
                self.logger.warning(f"âš ï¸ ê³„ì¸µì  KG Builder: Memgraph ì´ˆê¸°í™” ì‹¤íŒ¨ ({e})")
                self.memgraph_service = None
    
    def build_hierarchical_knowledge_graph(
        self, 
        file_path: str, 
        document_text: str, 
        keywords: Dict[str, Any], 
        metadata: Dict[str, Any], 
        structure_analysis: Dict[str, Any] = None, 
        parsing_results: Dict[str, Any] = None, 
        force_rebuild: bool = False
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì¸µì  Knowledge Graph êµ¬ì¶•
        
        Args:
            file_path: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
            document_text: ë¬¸ì„œ í…ìŠ¤íŠ¸
            keywords: í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼
            metadata: íŒŒì¼ ë©”íƒ€ë°ì´í„°
            structure_analysis: ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ê²°ê³¼
            parsing_results: íŒŒì‹± ê²°ê³¼
            force_rebuild: ê°•ì œ ì¬êµ¬ì¶• ì—¬ë¶€
            
        Returns:
            ê³„ì¸µì  KG ë°ì´í„°
        """
        self.logger.info(f"ğŸ—ï¸ ê³„ì¸µì  KG êµ¬ì¶• ì‹œì‘: {file_path}")
        
        # ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥ (ê´€ê³„ ë¶„ì„ìš©)
        self._current_document_text = document_text
        
        # ë„ë©”ì¸ ê°ì§€
        domain, domain_confidence = self.schema_manager.detect_document_domain(document_text, metadata)
        
        result = {
            "entities": [],
            "relationships": [],
            "structural_hierarchy": [],
            "metadata": {
                "created_at": self._get_timestamp(),
                "file_path": file_path,
                "hierarchical_structure": True,
                "detected_domain": domain.value,
                "domain_confidence": domain_confidence,
                "extractors_used": list(keywords.keys()) if keywords else []
            }
        }
        
        # 1. ë¬¸ì„œ ë£¨íŠ¸ ì—”í‹°í‹° ìƒì„±
        doc_id = _hash(str(file_path))
        doc_entity = self._create_document_entity(doc_id, file_path, metadata, parsing_results, domain)
        result["entities"].append(doc_entity)
        
        # 2. êµ¬ì¡° ìš”ì†Œ ë¶„ì„ ë° ì—”í‹°í‹° ìƒì„±
        structural_elements = self._analyze_document_structure(
            doc_id, structure_analysis, parsing_results, domain
        )
        
        # 3. êµ¬ì¡° ìš”ì†Œë¥¼ KG ì—”í‹°í‹°ë¡œ ë³€í™˜
        structure_entities, structure_relationships = self._create_structure_entities_and_relationships(
            doc_id, structural_elements, domain
        )
        result["entities"].extend(structure_entities)
        result["relationships"].extend(structure_relationships)
        
        # 4. í‚¤ì›Œë“œë¥¼ êµ¬ì¡° ìš”ì†Œë³„ë¡œ ë¶„ë¥˜í•˜ê³  ì—”í‹°í‹°í™”
        keyword_entities, keyword_relationships = self._create_hierarchical_keyword_entities(
            doc_id, keywords, structural_elements, document_text, domain
        )
        result["entities"].extend(keyword_entities)
        result["relationships"].extend(keyword_relationships)
        
        # 5. ì—”í‹°í‹° ê°„ ì§ì ‘ ê´€ê³„ ì¶”ì¶œ **NEW!**
        entity_relationships = self._extract_entity_to_entity_relationships(
            result["entities"], document_text, domain, structural_elements
        )
        result["relationships"].extend(entity_relationships)
        
        # 6. êµ¬ì¡°ì  ê³„ì¸µ ì •ë³´ êµ¬ì¶•
        result["structural_hierarchy"] = self._build_structural_hierarchy(structural_elements)
        
        # 7. Memgraphì— ìë™ ì €ì¥
        if self.memgraph_service and self.auto_save_to_memgraph:
            self._save_to_memgraph(result, file_path)
        
        self.logger.info(f"âœ… ê³„ì¸µì  KG êµ¬ì¶• ì™„ë£Œ: {len(result['entities'])}ê°œ ì—”í‹°í‹°, {len(result['relationships'])}ê°œ ê´€ê³„")
        return result
    
    def _create_document_entity(self, doc_id: str, file_path: str, metadata: Dict[str, Any], 
                              parsing_results: Dict[str, Any], domain: DocumentDomain) -> Dict[str, Any]:
        """ë¬¸ì„œ ë£¨íŠ¸ ì—”í‹°í‹° ìƒì„±"""
        return {
            "id": doc_id,
            "type": "Document",
            "properties": {
                "title": metadata.get("name", Path(file_path).name),
                "path": file_path,
                "domain": domain.value,
                "size": metadata.get("size"),
                "extension": metadata.get("extension"),
                "parser_count": len(parsing_results.get("parsing_results", {})) if parsing_results else 0,
                "best_parser": parsing_results.get("summary", {}).get("best_parser") if parsing_results else None,
                "hierarchical_root": True
            }
        }
    
    def _analyze_document_structure(self, doc_id: str, structure_analysis: Dict[str, Any], 
                                   parsing_results: Dict[str, Any], domain: DocumentDomain) -> List[StructuralElement]:
        """ë¬¸ì„œ êµ¬ì¡° ë¶„ì„í•˜ì—¬ êµ¬ì¡° ìš”ì†Œ ëª©ë¡ ìƒì„±"""
        structural_elements = []
        
        # structure_analysis í˜•ì‹ í™•ì¸ ë° ë³€í™˜
        if not structure_analysis:
            self.logger.warning("êµ¬ì¡° ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŒ, ê¸°ë³¸ êµ¬ì¡° ìƒì„±")
            return self._create_default_structure(doc_id)
        
        # local_analysis.pyì—ì„œ ì˜¤ëŠ” í˜•ì‹ ì²˜ë¦¬: {"sections": [...], "tables_count": ...}
        if "sections" in structure_analysis:
            sections = structure_analysis.get("sections", [])
            if sections:
                self.logger.info(f"ğŸ“Š êµ¬ì¡° ë¶„ì„ ê²°ê³¼ í™œìš©: {len(sections)}ê°œ ì„¹ì…˜ ë°œê²¬")
                # ì„¹ì…˜ì„ StructuralElementë¡œ ë³€í™˜
                for i, section in enumerate(sections):
                    section_id = f"section_{doc_id}_{i}"
                    element = StructuralElement(
                        id=section_id,
                        type="Section",
                        properties={
                            "title": section.get("title", f"Section {i+1}"),
                            "level": section.get("level", 2),
                            "line": section.get("line"),
                            "parser": "local_analysis",
                            "index": i,
                            "number": section.get("number")
                        },
                        parent_id=doc_id,
                        content="",
                        position={"line": section.get("line")}
                    )
                    structural_elements.append(element)
                
                # í…Œì´ë¸” ì •ë³´ ì²˜ë¦¬
                tables_count = structure_analysis.get("tables_count", 0)
                if tables_count > 0:
                    table_id = f"table_{doc_id}_collection"
                    table_element = StructuralElement(
                        id=table_id,
                        type="Table",
                        properties={
                            "title": f"Tables ({tables_count})",
                            "count": tables_count,
                            "parser": "local_analysis",
                            "index": len(structural_elements)
                        },
                        parent_id=doc_id,
                        content="",
                        position={}
                    )
                    structural_elements.append(table_element)
                
                return structural_elements
        
        # ê¸°ì¡´ í˜•ì‹ ì²˜ë¦¬: {"structure_elements": [...], "summary": {...}}
        if not structure_analysis.get("structure_elements"):
            self.logger.warning("êµ¬ì¡° ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŒ, ê¸°ë³¸ êµ¬ì¡° ìƒì„±")
            return self._create_default_structure(doc_id)
        
        best_parser = structure_analysis.get("summary", {}).get("best_parser")
        if not best_parser:
            return structural_elements
        
        # íŒŒì„œë³„ êµ¬ì¡° ì •ë³´ì—ì„œ êµ¬ì¡° ìš”ì†Œ ì¶”ì¶œ
        if best_parser in parsing_results.get("parsing_results", {}):
            parser_result = parsing_results["parsing_results"][best_parser]
            
            if "structured_info" in parser_result:
                structured_info = parser_result["structured_info"]
                structural_elements = self._extract_structural_elements_from_parser(
                    doc_id, best_parser, structured_info, domain
                )
        
        self.logger.info(f"ğŸ“Š êµ¬ì¡° ìš”ì†Œ ë¶„ì„ ì™„ë£Œ: {len(structural_elements)}ê°œ ìš”ì†Œ")
        return structural_elements
    
    def _extract_structural_elements_from_parser(self, doc_id: str, parser_name: str, 
                                               structured_info: Dict[str, Any], 
                                               domain: DocumentDomain) -> List[StructuralElement]:
        """íŒŒì„œ ê²°ê³¼ì—ì„œ êµ¬ì¡° ìš”ì†Œ ì¶”ì¶œ"""
        elements = []
        
        # Document Structure ì²˜ë¦¬
        if "document_structure" in structured_info:
            doc_structure = structured_info["document_structure"]
            
            # ì„¹ì…˜ ì²˜ë¦¬
            sections = doc_structure.get("sections", [])
            for i, section in enumerate(sections):
                section_id = f"section_{doc_id}_{i}"
                element = StructuralElement(
                    id=section_id,
                    type="Section",
                    properties={
                        "title": section.get("title", f"Section {i+1}"),
                        "level": section.get("level", 1),
                        "line": section.get("line"),
                        "parser": parser_name,
                        "index": i
                    },
                    parent_id=doc_id,
                    content=section.get("content", ""),
                    position={"line": section.get("line")}
                )
                elements.append(element)
            
            # í…Œì´ë¸” ì²˜ë¦¬
            tables = doc_structure.get("tables", [])
            for i, table in enumerate(tables):
                table_id = f"table_{doc_id}_{i}"
                element = StructuralElement(
                    id=table_id,
                    type="Table",
                    properties={
                        "content": table.get("content", "")[:500],  # í…Œì´ë¸” ë‚´ìš© ì œí•œ
                        "page": table.get("page"),
                        "parser": parser_name,
                        "index": i,
                        "row_count": len(table.get("rows", [])),
                        "column_count": len(table.get("columns", []))
                    },
                    parent_id=doc_id,
                    content=table.get("content", ""),
                    position={"page": table.get("page")}
                )
                elements.append(element)
            
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            images = doc_structure.get("images", [])
            for i, image in enumerate(images):
                image_id = f"image_{doc_id}_{i}"
                element = StructuralElement(
                    id=image_id,
                    type="Image",
                    properties={
                        "caption": image.get("caption", ""),
                        "page": image.get("page"),
                        "parser": parser_name,
                        "index": i,
                        "width": image.get("width"),
                        "height": image.get("height")
                    },
                    parent_id=doc_id,
                    content=image.get("caption", ""),
                    position={"page": image.get("page")}
                )
                elements.append(element)
        
        # Docling íŒŒì„œì˜ ì¶”ê°€ êµ¬ì¡° ì •ë³´ ì²˜ë¦¬
        if parser_name == "docling":
            elements.extend(self._extract_docling_specific_elements(doc_id, structured_info))
        
        return elements
    
    def _extract_docling_specific_elements(self, doc_id: str, structured_info: Dict[str, Any]) -> List[StructuralElement]:
        """Docling íŒŒì„œ íŠ¹í™” êµ¬ì¡° ìš”ì†Œ ì¶”ì¶œ"""
        elements = []
        
        # Doclingì˜ ìƒì„¸ êµ¬ì¡° ì •ë³´ ì²˜ë¦¬
        if "detailed_structure" in structured_info:
            detailed = structured_info["detailed_structure"]
            
            # ë‹¨ë½ ì²˜ë¦¬
            paragraphs = detailed.get("paragraphs", [])
            for i, para in enumerate(paragraphs):
                para_id = f"paragraph_{doc_id}_{i}"
                element = StructuralElement(
                    id=para_id,
                    type="Paragraph",
                    properties={
                        "text": para.get("text", "")[:200],  # ë‹¨ë½ í…ìŠ¤íŠ¸ ì œí•œ
                        "page": para.get("page"),
                        "parser": "docling",
                        "index": i,
                        "word_count": len(para.get("text", "").split())
                    },
                    parent_id=doc_id,
                    content=para.get("text", ""),
                    position={"page": para.get("page")}
                )
                elements.append(element)
            
            # ëª©ë¡ ì²˜ë¦¬
            lists = detailed.get("lists", [])
            for i, list_item in enumerate(lists):
                list_id = f"list_{doc_id}_{i}"
                element = StructuralElement(
                    id=list_id,
                    type="List",
                    properties={
                        "items": list_item.get("items", [])[:10],  # ëª©ë¡ í•­ëª© ì œí•œ
                        "list_type": list_item.get("type", "unordered"),
                        "page": list_item.get("page"),
                        "parser": "docling",
                        "index": i,
                        "item_count": len(list_item.get("items", []))
                    },
                    parent_id=doc_id,
                    content="\n".join(list_item.get("items", [])),
                    position={"page": list_item.get("page")}
                )
                elements.append(element)
        
        return elements
    
    def _create_default_structure(self, doc_id: str) -> List[StructuralElement]:
        """ê¸°ë³¸ êµ¬ì¡° ìš”ì†Œ ìƒì„± (êµ¬ì¡° ë¶„ì„ ê²°ê³¼ê°€ ì—†ì„ ë•Œ)"""
        return [
            StructuralElement(
                id=f"default_section_{doc_id}",
                type="Section",
                properties={
                    "title": "Default Section",
                    "level": 1,
                    "parser": "default",
                    "index": 0
                },
                parent_id=doc_id,
                content=""
            )
        ]
    
    def _create_structure_entities_and_relationships(self, doc_id: str, structural_elements: List[StructuralElement], 
                                                   domain: DocumentDomain) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """êµ¬ì¡° ìš”ì†Œë¥¼ KG ì—”í‹°í‹°ì™€ ê´€ê³„ë¡œ ë³€í™˜"""
        entities = []
        relationships = []
        
        for element in structural_elements:
            # êµ¬ì¡° ì—”í‹°í‹° ìƒì„±
            structure_entity = {
                "id": element.id,
                "type": element.type,
                "properties": {
                    **element.properties,
                    "domain": domain.value,
                    "structural_element": True,
                    "content_preview": element.content[:100] if element.content else "",
                    "has_content": bool(element.content)
                }
            }
            entities.append(structure_entity)
            
            # ë¶€ëª¨-ìì‹ ê´€ê³„ ìƒì„±
            if element.parent_id:
                relationship = {
                    "source": element.parent_id,
                    "target": element.id,
                    "type": "CONTAINS_STRUCTURE",
                    "properties": {
                        "structure_type": element.type,
                        "relationship_name": f"CONTAINS_{element.type.upper()}",
                        "position_info": element.position or {},
                        "domain": domain.value
                    }
                }
                relationships.append(relationship)
        
        return entities, relationships
    
    def _create_hierarchical_keyword_entities(self, doc_id: str, keywords: Dict[str, Any], 
                                            structural_elements: List[StructuralElement], 
                                            document_text: str, domain: DocumentDomain) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """í‚¤ì›Œë“œë¥¼ êµ¬ì¡° ìš”ì†Œë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ ê³„ì¸µì  ì—”í‹°í‹° ìƒì„±"""
        entities = []
        relationships = []
        domain_schema = self.schema_manager.get_domain_schema(domain)
        
        # êµ¬ì¡° ìš”ì†Œë³„ ì»¨í…ìŠ¤íŠ¸ ë§¤í•‘ ìƒì„±
        structure_contexts = self._build_structure_context_map(structural_elements, document_text)
        
        for extractor_name, extractor_keywords in keywords.items():
            for kw_data in extractor_keywords:
                keyword = kw_data.get("keyword", "")
                if not keyword or len(keyword) < 2:
                    continue
                
                # í‚¤ì›Œë“œë¥¼ ë‹¨ì–´ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„í•´
                word_keywords = self._extract_words_from_keyword(keyword)
                
                for word_keyword in word_keywords:
                    if len(word_keyword) < 2:
                        continue
                    
                    # ì´ í‚¤ì›Œë“œê°€ ì†í•œ êµ¬ì¡° ìš”ì†Œ ì°¾ê¸°
                    containing_structures = self._find_containing_structures(
                        word_keyword, structure_contexts, kw_data
                    )
                    
                    # ë„ë©”ì¸ë³„ ì—”í‹°í‹° íƒ€ì… ê²°ì •
                    entity_type = self.schema_manager._classify_keyword_to_entity_type(
                        word_keyword, domain_schema["entities"]
                    )
                    
                    # ê° êµ¬ì¡° ìš”ì†Œë³„ë¡œ í‚¤ì›Œë“œ ì—”í‹°í‹° ìƒì„±
                    for structure_id, context_info in containing_structures:
                        # ê³ ìœ  ID ìƒì„± (í‚¤ì›Œë“œ + êµ¬ì¡° ìš”ì†Œ + ì¶”ì¶œê¸°)
                        kw_entity_id = f"{entity_type.lower()}_{_hash(word_keyword)}_{_hash(structure_id)}_{extractor_name}"
                        
                        # ì¶”ê°€ ì†ì„± ìƒì„±
                        additional_props = self.schema_manager._get_additional_entity_properties(
                            word_keyword, entity_type, domain
                        )
                        
                        # í‚¤ì›Œë“œ ì—”í‹°í‹° ìƒì„±
                        keyword_entity = {
                            "id": kw_entity_id,
                            "type": entity_type,
                            "properties": {
                                "text": word_keyword,
                                "domain": domain.value,
                                "source_structure": structure_id,
                                "source_structure_type": context_info.get("structure_type"),
                                "extractor": extractor_name,
                                "score": kw_data.get("score", 0),
                                "category": kw_data.get("category", "unknown"),
                                "extraction_context": context_info,
                                "hierarchical_entity": True,
                                **additional_props
                            }
                        }
                        entities.append(keyword_entity)
                        
                        # êµ¬ì¡° ìš”ì†Œ â†’ í‚¤ì›Œë“œ ì—”í‹°í‹° ê´€ê³„
                        context_snippet = context_info.get("context", "")
                        base_rel_type, specific_rel_type = self.schema_manager.get_enhanced_relationship_type(
                            context_info.get("structure_type", "Structure"), entity_type, context_snippet, domain
                        )
                        
                        structure_relationship = {
                            "source": structure_id,
                            "target": kw_entity_id,
                            "type": base_rel_type,
                            "properties": {
                                "relationship_name": specific_rel_type,
                                "extraction_method": extractor_name,
                                "confidence_score": kw_data.get("score", 0),
                                "context_snippet": context_snippet[:100],
                                "domain": domain.value,
                                "hierarchical_relationship": True
                            }
                        }
                        relationships.append(structure_relationship)
        
        self.logger.info(f"ğŸ”— ê³„ì¸µì  í‚¤ì›Œë“œ ì—”í‹°í‹° ìƒì„±: {len(entities)}ê°œ ì—”í‹°í‹°, {len(relationships)}ê°œ ê´€ê³„")
        return entities, relationships
    
    def _build_structure_context_map(self, structural_elements: List[StructuralElement], 
                                   document_text: str) -> Dict[str, Dict[str, Any]]:
        """êµ¬ì¡° ìš”ì†Œë³„ ì»¨í…ìŠ¤íŠ¸ ë§¤í•‘ ìƒì„±"""
        context_map = {}
        
        for element in structural_elements:
            context_map[element.id] = {
                "structure_type": element.type,
                "content": element.content,
                "properties": element.properties,
                "position": element.position or {},
                "context": element.content[:200] if element.content else ""
            }
        
        return context_map
    
    def _find_containing_structures(self, keyword: str, structure_contexts: Dict[str, Dict[str, Any]], 
                                  kw_data: Dict[str, Any]) -> List[Tuple[str, Dict[str, Any]]]:
        """í‚¤ì›Œë“œê°€ í¬í•¨ëœ êµ¬ì¡° ìš”ì†Œë“¤ ì°¾ê¸°"""
        containing_structures = []
        keyword_lower = keyword.lower()
        
        # í‚¤ì›Œë“œ ìœ„ì¹˜ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ì •í™•í•œ ë§¤í•‘
        if kw_data.get("start_position") is not None:
            # TODO: ìœ„ì¹˜ ê¸°ë°˜ ì •í™•í•œ êµ¬ì¡° ë§¤í•‘ êµ¬í˜„
            pass
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë§¤ì¹­
        for structure_id, context_info in structure_contexts.items():
            content = context_info.get("content", "").lower()
            if keyword_lower in content:
                containing_structures.append((structure_id, context_info))
        
        # ë§¤ì¹­ë˜ëŠ” êµ¬ì¡°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ êµ¬ì¡°ì— í• ë‹¹
        if not containing_structures and structure_contexts:
            default_structure = list(structure_contexts.items())[0]
            containing_structures.append(default_structure)
        
        return containing_structures
    
    def _extract_words_from_keyword(self, keyword: str) -> List[str]:
        """í‚¤ì›Œë“œì—ì„œ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë“¤ ì¶”ì¶œ"""
        import re
        
        # ë‹¨ìˆœí•œ ê²½ìš°ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜
        if len(keyword.split()) <= 2:
            cleaned = self._remove_korean_particles(keyword.strip())
            return [cleaned] if cleaned else []
        
        # ë³µì¡í•œ í‚¤ì›Œë“œëŠ” ë‹¨ì–´ë³„ë¡œ ë¶„í•´
        words = []
        raw_words = re.split(r'[^\wê°€-í£]+', keyword)
        
        for word in raw_words:
            word = word.strip()
            if len(word) >= 2:
                cleaned_word = self._remove_korean_particles(word)
                if cleaned_word and not self._is_stop_word(cleaned_word):
                    words.append(cleaned_word)
        
        return words if words else [self._remove_korean_particles(keyword.strip())]
    
    def _remove_korean_particles(self, word: str) -> str:
        """í•œêµ­ì–´ ì¡°ì‚¬ ì œê±°"""
        if not word:
            return word
        
        particles = [
            'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ì—ê²Œ', 'í•œí…Œ', 'ìœ¼ë¡œ', 'ë¡œ',
            'ì™€', 'ê³¼', 'í•˜ê³ ', 'ì´ë‘', 'ë‘', 'ì•„', 'ì•¼', 'ì´ì—¬', 'ì—¬',
            'ì€', 'ëŠ”', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì¡°ì°¨', 'ë§ˆì €', 'ë°–ì—', 'ë¿'
        ]
        
        particles.sort(key=len, reverse=True)
        original_word = word
        
        for particle in particles:
            if word.endswith(particle) and len(word) > len(particle):
                candidate = word[:-len(particle)]
                if len(candidate) >= 2:
                    word = candidate
                    break
        
        return word if len(word) >= 2 else (original_word if len(original_word) >= 2 else "")
    
    def _is_stop_word(self, word: str) -> bool:
        """ë¶ˆìš©ì–´ í™•ì¸"""
        korean_stops = {'ì´ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ë‹¤', 'í•˜ë‹¤', 'ê·¸ê²ƒ', 'ê²ƒì´'}
        english_stops = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        return word.lower() in korean_stops or word.lower() in english_stops
    
    def _build_structural_hierarchy(self, structural_elements: List[StructuralElement]) -> List[Dict[str, Any]]:
        """êµ¬ì¡°ì  ê³„ì¸µ ì •ë³´ êµ¬ì¶•"""
        hierarchy = []
        
        for element in structural_elements:
            hierarchy_item = {
                "id": element.id,
                "type": element.type,
                "parent_id": element.parent_id,
                "children": element.children or [],
                "level": self._calculate_hierarchy_level(element, structural_elements),
                "properties": element.properties
            }
            hierarchy.append(hierarchy_item)
        
        return sorted(hierarchy, key=lambda x: (x.get("level", 0), x.get("id", "")))
    
    def _calculate_hierarchy_level(self, element: StructuralElement, all_elements: List[StructuralElement]) -> int:
        """êµ¬ì¡° ìš”ì†Œì˜ ê³„ì¸µ ë ˆë²¨ ê³„ì‚°"""
        level = 0
        current_parent = element.parent_id
        
        while current_parent:
            level += 1
            parent_element = next((e for e in all_elements if e.id == current_parent), None)
            if parent_element:
                current_parent = parent_element.parent_id
            else:
                break
        
        return level
    
    def _save_to_memgraph(self, result: Dict[str, Any], file_path: str):
        """Memgraphì— ê³„ì¸µì  KG ë°ì´í„° ì €ì¥"""
        try:
            self.logger.info(f"ğŸ’¾ ê³„ì¸µì  KGë¥¼ Memgraphì— ì €ì¥ ì¤‘: {file_path}")
            
            success = self.memgraph_service.insert_kg_data(result, clear_existing=True)
            if success:
                result["metadata"]["memgraph_saved"] = True
                result["metadata"]["memgraph_saved_at"] = self._get_timestamp()
                self.logger.info(f"âœ… ê³„ì¸µì  KG Memgraph ì €ì¥ ì™„ë£Œ: {file_path}")
            else:
                result["metadata"]["memgraph_saved"] = False
                self.logger.warning(f"âš ï¸ ê³„ì¸µì  KG Memgraph ì €ì¥ ì‹¤íŒ¨: {file_path}")
        except Exception as e:
            self.logger.error(f"âŒ ê³„ì¸µì  KG Memgraph ì €ì¥ ì˜¤ë¥˜: {e}")
            result["metadata"]["memgraph_saved"] = False
            result["metadata"]["memgraph_error"] = str(e)
    
    def _extract_entity_to_entity_relationships(self, entities: List[Dict[str, Any]], document_text: str, 
                                              domain: DocumentDomain, structural_elements: List[StructuralElement]) -> List[Dict[str, Any]]:
        """ì—”í‹°í‹° ê°„ ì§ì ‘ ê´€ê³„ ì¶”ì¶œ"""
        relationships = []
        
        # hierarchical_entity=Trueì¸ í‚¤ì›Œë“œ ì—”í‹°í‹°ë§Œ í•„í„°ë§
        keyword_entities = [
            entity for entity in entities 
            if entity.get("properties", {}).get("hierarchical_entity") == True
        ]
        
        if len(keyword_entities) < 2:
            self.logger.info("ğŸ”— ì—”í‹°í‹° ê´€ê³„ ì¶”ì¶œ: ê´€ê³„ë¥¼ í˜•ì„±í•  ì—”í‹°í‹°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤")
            return relationships
        
        self.logger.info(f"ğŸ”— ì—”í‹°í‹° ê°„ ê´€ê³„ ì¶”ì¶œ ì‹œì‘: {len(keyword_entities)}ê°œ ì—”í‹°í‹° ë¶„ì„")
        
        # 1. ë™ì¼ êµ¬ì¡° ë‚´ ì—”í‹°í‹° í˜ì–´ ì°¾ê¸°
        structure_entity_pairs = self._find_structure_based_entity_pairs(keyword_entities)
        
        # 2. ê° í˜ì–´ì— ëŒ€í•´ ê´€ê³„ ì¶”ë¡ 
        for entity1, entity2, shared_context in structure_entity_pairs:
            relationship = self._infer_entity_relationship(entity1, entity2, shared_context, domain)
            if relationship:
                relationships.append(relationship)
        
        # 3. ë„ë©”ì¸ë³„ íŠ¹ìˆ˜ ê´€ê³„ ì¶”ë¡ 
        domain_relationships = self._infer_domain_specific_relationships(keyword_entities, domain, document_text)
        relationships.extend(domain_relationships)
        
        # 4. LLM ê¸°ë°˜ ê³ ê¸‰ ê´€ê³„ ì¶”ë¡  (ì„ íƒì )
        llm_relationships = self._llm_based_relationship_extraction(keyword_entities, document_text, domain)
        relationships.extend(llm_relationships)
        
        self.logger.info(f"âœ… ì—”í‹°í‹° ê´€ê³„ ì¶”ì¶œ ì™„ë£Œ: {len(relationships)}ê°œ ì§ì ‘ ê´€ê³„ ë°œê²¬")
        return relationships
    
    def _find_structure_based_entity_pairs(self, keyword_entities: List[Dict[str, Any]]) -> List[Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]]:
        """êµ¬ì¡° ê¸°ë°˜ ì—”í‹°í‹° í˜ì–´ ì°¾ê¸° - ì˜ë¯¸ìˆëŠ” ê´€ê³„ë§Œ ì„ íƒ"""
        entity_pairs = []
        
        # êµ¬ì¡°ë³„ë¡œ ì—”í‹°í‹° ê·¸ë£¹í™”
        structure_entities = {}
        for entity in keyword_entities:
            source_structure = entity.get("properties", {}).get("source_structure")
            if source_structure:
                if source_structure not in structure_entities:
                    structure_entities[source_structure] = []
                structure_entities[source_structure].append(entity)
        
        # ê° êµ¬ì¡° ë‚´ì—ì„œ ì˜ë¯¸ìˆëŠ” ì—”í‹°í‹° í˜ì–´ë§Œ ìƒì„±
        for structure_id, entities in structure_entities.items():
            if len(entities) < 2:
                continue
            
            # ì—”í‹°í‹°ë¥¼ íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
            entities_by_type = {}
            for entity in entities:
                entity_type = entity.get("type", "Unknown")
                if entity_type not in entities_by_type:
                    entities_by_type[entity_type] = []
                entities_by_type[entity_type].append(entity)
            
            # ì˜ë¯¸ìˆëŠ” ê´€ê³„ íŒ¨í„´ë§Œ ì„ íƒì ìœ¼ë¡œ í˜ì–´ë§
            meaningful_pairs = self._select_meaningful_pairs(entities_by_type, structure_id)
            entity_pairs.extend(meaningful_pairs)
        
        self.logger.info(f"ğŸ” êµ¬ì¡° ê¸°ë°˜ ì—”í‹°í‹° í˜ì–´ ë°œê²¬: {len(entity_pairs)}ê°œ")
        return entity_pairs
    
    def _select_meaningful_pairs(self, entities_by_type: Dict[str, List[Dict]], structure_id: str) -> List[Tuple]:
        """ì˜ë¯¸ìˆëŠ” ì—”í‹°í‹° í˜ì–´ë§Œ ì„ íƒ - ë¬¸ì„œì—ì„œ ì‹¤ì œ ì–¸ê¸‰ëœ ê´€ê³„ë§Œ"""
        pairs = []
        
        # ëª¨ë“  ì—”í‹°í‹° ëª©ë¡
        all_entities = []
        for entities in entities_by_type.values():
            all_entities.extend(entities)
        
        # ë¬¸ì„œ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œë¡œ ì–¸ê¸‰ëœ ê´€ê³„ë§Œ ì°¾ê¸°
        for i, entity1 in enumerate(all_entities):
            for j, entity2 in enumerate(all_entities):
                if i >= j:  # ì¤‘ë³µ ë°©ì§€
                    continue
                
                # ë¬¸ì„œì—ì„œ ì‹¤ì œ ê´€ê³„ê°€ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if self._check_actual_relationship_in_text(entity1, entity2):
                    shared_context = {
                        "structure_id": structure_id,
                        "structure_type": entity1.get("properties", {}).get("source_structure_type"),
                        "context": self._find_relationship_context(entity1, entity2),
                        "extractor": entity1.get("properties", {}).get("extractor")
                    }
                    pairs.append((entity1, entity2, shared_context))
        
        return pairs
    
    def _check_actual_relationship_in_text(self, entity1: Dict, entity2: Dict) -> bool:
        """ë¬¸ì„œì—ì„œ ë‘ ì—”í‹°í‹° ê°„ ì‹¤ì œ ê´€ê³„ê°€ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        text1 = entity1.get("properties", {}).get("text", "").lower()
        text2 = entity2.get("properties", {}).get("text", "").lower()
        
        # ë‘ ì—”í‹°í‹°ê°€ ê°€ê¹Œìš´ ê±°ë¦¬ì— í•¨ê»˜ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
        return self._are_entities_mentioned_together(text1, text2)
    
    def _are_entities_mentioned_together(self, entity1_text: str, entity2_text: str) -> bool:
        """ë‘ ì—”í‹°í‹°ê°€ ë¬¸ì„œì—ì„œ ê°€ê¹Œìš´ ê±°ë¦¬ì— í•¨ê»˜ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        # ì›ë³¸ ë¬¸ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (selfì— ì €ì¥ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©)
        if not hasattr(self, '_current_document_text'):
            return False
        
        doc_text = self._current_document_text.lower()
        
        # ë‘ ì—”í‹°í‹° ìœ„ì¹˜ ì°¾ê¸°
        entity1_positions = []
        entity2_positions = []
        
        # entity1 ëª¨ë“  ìœ„ì¹˜ ì°¾ê¸°
        start = 0
        while True:
            pos = doc_text.find(entity1_text, start)
            if pos == -1:
                break
            entity1_positions.append(pos)
            start = pos + 1
        
        # entity2 ëª¨ë“  ìœ„ì¹˜ ì°¾ê¸°
        start = 0
        while True:
            pos = doc_text.find(entity2_text, start)
            if pos == -1:
                break
            entity2_positions.append(pos)
            start = pos + 1
        
        # ë” ì—„ê²©í•œ ê¸°ì¤€: ê°™ì€ ë¬¸ì¥ì´ë‚˜ ë‹¨ë½ì—ì„œ ì–¸ê¸‰ (100ë¬¸ì ì´ë‚´)
        proximity_threshold = 100
        
        for pos1 in entity1_positions:
            for pos2 in entity2_positions:
                distance = abs(pos1 - pos2)
                if distance <= proximity_threshold:
                    # ì¶”ê°€ ê²€ì¦: ê°™ì€ ë¬¸ì¥ì´ë‚˜ ëª©ë¡ì— ìˆëŠ”ì§€ í™•ì¸
                    if self._are_in_same_context_unit(pos1, pos2, doc_text):
                        return True
        
        return False
    
    def _are_in_same_context_unit(self, pos1: int, pos2: int, text: str) -> bool:
        """ë‘ ìœ„ì¹˜ê°€ ê°™ì€ ì»¨í…ìŠ¤íŠ¸ ë‹¨ìœ„(ë¬¸ì¥, ëª©ë¡ í•­ëª©)ì— ìˆëŠ”ì§€ í™•ì¸"""
        # ë‘ ìœ„ì¹˜ ì‚¬ì´ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        start_pos = min(pos1, pos2)
        end_pos = max(pos1, pos2)
        between_text = text[start_pos:end_pos]
        
        # ë¬¸ì¥ êµ¬ë¶„ìë‚˜ ëª©ë¡ êµ¬ë¶„ìê°€ ìˆìœ¼ë©´ ë‹¤ë¥¸ ì»¨í…ìŠ¤íŠ¸
        separators = ['\n\n', '. ', '.\n', '|', '- **', '### ', '## ']
        
        for separator in separators:
            if separator in between_text:
                return False
        
        return True
    
    def _find_relationship_context(self, entity1: Dict, entity2: Dict) -> str:
        """ë‘ ì—”í‹°í‹° ê°„ ê´€ê³„ì˜ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì°¾ê¸°"""
        if not hasattr(self, '_current_document_text'):
            return ""
        
        text1 = entity1.get("properties", {}).get("text", "")
        text2 = entity2.get("properties", {}).get("text", "")
        doc_text = self._current_document_text
        
        # ë¬¸ì„œì—ì„œ ë‘ ì—”í‹°í‹°ê°€ ê°€ì¥ ê°€ê¹Œì´ ì–¸ê¸‰ëœ ìœ„ì¹˜ ì°¾ê¸°
        min_distance = float('inf')
        best_context = ""
        
        # ê° ì—”í‹°í‹°ì˜ ëª¨ë“  ì–¸ê¸‰ ìœ„ì¹˜ ì°¾ê¸°
        positions1 = self._find_all_positions(doc_text.lower(), text1.lower())
        positions2 = self._find_all_positions(doc_text.lower(), text2.lower())
        
        # ê°€ì¥ ê°€ê¹Œìš´ ìœ„ì¹˜ ì¡°í•© ì°¾ê¸°
        for pos1 in positions1:
            for pos2 in positions2:
                distance = abs(pos1 - pos2)
                if distance < min_distance:
                    min_distance = distance
                    # ë‘ ì—”í‹°í‹°ë¥¼ í¬í•¨í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    start_pos = max(0, min(pos1, pos2) - 100)
                    end_pos = min(len(doc_text), max(pos1 + len(text1), pos2 + len(text2)) + 100)
                    best_context = doc_text[start_pos:end_pos].strip()
        
        return best_context[:200] + "..." if len(best_context) > 200 else best_context
    
    def _find_all_positions(self, text: str, keyword: str) -> List[int]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œì˜ ëª¨ë“  ìœ„ì¹˜ ì°¾ê¸°"""
        positions = []
        start = 0
        while True:
            pos = text.find(keyword, start)
            if pos == -1:
                break
            positions.append(pos)
            start = pos + 1
        return positions
    
    def _are_complementary_technologies(self, tech1: Dict, tech2: Dict) -> bool:
        """ë‘ ê¸°ìˆ ì´ ë³´ì™„ì  ê´€ê³„ì¸ì§€ í™•ì¸"""
        text1 = tech1.get("properties", {}).get("text", "").lower()
        text2 = tech2.get("properties", {}).get("text", "").lower()
        
        # ë³´ì™„ì  ê¸°ìˆ  íŒ¨í„´
        complementary_patterns = [
            ("fastapi", "sqlalchemy"),     # ì›¹ í”„ë ˆì„ì›Œí¬ì™€ ORM
            ("redis", "postgresql"),        # ìºì‹œì™€ ì£¼ ë°ì´í„°ë² ì´ìŠ¤
            ("elasticsearch", "postgresql"), # ê²€ìƒ‰ ì—”ì§„ê³¼ ì£¼ ë°ì´í„°ë² ì´ìŠ¤
            ("fastapi", "pydantic"),        # ì›¹ í”„ë ˆì„ì›Œí¬ì™€ ê²€ì¦ ë¼ì´ë¸ŒëŸ¬ë¦¬
        ]
        
        for pattern1, pattern2 in complementary_patterns:
            if (pattern1 in text1 and pattern2 in text2) or (pattern2 in text1 and pattern1 in text2):
                return True
        
        return False
    
    def _infer_entity_relationship(self, entity1: Dict[str, Any], entity2: Dict[str, Any], 
                                 shared_context: Dict[str, Any], domain: DocumentDomain) -> Optional[Dict[str, Any]]:
        """ë‘ ì—”í‹°í‹° ê°„ì˜ ê´€ê³„ ì¶”ë¡  - ì˜ë¯¸ìˆëŠ” ê´€ê³„ë§Œ"""
        type1 = entity1.get("type")
        type2 = entity2.get("type")
        text1 = entity1.get("properties", {}).get("text", "")
        text2 = entity2.get("properties", {}).get("text", "")
        
        if not all([type1, type2, text1, text2]):
            return None
        
        # ë„ë©”ì¸ë³„ ê´€ê³„ ê·œì¹™ ì ìš©
        relationship_type = self._get_domain_relationship_type(type1, type2, text1, text2, domain, shared_context)
        
        # ê´€ê³„ê°€ ì—†ê±°ë‚˜ ë„ˆë¬´ ì¼ë°˜ì ì¸ ê²½ìš° í•„í„°ë§
        if not relationship_type or relationship_type == "ASSOCIATED_WITH":
            return None
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        confidence_score = self._calculate_relationship_confidence(
            entity1, entity2, relationship_type, shared_context
        )
        
        # ë‚®ì€ ì‹ ë¢°ë„ ê´€ê³„ í•„í„°ë§ (ì„ê³„ê°’: 0.5)
        if confidence_score < 0.5:
            return None
        
        return {
            "source": entity1["id"],
            "target": entity2["id"], 
            "type": "RELATED_TO",
            "properties": {
                "relationship_name": relationship_type,
                "domain": domain.value,
                "context_snippet": shared_context.get("context", "")[:100],
                "extraction_method": shared_context.get("extractor"),
                "confidence_score": confidence_score,
                "entity_to_entity": True,
                "inferred": True
            }
        }
    
    def _calculate_relationship_confidence(self, entity1: Dict, entity2: Dict, 
                                         relationship_type: str, context: Dict) -> float:
        """ê´€ê³„ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        # ê¸°ë³¸ ì ìˆ˜: ë‘ ì—”í‹°í‹°ì˜ ìµœì†Œ ì ìˆ˜
        base_score = min(
            entity1.get("properties", {}).get("score", 0),
            entity2.get("properties", {}).get("score", 0)
        )
        
        # ê´€ê³„ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
        relationship_weights = {
            "USES": 0.9,
            "INTEGRATES_WITH": 0.85,
            "IMPLEMENTS": 0.8,
            "CONNECTS_TO": 0.8,
            "COMPLEMENTS": 0.75,
            "SUPPORTS": 0.7,
            "RELATED_TO": 0.5,  # ì¼ë°˜ì  ê´€ê³„ëŠ” ë‚®ì€ ê°€ì¤‘ì¹˜
        }
        
        weight = relationship_weights.get(relationship_type, 0.6)
        
        # ì»¨í…ìŠ¤íŠ¸ ë³´ë„ˆìŠ¤ (ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì ìˆ˜ ì¦ê°€)
        context_bonus = 0.1 if context.get("context") else 0
        
        return min(base_score * weight + context_bonus, 1.0)
    
    def _get_domain_relationship_type(self, type1: str, type2: str, text1: str, text2: str, 
                                    domain: DocumentDomain, context: Dict[str, Any]) -> Optional[str]:
        """ë„ë©”ì¸ë³„ ê´€ê³„ íƒ€ì… ê²°ì •"""
        
        # ê¸°ìˆ  ë„ë©”ì¸ ê´€ê³„ ê·œì¹™ - ì˜ë¯¸ìˆëŠ” ê´€ê³„ë§Œ
        if domain == DocumentDomain.TECHNICAL:
            # íŠ¹ì • íƒ€ì… ê°„ ì§ì ‘ì ì¸ ê´€ê³„ë§Œ í—ˆìš©
            tech_rules = {
                ("Technology", "Database"): "USES",
                ("Framework", "Database"): "CONNECTS_TO", 
                ("Technology", "Framework"): "INTEGRATES_WITH",
                ("API", "Database"): "ACCESSES",
                ("Service", "Technology"): "IMPLEMENTS",
                ("Tool", "Framework"): "SUPPORTS"
            }
            
            # ì–‘ë°©í–¥ í™•ì¸
            direct = tech_rules.get((type1, type2))
            if direct:
                return direct
            
            reverse = tech_rules.get((type2, type1))
            if reverse:
                # ì—­ë°©í–¥ ê´€ê³„ ë§¤í•‘
                reverse_mapping = {
                    "USES": "USED_BY",
                    "CONNECTS_TO": "CONNECTED_BY", 
                    "INTEGRATES_WITH": "INTEGRATED_BY",
                    "ACCESSES": "ACCESSED_BY",
                    "IMPLEMENTS": "IMPLEMENTED_BY",
                    "SUPPORTS": "SUPPORTED_BY"
                }
                return reverse_mapping.get(reverse, reverse)
            
            # ëª…ì‹œì ìœ¼ë¡œ ì •ì˜ë˜ì§€ ì•Šì€ ê´€ê³„ëŠ” None ë°˜í™˜
            return None
        
        # í•™ìˆ  ë„ë©”ì¸ ê´€ê³„ ê·œì¹™
        elif domain == DocumentDomain.ACADEMIC:
            academic_rules = {
                ("Author", "Research_Method"): "USES",
                ("Theory", "Research_Method"): "APPLIED_BY",
                ("Concept", "Theory"): "RELATED_TO",
                ("Dataset", "Research_Method"): "ANALYZED_BY"
            }
            
            return academic_rules.get((type1, type2)) or academic_rules.get((type2, type1))
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸ ê´€ê³„ ê·œì¹™  
        elif domain == DocumentDomain.BUSINESS:
            business_rules = {
                ("Company", "Product"): "PRODUCES",
                ("Strategy", "Goal"): "ACHIEVES",
                ("Process", "Tool"): "UTILIZES"
            }
            
            return business_rules.get((type1, type2)) or business_rules.get((type2, type1))
        
        # ê¸°ë³¸ ê´€ê³„ (ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜)
        context_text = context.get("context", "").lower()
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê´€ê³„ íŒíŠ¸ ì°¾ê¸°
        if any(word in context_text for word in ["ì‚¬ìš©", "í™œìš©", "ì´ìš©"]):
            return "USES"
        elif any(word in context_text for word in ["ì—°ê²°", "ì—°ë™", "í†µí•©"]):
            return "INTEGRATES_WITH" 
        elif any(word in context_text for word in ["êµ¬í˜„", "ê°œë°œ"]):
            return "IMPLEMENTS"
        elif any(word in context_text for word in ["ì§€ì›", "ë„êµ¬"]):
            return "SUPPORTS"
        
        return "ASSOCIATED_WITH"  # ê¸°ë³¸ ê´€ê³„
    
    def _infer_domain_specific_relationships(self, keyword_entities: List[Dict[str, Any]], 
                                           domain: DocumentDomain, document_text: str) -> List[Dict[str, Any]]:
        """ë„ë©”ì¸ë³„ íŠ¹ìˆ˜ ê´€ê³„ ì¶”ë¡ """
        relationships = []
        
        if domain == DocumentDomain.TECHNICAL:
            # ê¸°ìˆ  ìŠ¤íƒ ê´€ê³„ ì¶”ë¡ 
            tech_entities = [e for e in keyword_entities if e.get("type") in ["Technology", "Framework", "Database", "Tool"]]
            
            # ì˜ ì•Œë ¤ì§„ ê¸°ìˆ  ì¡°í•© íŒ¨í„´
            known_patterns = [
                (["FastAPI", "SQLAlchemy"], "INTEGRATES_WITH"),
                (["React", "Node.js"], "WORKS_WITH"),
                (["PostgreSQL", "Redis"], "COMPLEMENTS"),
                (["Docker", "Kubernetes"], "ORCHESTRATED_BY")
            ]
            
            for pattern, rel_type in known_patterns:
                entities_in_pattern = []
                for entity in tech_entities:
                    entity_text = entity.get("properties", {}).get("text", "")
                    if any(pattern_item.lower() in entity_text.lower() for pattern_item in pattern):
                        entities_in_pattern.append(entity)
                
                # íŒ¨í„´ì— ë§ëŠ” ì—”í‹°í‹°ë“¤ ê°„ ê´€ê³„ ìƒì„±
                for i in range(len(entities_in_pattern)):
                    for j in range(i + 1, len(entities_in_pattern)):
                        relationship = {
                            "source": entities_in_pattern[i]["id"],
                            "target": entities_in_pattern[j]["id"],
                            "type": "RELATED_TO",
                            "properties": {
                                "relationship_name": rel_type,
                                "domain": domain.value,
                                "pattern_based": True,
                                "confidence_score": 0.8,
                                "entity_to_entity": True,
                                "inferred": True
                            }
                        }
                        relationships.append(relationship)
        
        return relationships
    
    def _llm_based_relationship_extraction(self, keyword_entities: List[Dict[str, Any]], 
                                         document_text: str, domain: DocumentDomain) -> List[Dict[str, Any]]:
        """LLM ê¸°ë°˜ ì—”í‹°í‹° ê´€ê³„ ì¶”ì¶œ"""
        relationships = []
        
        if not self.llm_service or not self.llm_service.is_loaded:
            self.logger.info("ğŸ¤– LLM ì„œë¹„ìŠ¤ê°€ ì—†ì–´ LLM ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤")
            return relationships
        
        self.logger.info(f"ğŸ¤– LLM ê¸°ë°˜ ì—”í‹°í‹° ê´€ê³„ ì¶”ì¶œ ì‹œì‘")
        
        entity_map = {e['id']: e for e in keyword_entities}
        
        # 1. ì—”í‹°í‹°ê°€ ì–¸ê¸‰ëœ ì»¨í…ìŠ¤íŠ¸(ë¬¸ì¥) ì¶”ì¶œ
        entity_contexts = self._get_entity_contexts(keyword_entities, document_text)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ê³„ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._create_relationship_extraction_prompt(entity_contexts, domain)
        
        if not prompt:
            self.logger.info("ğŸ¤– ê´€ê³„ë¥¼ ì¶”ì¶œí•  ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ì—¬ LLM í˜¸ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return relationships
            
        try:
            if hasattr(self.llm_service, 'ollama_client') and self.llm_service.ollama_client:
                llm_response = self.llm_service.ollama_client.invoke(prompt)
                
                if llm_response:
                    parsed_relationships = self._parse_llm_relationship_response(
                        llm_response, entity_map, domain
                    )
                    relationships.extend(parsed_relationships)
                    self.logger.info(f"ğŸ¤– LLMì´ {len(parsed_relationships)}ê°œì˜ ì—”í‹°í‹° ê´€ê³„ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤")
            else:
                self.logger.warning("ğŸ¤– LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
        except Exception as e:
            self.logger.warning(f"ğŸ¤– LLM ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return relationships

    def _get_entity_contexts(self, entities: List[Dict[str, Any]], document_text: str, window_size: int = 1) -> Dict[str, Any]:
        """ë¬¸ì„œì—ì„œ ì—”í‹°í‹°ê°€ ì–¸ê¸‰ëœ ë¬¸ì¥(ì»¨í…ìŠ¤íŠ¸)ì„ ì°¾ìŠµë‹ˆë‹¤."""
        import re
        sentences = re.split(r'(?<=[.!?])\s+', document_text)
        
        entity_contexts = {"entities": {}, "contexts": []}
        
        for entity in entities:
            text = entity.get("properties", {}).get("text", "")
            if text:
                entity_contexts["entities"][text] = {"id": entity["id"], "type": entity["type"]}

        for i, sentence in enumerate(sentences):
            found_entities = []
            for entity_text in entity_contexts["entities"].keys():
                if re.search(r'\b' + re.escape(entity_text) + r'\b', sentence, re.IGNORECASE):
                    found_entities.append(entity_text)
            
            if len(found_entities) > 1:
                context_sentences = sentences[max(0, i - window_size) : i + window_size + 1]
                context = " ".join(context_sentences).strip()
                entity_info = {
                    "entities": found_entities,
                    "context": context
                }
                entity_contexts["contexts"].append(entity_info)

        return entity_contexts

    def _create_relationship_extraction_prompt(self, entity_contexts: Dict[str, Any], domain: DocumentDomain) -> Optional[str]:
        """ê´€ê³„ ì¶”ì¶œì„ ìœ„í•œ ê°œì„ ëœ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        if not entity_contexts["contexts"]:
            return None

        domain_info = self.schema_manager.get_domain_schema(domain)
        relationship_types = ", ".join(domain_info.get("relationships", ["RELATED_TO"]))

        prompt_parts = [
            "Please analyze the relationships between the following entities based on the provided contexts from a document.",
            "The domain of the document is '{domain.value}'.",
            "\n**Instructions**:",
            "1. For each context, identify the relationship between the mentioned entities.",
            "2. The relationship must be one of the following types: {relationship_types}, or RELATED_TO if none apply.",
            "3. Format the output as a JSON array of objects, where each object has 'entity1', 'entity2', 'relationship', 'confidence' (from 0.0 to 1.0), and 'evidence' (the sentence supporting the relationship).",
            "4. Only extract relationships that are clearly and directly stated in the context. Do not infer relationships.",
            "\n**Contexts & Entities**:"]

        for item in entity_contexts["contexts"]:
            context_str = f"\n- **Context**: \"{item['context']}\""
            entities_str = f"  **Entities**: {', '.join(item['entities'])}"
            prompt_parts.append(context_str)
            prompt_parts.append(entities_str)

        prompt_parts.append("\n**Output (JSON Array)**:")
        
        return "\n".join(prompt_parts)

    def _parse_llm_relationship_response(self, llm_response: str, entity_map: Dict[str, Dict[str, Any]], 
                                       domain: DocumentDomain) -> List[Dict[str, Any]]:
        """LLM ì‘ë‹µ(JSON)ì—ì„œ ê´€ê³„ ì •ë³´ íŒŒì‹±"""
        relationships = []
        try:
            # Find the JSON part of the response
            from typing import re
            json_match = re.search(r'\[(.*?)(\s*,?.*)*\]', llm_response, re.DOTALL)
            if not json_match:
                self.logger.warning("LLM ì‘ë‹µì—ì„œ JSON ë°°ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return relationships

            parsed_json = json.loads(json_match.group(0))
            
            for rel in parsed_json:
                entity1_text = rel.get("entity1")
                entity2_text = rel.get("entity2")
                relationship_type = rel.get("relationship")
                confidence = rel.get("confidence")
                evidence = rel.get("evidence")

                # Find the full entity from the entity map
                entity1 = next((e for e in entity_map.values() if e['properties']['text'] == entity1_text), None)
                entity2 = next((e for e in entity_map.values() if e['properties']['text'] == entity2_text), None)

                if entity1 and entity2 and entity1["id"] != entity2["id"]:
                    relationship = {
                        "source": entity1["id"],
                        "target": entity2["id"],
                        "type": "RELATED_TO",
                        "properties": {
                            "relationship_name": relationship_type,
                            "domain": domain.value,
                            "extraction_method": "llm",
                            "confidence_score": confidence,
                            "context_snippet": evidence,
                            "entity_to_entity": True,
                            "inferred": True,
                            "llm_extracted": True
                        }
                    }
                    relationships.append(relationship)
        except json.JSONDecodeError as e:
            self.logger.warning(f"LLM ì‘ë‹µ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        except Exception as e:
            self.logger.warning(f"LLM ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            
        return relationships
    
    def _get_timestamp(self) -> str:
        """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±"""
        return datetime.now().isoformat()


def create_hierarchical_kg_builder(memgraph_config: Dict[str, Any] = None) -> HierarchicalKGBuilder:
    """ê³„ì¸µì  KG ë¹Œë” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return HierarchicalKGBuilder(memgraph_config, auto_save_to_memgraph=True)