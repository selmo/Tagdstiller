import logging
from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from typing import List, Dict, Any, Iterator
import json
import time
import threading
import sys
from db.models import Config
from response_models import ConfigCreate, ConfigUpdate, ConfigResponse
from dependencies import get_db

logger = logging.getLogger(__name__)

# Global dictionary to track download progress
download_progress = {}

router = APIRouter(prefix="/configs", tags=["configs"])

@router.get("/", response_model=List[ConfigResponse])
def list_configs(db: Session = Depends(get_db)):
    """Get all configuration key-value pairs."""
    configs = db.query(Config).all()
    return configs

@router.get("/{key}", response_model=ConfigResponse)
def get_config(key: str, db: Session = Depends(get_db)):
    """Get a specific configuration value by key."""
    config = db.query(Config).filter(Config.key == key).first()
    if not config:
        raise HTTPException(status_code=404, detail=f"Config key '{key}' not found")
    return config

@router.put("/{key}", response_model=ConfigResponse)
def update_config(key: str, config_data: ConfigUpdate, db: Session = Depends(get_db)):
    """Update or create a configuration value."""
    config = db.query(Config).filter(Config.key == key).first()
    
    if config:
        # Update existing config
        config.value = config_data.value
        if config_data.description is not None:
            config.description = config_data.description
        # updated_at will be automatically set by onupdate
    else:
        # Create new config
        config = Config(
            key=key,
            value=config_data.value,
            description=config_data.description
        )
        db.add(config)
    
    db.commit()
    db.refresh(config)
    return config

@router.post("/", response_model=ConfigResponse)
def create_config(config_data: ConfigCreate, db: Session = Depends(get_db)):
    """Create a new configuration entry."""
    # Check if key already exists
    existing_config = db.query(Config).filter(Config.key == config_data.key).first()
    if existing_config:
        raise HTTPException(status_code=400, detail=f"Config key '{config_data.key}' already exists")
    
    config = Config(
        key=config_data.key,
        value=config_data.value,
        description=config_data.description
    )
    db.add(config)
    db.commit()
    db.refresh(config)
    return config

@router.delete("/{key}")
def delete_config(key: str, db: Session = Depends(get_db)):
    """Delete a configuration entry."""
    config = db.query(Config).filter(Config.key == key).first()
    if not config:
        raise HTTPException(status_code=404, detail=f"Config key '{key}' not found")
    
    db.delete(config)
    db.commit()
    return {"message": f"Config key '{key}' deleted successfully"}

@router.get("/keybert/models")
def get_keybert_models() -> Dict[str, Any]:
    """Get available KeyBERT models for selection."""
    
    # Popular sentence transformer models for KeyBERT
    models = {
        "multilingual": [
            {
                "name": "all-MiniLM-L6-v2",
                "description": "Fast and efficient multilingual model (384 dimensions)",
                "size": "Small (~90MB)",
                "languages": ["English", "Korean", "Chinese", "Japanese", "and 100+ more"],
                "speed": "Fast",
                "quality": "Good",
                "recommended": True
            },
            {
                "name": "paraphrase-multilingual-MiniLM-L12-v2", 
                "description": "Multilingual paraphrase model (384 dimensions)",
                "size": "Medium (~420MB)",
                "languages": ["50+ languages including Korean"],
                "speed": "Medium",
                "quality": "Very Good"
            },
            {
                "name": "paraphrase-multilingual-mpnet-base-v2",
                "description": "High-quality multilingual model (768 dimensions)",
                "size": "Large (~1.1GB)",
                "languages": ["50+ languages including Korean"],
                "speed": "Slow",
                "quality": "Excellent"
            },
            {
                "name": "distiluse-base-multilingual-cased",
                "description": "Multilingual DistilUSE model (512 dimensions)",
                "size": "Medium (~500MB)",
                "languages": ["15 languages including Korean"],
                "speed": "Fast",
                "quality": "Good"
            },
            {
                "name": "sentence-transformers/LaBSE",
                "description": "Language-agnostic BERT Sentence Embedding",
                "size": "Large (~1.9GB)",
                "languages": ["109+ languages including Korean"],
                "speed": "Slow",
                "quality": "Excellent"
            }
        ],
        "korean_optimized": [
            {
                "name": "jhgan/ko-sroberta-multitask",
                "description": "Korean-optimized SBERT model",
                "size": "Medium (~400MB)",
                "languages": ["Korean", "English"],
                "speed": "Medium",
                "quality": "Excellent for Korean",
                "recommended": True
            },
            {
                "name": "jhgan/ko-sbert-nli",
                "description": "Korean SBERT for semantic similarity",
                "size": "Medium (~400MB)",
                "languages": ["Korean"],
                "speed": "Medium", 
                "quality": "Very Good for Korean"
            },
            {
                "name": "BM-K/KoSimCSE-roberta-multitask",
                "description": "Korean SimCSE model for semantic similarity",
                "size": "Medium (~420MB)",
                "languages": ["Korean"],
                "speed": "Medium",
                "quality": "Excellent for Korean"
            },
            {
                "name": "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
                "description": "Korean SBERT trained on KLUE NLI and augmented STS",
                "size": "Medium (~400MB)",
                "languages": ["Korean"],
                "speed": "Medium",
                "quality": "Very Good for Korean"
            }
        ],
        "english_only": [
            {
                "name": "all-mpnet-base-v2",
                "description": "High-performance English model (768 dimensions)",
                "size": "Large (~420MB)",
                "languages": ["English"],
                "speed": "Medium",
                "quality": "Excellent"
            },
            {
                "name": "all-distilroberta-v1",
                "description": "Fast English model based on DistilRoBERTa",
                "size": "Medium (~290MB)",
                "languages": ["English"],
                "speed": "Fast",
                "quality": "Good"
            },
            {
                "name": "all-roberta-large-v1",
                "description": "Large RoBERTa model for high accuracy",
                "size": "Large (~1.4GB)",
                "languages": ["English"],
                "speed": "Slow",
                "quality": "Excellent"
            },
            {
                "name": "paraphrase-albert-small-v2",
                "description": "Small ALBERT model for paraphrase detection",
                "size": "Small (~40MB)",
                "languages": ["English"],
                "speed": "Very Fast",
                "quality": "Good"
            },
            {
                "name": "msmarco-distilbert-base-v4",
                "description": "DistilBERT trained on MS MARCO dataset",
                "size": "Medium (~260MB)",
                "languages": ["English"],
                "speed": "Fast",
                "quality": "Very Good"
            }
        ]
    }
    
    return {
        "models": models,
        "current_model": "all-MiniLM-L6-v2",  # Default model
        "recommendation": "For Korean documents, use 'jhgan/ko-sroberta-multitask'. For multilingual use, 'all-MiniLM-L6-v2' is recommended for speed, or 'paraphrase-multilingual-mpnet-base-v2' for quality."
    }

@router.get("/keybert/models/download/progress/{progress_key}")
def get_download_progress_stream(progress_key: str):
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥ ì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
    
    def generate_progress() -> Iterator[str]:
        max_wait_time = 300  # 5ë¶„ ìµœëŒ€ ëŒ€ê¸°
        start_time = time.time()
        
        while time.time() - start_time < max_wait_time:
            if progress_key in download_progress:
                progress_data = download_progress[progress_key]
                
                # ì„œë²„ ë¡œê·¸ì— ì§„í–‰ë¥  ê¸°ë¡
                if progress_data["progress"] % 10 == 0 or progress_data["status"] in ["completed", "error"]:
                    logger.info(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥  - {progress_data['model_name']}: {progress_data['progress']}% ({progress_data['message']})")
                
                # SSE í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì „ì†¡
                yield f"data: {json.dumps(progress_data)}\n\n"
                
                # ì™„ë£Œë˜ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì¢…ë£Œ
                if progress_data["status"] in ["completed", "error"]:
                    break
                    
            else:
                # ì§„í–‰ë¥  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                yield f"data: {json.dumps({'status': 'not_found', 'message': 'ì§„í–‰ë¥  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'})}\n\n"
                break
            
            time.sleep(0.5)  # 0.5ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
        
        # íƒ€ì„ì•„ì›ƒ ì‹œ
        if time.time() - start_time >= max_wait_time:
            yield f"data: {json.dumps({'status': 'timeout', 'message': 'íƒ€ì„ì•„ì›ƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤'})}\n\n"
    
    return StreamingResponse(
        generate_progress(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control"
        }
    )

@router.post("/keybert/models/{model_name}/download")
def download_keybert_model(model_name: str) -> Dict[str, Any]:
    """KeyBERT ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œë“œí•©ë‹ˆë‹¤."""
    logger.info(f"ğŸ”„ KeyBERT ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìš”ì²­: {model_name}")
    
    # ì§„í–‰ë¥  ì´ˆê¸°í™”
    progress_key = f"keybert_download_{model_name}"
    download_progress[progress_key] = {
        "status": "starting",
        "progress": 0,
        "message": "ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì¤‘...",
        "model_name": model_name,
        "start_time": time.time()
    }
    
    try:
        from sentence_transformers import SentenceTransformer
        import os
        from pathlib import Path
        
        # í˜„ì¬ ìºì‹œ ìƒíƒœ í™•ì¸ (ìƒˆë¡œìš´ huggingface hub ìºì‹œ ìœ„ì¹˜)
        cache_dirs = [
            Path.home() / ".cache" / "huggingface" / "hub",  # ìƒˆ ìœ„ì¹˜
            Path.home() / ".cache" / "torch" / "sentence_transformers"  # ê¸°ì¡´ ìœ„ì¹˜ (fallback)
        ]
        was_cached = False
        
        download_progress[progress_key].update({
            "status": "checking_cache",
            "progress": 10,
            "message": "ìºì‹œ ìƒíƒœ í™•ì¸ ì¤‘..."
        })
        logger.info(f"ğŸ“¦ ëª¨ë¸ '{model_name}' ìºì‹œ ìƒíƒœ í™•ì¸ ì¤‘...")
        
        for cache_dir in cache_dirs:
            if cache_dir.exists():
                for item in cache_dir.iterdir():
                    if item.is_dir():
                        # ëª¨ë¸ ì´ë¦„ì´ í¬í•¨ëœ í´ë”ì¸ì§€ í™•ì¸ (ë” ì •í™•í•œ ë§¤ì¹­)
                        folder_name = item.name.lower()
                        model_name_for_hub = f"models--sentence-transformers--{model_name.replace('/', '--')}"
                        model_name_variations = [
                            model_name.lower(),
                            model_name.lower().replace("/", "_"),
                            model_name.lower().replace("-", "_"),
                            model_name.lower().replace("/", "__"),
                            model_name_for_hub.lower(),  # huggingface hub í˜•ì‹
                        ]
                        
                        # ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        for variation in model_name_variations:
                            if variation in folder_name or folder_name in variation:
                                was_cached = True
                                logger.info(f"ğŸ“¦ ëª¨ë¸ '{model_name}'ì´ ì´ë¯¸ ìºì‹œë˜ì–´ ìˆìŒ (í´ë”: {item.name})")
                                break
                        
                        if was_cached:
                            break
            
            if was_cached:
                break
        
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = time.time()
        
        if was_cached:
            download_progress[progress_key].update({
                "status": "loading_cached",
                "progress": 30,
                "message": "ìºì‹œëœ ëª¨ë¸ ë¡œë“œ ì¤‘..."
            })
            logger.info(f"ğŸ”„ KeyBERT ëª¨ë¸ '{model_name}' ìºì‹œì—ì„œ ë¡œë“œ ì¤‘...")
        else:
            download_progress[progress_key].update({
                "status": "downloading",
                "progress": 20,
                "message": "ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"
            })
            logger.info(f"ğŸ“¥ KeyBERT ëª¨ë¸ '{model_name}' ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ë‹¤ìš´ë¡œë“œ/ë¡œë“œ ì¤‘)
        download_progress[progress_key].update({
            "progress": 50,
            "message": "ëª¨ë¸ ì´ˆê¸°í™” ì¤‘..."
        })
        
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ (ìë™ìœ¼ë¡œ ìºì‹œë¨)
        model = SentenceTransformer(model_name)
        
        # ì™„ë£Œ ì‹œê°„ ê¸°ë¡
        process_time = time.time() - start_time
        
        download_progress[progress_key].update({
            "status": "validating",
            "progress": 80,
            "message": "ëª¨ë¸ ê²€ì¦ ì¤‘..."
        })
        
        if was_cached:
            logger.info(f"âœ… KeyBERT ëª¨ë¸ '{model_name}' ìºì‹œì—ì„œ ë¡œë“œ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {process_time:.2f}ì´ˆ)")
        else:
            logger.info(f"âœ… KeyBERT ëª¨ë¸ '{model_name}' ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {process_time:.2f}ì´ˆ)")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ ëª¨ë¸ ê²€ì¦
        test_text = "í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤"
        embeddings = model.encode([test_text])
        embedding_dim = embeddings.shape[1] if len(embeddings.shape) > 1 else embeddings.shape[0]
        logger.info(f"ğŸ§ª ëª¨ë¸ ê²€ì¦ ì™„ë£Œ - ì„ë² ë”© ì°¨ì›: {embedding_dim}, í˜•íƒœ: {embeddings.shape}")
        
        download_progress[progress_key].update({
            "progress": 90,
            "message": "ìºì‹œ í¬ê¸° ê³„ì‚° ì¤‘..."
        })
        
        # ë‹¤ìš´ë¡œë“œ í›„ ìºì‹œ í¬ê¸° í™•ì¸
        total_cache_size = 0
        cache_paths = []
        
        for cache_dir in cache_dirs:
            if cache_dir.exists():
                for item in cache_dir.iterdir():
                    if item.is_dir():
                        folder_name = item.name.lower()
                        model_name_for_hub = f"models--sentence-transformers--{model_name.replace('/', '--')}"
                        model_name_variations = [
                            model_name.lower(),
                            model_name.lower().replace("/", "_"),
                            model_name.lower().replace("-", "_"),
                            model_name.lower().replace("/", "__"),
                            model_name_for_hub.lower(),  # huggingface hub í˜•ì‹
                        ]
                        
                        for variation in model_name_variations:
                            if variation in folder_name or folder_name in variation:
                                cache_paths.append(str(item))
                                folder_size = sum(f.stat().st_size for f in item.rglob('*') if f.is_file())
                                total_cache_size += folder_size
                                break
        
        cache_size_mb = round(total_cache_size / (1024*1024), 2)
        logger.info(f"ğŸ’¾ ëª¨ë¸ '{model_name}' ìºì‹œ í¬ê¸°: {cache_size_mb}MB")
        
        # ì§„í–‰ë¥  ì™„ë£Œ
        download_progress[progress_key].update({
            "status": "completed",
            "progress": 100,
            "message": f"ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {process_time:.1f}ì´ˆ, í¬ê¸°: {cache_size_mb}MB)"
        })
        
        # 3ì´ˆ í›„ ì§„í–‰ë¥  ë°ì´í„° ì •ë¦¬
        def cleanup_progress():
            time.sleep(3)
            if progress_key in download_progress:
                del download_progress[progress_key]
        
        threading.Thread(target=cleanup_progress, daemon=True).start()
        
        return {
            "status": "success",
            "message": f"ëª¨ë¸ '{model_name}'ì´ ì„±ê³µì ìœ¼ë¡œ {'ë¡œë“œ' if was_cached else 'ë‹¤ìš´ë¡œë“œ'}ë˜ì—ˆìŠµë‹ˆë‹¤",
            "model_name": model_name,
            "download_time_seconds": round(process_time, 2),
            "embedding_dimension": embedding_dim,
            "model_size_mb": cache_size_mb,
            "was_cached": was_cached,
            "cache_paths": cache_paths,
            "action": "loaded" if was_cached else "downloaded",
            "progress_key": progress_key
        }
        
    except Exception as e:
        error_msg = f"ëª¨ë¸ '{model_name}' {'ë¡œë“œ' if 'cached' in str(e).lower() else 'ë‹¤ìš´ë¡œë“œ'} ì‹¤íŒ¨: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        
        # ì§„í–‰ë¥  ì˜¤ë¥˜ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
        download_progress[progress_key].update({
            "status": "error",
            "progress": 0,
            "message": f"ì˜¤ë¥˜: {str(e)}"
        })
        
        return {
            "status": "error",
            "message": error_msg,
            "model_name": model_name,
            "error": str(e),
            "progress_key": progress_key
        }

@router.delete("/keybert/models/{model_name}/cache")
def delete_keybert_model_cache(model_name: str) -> Dict[str, Any]:
    """KeyBERT ëª¨ë¸ ìºì‹œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    logger.info(f"ğŸ—‘ï¸ KeyBERT ëª¨ë¸ ìºì‹œ ì‚­ì œ ìš”ì²­: {model_name}")
    
    try:
        import os
        import shutil
        from pathlib import Path
        
        # sentence-transformers ìºì‹œ ë””ë ‰í† ë¦¬ ì°¾ê¸° (ìƒˆë¡œìš´ huggingface hub ìºì‹œ ìœ„ì¹˜)
        cache_dirs = [
            Path.home() / ".cache" / "huggingface" / "hub",  # ìƒˆ ìœ„ì¹˜
            Path.home() / ".cache" / "torch" / "sentence_transformers"  # ê¸°ì¡´ ìœ„ì¹˜ (fallback)
        ]
        
        deleted_paths = []
        total_size_deleted = 0
        
        for cache_dir in cache_dirs:
            if cache_dir.exists():
                # ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ìºì‹œ í´ë”ë“¤ ì°¾ê¸°
                model_folders = []
                for item in cache_dir.iterdir():
                    if item.is_dir():
                        # ëª¨ë¸ ì´ë¦„ì´ í¬í•¨ëœ í´ë”ì¸ì§€ í™•ì¸ (ë” ì •í™•í•œ ë§¤ì¹­)
                        folder_name = item.name.lower()
                        model_name_for_hub = f"models--sentence-transformers--{model_name.replace('/', '--')}"
                        model_name_variations = [
                            model_name.lower(),
                            model_name.lower().replace("/", "_"),
                            model_name.lower().replace("-", "_"),
                            model_name.lower().replace("/", "__"),
                            model_name_for_hub.lower(),  # huggingface hub í˜•ì‹
                        ]
                        
                        # ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        for variation in model_name_variations:
                            if variation in folder_name or folder_name in variation:
                                model_folders.append(item)
                                break
            
            # ì°¾ì€ í´ë”ë“¤ ì‚­ì œ
            for folder in model_folders:
                try:
                    # í´ë” í¬ê¸° ê³„ì‚°
                    folder_size = sum(f.stat().st_size for f in folder.rglob('*') if f.is_file())
                    
                    # í´ë” ì‚­ì œ
                    shutil.rmtree(folder)
                    deleted_paths.append(str(folder))
                    total_size_deleted += folder_size
                    logger.info(f"ğŸ—‘ï¸ ì‚­ì œëœ ìºì‹œ í´ë”: {folder} ({folder_size / (1024*1024):.2f}MB)")
                    
                except Exception as folder_error:
                    logger.warning(f"âš ï¸ í´ë” ì‚­ì œ ì‹¤íŒ¨: {folder} - {folder_error}")
        
        if deleted_paths:
            logger.info(f"âœ… KeyBERT ëª¨ë¸ '{model_name}' ìºì‹œ ì‚­ì œ ì™„ë£Œ - ì´ {len(deleted_paths)}ê°œ í´ë”, {total_size_deleted / (1024*1024):.2f}MB")
            return {
                "status": "success",
                "message": f"ëª¨ë¸ '{model_name}' ìºì‹œê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤",
                "model_name": model_name,
                "deleted_paths": deleted_paths,
                "total_size_mb": round(total_size_deleted / (1024*1024), 2)
            }
        else:
            logger.info(f"â„¹ï¸ KeyBERT ëª¨ë¸ '{model_name}' ìºì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return {
                "status": "success",
                "message": f"ëª¨ë¸ '{model_name}'ì˜ ìºì‹œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ë‹¤ìš´ë¡œë“œë˜ì§€ ì•ŠìŒ)",
                "model_name": model_name,
                "deleted_paths": [],
                "total_size_mb": 0
            }
            
    except Exception as e:
        error_msg = f"ëª¨ë¸ '{model_name}' ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        return {
            "status": "error",
            "message": error_msg,
            "model_name": model_name,
            "error": str(e)
        }

@router.get("/keybert/models/{model_name}/status")
def get_keybert_model_status(model_name: str) -> Dict[str, Any]:
    """KeyBERT ëª¨ë¸ì˜ ìºì‹œ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        import os
        from pathlib import Path
        
        # sentence-transformers ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸ (ìƒˆë¡œìš´ huggingface hub ìºì‹œ ìœ„ì¹˜)
        cache_dirs = [
            Path.home() / ".cache" / "huggingface" / "hub",  # ìƒˆ ìœ„ì¹˜
            Path.home() / ".cache" / "torch" / "sentence_transformers"  # ê¸°ì¡´ ìœ„ì¹˜ (fallback)
        ]
        
        is_cached = False
        cache_paths = []
        total_size = 0
        
        for cache_dir in cache_dirs:
            if cache_dir.exists():
                # ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ìºì‹œ í´ë”ë“¤ ì°¾ê¸°
                for item in cache_dir.iterdir():
                    if item.is_dir():
                        folder_name = item.name.lower()
                        model_name_for_hub = f"models--sentence-transformers--{model_name.replace('/', '--')}"
                        model_name_variations = [
                            model_name.lower(),
                            model_name.lower().replace("/", "_"),
                            model_name.lower().replace("-", "_"),
                            model_name.lower().replace("/", "__"),
                            model_name_for_hub.lower(),  # huggingface hub í˜•ì‹
                        ]
                        
                        # ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        for variation in model_name_variations:
                            if variation in folder_name or folder_name in variation:
                                is_cached = True
                                cache_paths.append(str(item))
                                # í´ë” í¬ê¸° ê³„ì‚°
                                folder_size = sum(f.stat().st_size for f in item.rglob('*') if f.is_file())
                                total_size += folder_size
                                break
        
        return {
            "status": "success",
            "model_name": model_name,
            "is_cached": is_cached,
            "cache_paths": cache_paths,
            "total_size_mb": round(total_size / (1024*1024), 2) if total_size > 0 else 0
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"ëª¨ë¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}",
            "model_name": model_name,
            "error": str(e)
        }

@router.get("/spacy/models")
def get_spacy_models() -> Dict[str, Any]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ spaCy NER ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    
    # spaCy NER ëª¨ë¸ ëª©ë¡
    models = {
        "korean": [
            {
                "name": "ko_core_news_sm",
                "description": "í•œêµ­ì–´ ì†Œí˜• ëª¨ë¸ (12MB)",
                "size": "Small (~12MB)",
                "languages": ["Korean"],
                "capabilities": ["í† í°í™”", "í’ˆì‚¬ íƒœê¹…", "ê°œì²´ëª… ì¸ì‹"],
                "speed": "ë¹ ë¦„",
                "accuracy": "ì¢‹ìŒ",
                "recommended": True
            },
            {
                "name": "ko_core_news_md",
                "description": "í•œêµ­ì–´ ì¤‘í˜• ëª¨ë¸ (40MB)",
                "size": "Medium (~40MB)",
                "languages": ["Korean"],
                "capabilities": ["í† í°í™”", "í’ˆì‚¬ íƒœê¹…", "ê°œì²´ëª… ì¸ì‹", "ë‹¨ì–´ ë²¡í„°"],
                "speed": "ë³´í†µ",
                "accuracy": "ë§¤ìš° ì¢‹ìŒ"
            },
            {
                "name": "ko_core_news_lg",
                "description": "í•œêµ­ì–´ ëŒ€í˜• ëª¨ë¸ (560MB)",
                "size": "Large (~560MB)",
                "languages": ["Korean"],
                "capabilities": ["í† í°í™”", "í’ˆì‚¬ íƒœê¹…", "ê°œì²´ëª… ì¸ì‹", "ë‹¨ì–´ ë²¡í„°"],
                "speed": "ëŠë¦¼",
                "accuracy": "ìš°ìˆ˜"
            }
        ],
        "english": [
            {
                "name": "en_core_web_sm",
                "description": "ì˜ì–´ ì†Œí˜• ëª¨ë¸ (12MB)",
                "size": "Small (~12MB)",
                "languages": ["English"],
                "capabilities": ["í† í°í™”", "í’ˆì‚¬ íƒœê¹…", "ê°œì²´ëª… ì¸ì‹", "ì˜ì¡´ì„± íŒŒì‹±"],
                "speed": "ë¹ ë¦„",
                "accuracy": "ì¢‹ìŒ"
            },
            {
                "name": "en_core_web_md",
                "description": "ì˜ì–´ ì¤‘í˜• ëª¨ë¸ (40MB)",
                "size": "Medium (~40MB)",
                "languages": ["English"],
                "capabilities": ["í† í°í™”", "í’ˆì‚¬ íƒœê¹…", "ê°œì²´ëª… ì¸ì‹", "ì˜ì¡´ì„± íŒŒì‹±", "ë‹¨ì–´ ë²¡í„°"],
                "speed": "ë³´í†µ",
                "accuracy": "ë§¤ìš° ì¢‹ìŒ"
            },
            {
                "name": "en_core_web_lg",
                "description": "ì˜ì–´ ëŒ€í˜• ëª¨ë¸ (560MB)",
                "size": "Large (~560MB)",
                "languages": ["English"],
                "capabilities": ["í† í°í™”", "í’ˆì‚¬ íƒœê¹…", "ê°œì²´ëª… ì¸ì‹", "ì˜ì¡´ì„± íŒŒì‹±", "ë‹¨ì–´ ë²¡í„°"],
                "speed": "ëŠë¦¼",
                "accuracy": "ìš°ìˆ˜"
            },
            {
                "name": "en_core_web_trf",
                "description": "ì˜ì–´ Transformer ëª¨ë¸ (400MB)",
                "size": "Large (~400MB)",
                "languages": ["English"],
                "capabilities": ["í† í°í™”", "í’ˆì‚¬ íƒœê¹…", "ê°œì²´ëª… ì¸ì‹", "ì˜ì¡´ì„± íŒŒì‹±", "Transformer"],
                "speed": "ë§¤ìš° ëŠë¦¼",
                "accuracy": "ìµœê³ "
            }
        ],
        "multilingual": [
            {
                "name": "xx_ent_wiki_sm",
                "description": "ë‹¤êµ­ì–´ ê°œì²´ëª… ì¸ì‹ ëª¨ë¸ (12MB)",
                "size": "Small (~12MB)",
                "languages": ["Multi-language"],
                "capabilities": ["ê°œì²´ëª… ì¸ì‹"],
                "speed": "ë¹ ë¦„",
                "accuracy": "ì¢‹ìŒ"
            }
        ]
    }
    
    return {
        "models": models,
        "current_model": "ko_core_news_sm",  # Default model
        "recommendation": "í•œêµ­ì–´ ë¬¸ì„œëŠ” 'ko_core_news_sm', ì˜ì–´ ë¬¸ì„œëŠ” 'en_core_web_sm' ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    }

@router.post("/spacy/models/{model_name}/download")
def download_spacy_model(model_name: str) -> Dict[str, Any]:
    """spaCy ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
    logger.info(f"ğŸ”„ spaCy ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìš”ì²­: {model_name}")
    
    # ì§„í–‰ë¥  ì´ˆê¸°í™”
    progress_key = f"spacy_download_{model_name}"
    download_progress[progress_key] = {
        "status": "starting",
        "progress": 0,
        "message": "ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì¤‘...",
        "model_name": model_name,
        "start_time": time.time()
    }
    
    try:
        import subprocess
        import spacy
        from pathlib import Path
        
        # ëª¨ë¸ì´ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        download_progress[progress_key].update({
            "status": "checking",
            "progress": 10,
            "message": "ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸ ì¤‘..."
        })
        
        try:
            # ëª¨ë¸ ë¡œë“œ ì‹œë„
            nlp = spacy.load(model_name)
            
            download_progress[progress_key].update({
                "status": "completed",
                "progress": 100,
                "message": "ëª¨ë¸ì´ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤"
            })
            
            logger.info(f"âœ… spaCy ëª¨ë¸ '{model_name}'ì´ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŒ")
            
            # ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            model_info = {
                "lang": nlp.lang,
                "pipeline": nlp.pipe_names,
                "version": getattr(nlp.meta, 'version', 'unknown')
            }
            
            return {
                "status": "success",
                "message": f"ëª¨ë¸ '{model_name}'ì´ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤",
                "model_name": model_name,
                "already_installed": True,
                "model_info": model_info,
                "progress_key": progress_key
            }
            
        except OSError:
            # ëª¨ë¸ì´ ì—†ìœ¼ë¯€ë¡œ ë‹¤ìš´ë¡œë“œ ì§„í–‰
            logger.info(f"ğŸ“¥ spaCy ëª¨ë¸ '{model_name}' ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            
            download_progress[progress_key].update({
                "status": "downloading",
                "progress": 30,
                "message": "ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"
            })
            
            # spaCy ë‹¤ìš´ë¡œë“œ ëª…ë ¹ ì‹¤í–‰
            cmd = [sys.executable, "-m", "spacy", "download", model_name]
            
            # í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1
            )
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì‹œë®¬ë ˆì´ì…˜
            import threading
            def update_progress():
                progress_steps = [
                    (40, "íŒ¨í‚¤ì§€ ì •ë³´ í™•ì¸ ì¤‘..."),
                    (50, "ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘..."),
                    (70, "ëª¨ë¸ ì„¤ì¹˜ ì¤‘..."),
                    (90, "ì„¤ì¹˜ ë§ˆë¬´ë¦¬ ì¤‘...")
                ]
                
                for progress, message in progress_steps:
                    time.sleep(2)
                    if progress_key in download_progress:
                        download_progress[progress_key].update({
                            "progress": progress,
                            "message": message
                        })
            
            progress_thread = threading.Thread(target=update_progress)
            progress_thread.start()
            
            # í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
            stdout, stderr = process.communicate()
            
            if process.returncode == 0:
                # ë‹¤ìš´ë¡œë“œ ì„±ê³µ
                download_progress[progress_key].update({
                    "status": "validating",
                    "progress": 95,
                    "message": "ëª¨ë¸ ê²€ì¦ ì¤‘..."
                })
                
                # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
                nlp = spacy.load(model_name)
                
                download_progress[progress_key].update({
                    "status": "completed",
                    "progress": 100,
                    "message": "ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!"
                })
                
                logger.info(f"âœ… spaCy ëª¨ë¸ '{model_name}' ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜ ì™„ë£Œ")
                
                # ëª¨ë¸ ì •ë³´
                model_info = {
                    "lang": nlp.lang,
                    "pipeline": nlp.pipe_names,
                    "version": getattr(nlp.meta, 'version', 'unknown')
                }
                
                # 5ì´ˆ í›„ ì§„í–‰ë¥  ë°ì´í„° ì •ë¦¬
                def cleanup_progress():
                    time.sleep(5)
                    if progress_key in download_progress:
                        del download_progress[progress_key]
                
                threading.Thread(target=cleanup_progress, daemon=True).start()
                
                return {
                    "status": "success",
                    "message": f"ëª¨ë¸ '{model_name}'ì´ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤",
                    "model_name": model_name,
                    "model_info": model_info,
                    "progress_key": progress_key
                }
            else:
                # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
                error_msg = stderr if stderr else "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
                download_progress[progress_key].update({
                    "status": "error",
                    "progress": 0,
                    "message": f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {error_msg}"
                })
                
                logger.error(f"âŒ spaCy ëª¨ë¸ '{model_name}' ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {error_msg}")
                
                return {
                    "status": "error",
                    "message": f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {error_msg}",
                    "model_name": model_name,
                    "error": error_msg,
                    "progress_key": progress_key
                }
                
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ spaCy ëª¨ë¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {error_msg}")
        
        download_progress[progress_key].update({
            "status": "error",
            "progress": 0,
            "message": f"ì˜¤ë¥˜: {error_msg}"
        })
        
        return {
            "status": "error",
            "message": f"ëª¨ë¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {error_msg}",
            "model_name": model_name,
            "error": error_msg,
            "progress_key": progress_key
        }

@router.get("/spacy/models/download/progress/{progress_key}")
def get_spacy_download_progress(progress_key: str):
    """spaCy ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥ ì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
    
    def generate_progress() -> Iterator[str]:
        max_wait_time = 300  # 5ë¶„ ìµœëŒ€ ëŒ€ê¸°
        start_time = time.time()
        
        while time.time() - start_time < max_wait_time:
            if progress_key in download_progress:
                progress_data = download_progress[progress_key]
                
                # ì„œë²„ ë¡œê·¸ì— ì§„í–‰ë¥  ê¸°ë¡
                if progress_data["progress"] % 20 == 0 or progress_data["status"] in ["completed", "error"]:
                    logger.info(f"ğŸ“Š spaCy ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥  - {progress_data['model_name']}: {progress_data['progress']}% ({progress_data['message']})")
                
                # SSE í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì „ì†¡
                yield f"data: {json.dumps(progress_data)}\n\n"
                
                # ì™„ë£Œë˜ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì¢…ë£Œ
                if progress_data["status"] in ["completed", "error"]:
                    break
                    
            else:
                # ì§„í–‰ë¥  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                yield f"data: {json.dumps({'status': 'not_found', 'message': 'ì§„í–‰ë¥  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'})}\n\n"
                break
            
            time.sleep(0.5)  # 0.5ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
        
        # íƒ€ì„ì•„ì›ƒ ì‹œ
        if time.time() - start_time >= max_wait_time:
            yield f"data: {json.dumps({'status': 'timeout', 'message': 'íƒ€ì„ì•„ì›ƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤'})}\n\n"
    
    return StreamingResponse(
        generate_progress(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control"
        }
    )

@router.get("/spacy/models/{model_name}/status")
def get_spacy_model_status(model_name: str) -> Dict[str, Any]:
    """spaCy ëª¨ë¸ì˜ ì„¤ì¹˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        import spacy
        
        # ëª¨ë¸ ë¡œë“œ ì‹œë„
        try:
            nlp = spacy.load(model_name)
            
            # ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘
            model_info = {
                "lang": nlp.lang,
                "pipeline": nlp.pipe_names,
                "version": getattr(nlp.meta, 'version', 'unknown'),
                "size": len(nlp.vocab)  # ì–´íœ˜ í¬ê¸°
            }
            
            return {
                "status": "success",
                "model_name": model_name,
                "is_installed": True,
                "model_info": model_info
            }
            
        except OSError:
            return {
                "status": "success",
                "model_name": model_name,
                "is_installed": False,
                "message": f"ëª¨ë¸ '{model_name}'ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
            }
            
    except Exception as e:
        return {
            "status": "error",
            "message": f"ëª¨ë¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}",
            "model_name": model_name,
            "error": str(e)
        }