# íŒŒì¼ ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê³¼ì •

DocExtract ì‹œìŠ¤í…œì—ì„œ íŒŒì¼ì„ ì½ì–´ë“¤ì¸ í›„ ìˆ˜í–‰í•˜ëŠ” íŒŒì¼ íŒŒì‹±ê³¼ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê³¼ì •ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨
1. [ì „ì²´ í”„ë¡œì„¸ìŠ¤ ê°œìš”](#ì „ì²´-í”„ë¡œì„¸ìŠ¤-ê°œìš”)
2. [íŒŒì¼ í˜•ì‹ë³„ íŒŒì‹± ê³¼ì •](#íŒŒì¼-í˜•ì‹ë³„-íŒŒì‹±-ê³¼ì •)
3. [í…ìŠ¤íŠ¸ ì •ì œ ê³¼ì •](#í…ìŠ¤íŠ¸-ì •ì œ-ê³¼ì •)
4. [í‚¤ì›Œë“œ í•„í„°ë§ ë° ì •ê·œí™”](#í‚¤ì›Œë“œ-í•„í„°ë§-ë°-ì •ê·œí™”)
5. [ì‹¤ì œ ì²˜ë¦¬ ì˜ˆì œ](#ì‹¤ì œ-ì²˜ë¦¬-ì˜ˆì œ)
6. [ì„±ëŠ¥ ë° ìµœì í™”](#ì„±ëŠ¥-ë°-ìµœì í™”)

---

## ì „ì²´ í”„ë¡œì„¸ìŠ¤ ê°œìš”

íŒŒì¼ ì—…ë¡œë“œë¶€í„° í‚¤ì›Œë“œ ì¶”ì¶œ ì¤€ë¹„ê¹Œì§€ì˜ ì „ì²´ í”„ë¡œì„¸ìŠ¤:

```mermaid
graph TD
    A[íŒŒì¼ ì—…ë¡œë“œ] --> B[íŒŒì¼ í˜•ì‹ ê°ì§€]
    B --> C[ì ì ˆí•œ íŒŒì„œ ì„ íƒ]
    C --> D[íŒŒì¼ íŒŒì‹±]
    D --> E[ì›ì‹œ í…ìŠ¤íŠ¸ ì¶”ì¶œ]
    E --> F[í…ìŠ¤íŠ¸ ì •ì œ]
    F --> G[ìœ íš¨ì„± ê²€ì¦]
    G --> H[ìœ„ì¹˜ ë§¤í•‘ ìƒì„±]
    H --> I[í‚¤ì›Œë“œ ì¶”ì¶œ ì¤€ë¹„ ì™„ë£Œ]
```

### ì£¼ìš” ë‹¨ê³„ë³„ ì—­í• 

| ë‹¨ê³„ | ë‹´ë‹¹ ëª¨ë“ˆ | ì£¼ìš” ê¸°ëŠ¥ |
|------|----------|---------|
| íŒŒì¼ í˜•ì‹ ê°ì§€ | `AutoParser` | í™•ì¥ì, MIME íƒ€ì…, ë§¤ì§ ë°”ì´íŠ¸ ë¶„ì„ |
| íŒŒì¼ íŒŒì‹± | ê°œë³„ íŒŒì„œë“¤ | í˜•ì‹ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ |
| í…ìŠ¤íŠ¸ ì •ì œ | `TextCleaner` | ê¹¨ì§„ ë¬¸ì ì œê±°, ìœ ë‹ˆì½”ë“œ ì •ê·œí™” |
| ìœ„ì¹˜ ë§¤í•‘ | `PositionMapper` | í˜ì´ì§€/ì¤„/ì»¬ëŸ¼ ì •ë³´ ìƒì„± |

---

## íŒŒì¼ í˜•ì‹ë³„ íŒŒì‹± ê³¼ì •

### 1. ìë™ íŒŒì¼ í˜•ì‹ ê°ì§€ (`AutoParser`)

#### ê°ì§€ ìš°ì„ ìˆœìœ„
```python
# íŒŒì¼: /backend/services/parser/auto_parser.py:31-62
# 1ìˆœìœ„: íŒŒì¼ í™•ì¥ì ê¸°ë°˜
extension_priority = {
    '.pdf': ['pdf'],
    '.docx': ['docx'],
    '.html': ['html'],
    '.md': ['markdown'],
    '.txt': ['txt'],
    '.zip': ['zip']
}

# 2ìˆœìœ„: MIME íƒ€ì… ê¸°ë°˜
mime_type_mapping = {
    'application/pdf': ['pdf'],
    'text/html': ['html'],
    'text/plain': ['txt']
}

# 3ìˆœìœ„: íŒŒì¼ ë‚´ìš© ê¸°ë°˜ (ë§¤ì§ ë°”ì´íŠ¸)
magic_bytes = {
    b'%PDF-': 'pdf',
    b'PK\x03\x04': 'docx/zip',
    b'<html': 'html'
}
```

#### íŒŒì„œ ì‹œë„ ë¡œì§
```python
# íŒŒì¼: /backend/services/parser/auto_parser.py:76-92
# ì—¬ëŸ¬ íŒŒì„œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
for parser_name in detected_parsers:
    try:
        result = parser.parse(file_path)
        if result.success:
            return result  # ì²« ë²ˆì§¸ ì„±ê³µí•œ íŒŒì„œ ì‚¬ìš©
    except Exception:
        continue  # ë‹¤ìŒ íŒŒì„œ ì‹œë„
```

### 2. PDF íŒŒì¼ ì²˜ë¦¬ (`PdfParser`)

PDFëŠ” ê°€ì¥ ë³µì¡í•œ íŒŒì¼ í˜•ì‹ìœ¼ë¡œ, **5ê°€ì§€ ì—”ì§„**ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤:

#### íŒŒì‹± ì—”ì§„ ìš°ì„ ìˆœìœ„
```python
# íŒŒì¼: /backend/services/parser/pdf_parser.py:25-31
parsing_engines = [
    ("pymupdf4llm", self._parse_with_pymupdf4llm),      # ìµœê³  í’ˆì§ˆ
    ("pdfplumber", self._parse_with_pdfplumber),        # í‘œ ì²˜ë¦¬ ìš°ìˆ˜
    ("pymupdf_advanced", self._parse_with_pymupdf_advanced),  # ë ˆì´ì•„ì›ƒ ê³ ë ¤
    ("pymupdf_basic", self._parse_with_pymupdf_basic),  # ê¸°ë³¸ ì¶”ì¶œ
    ("pypdf2", self._parse_with_pypdf2)                 # ë°±ì—… ì—”ì§„
]
```

#### í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ
ê° ì—”ì§„ì˜ ê²°ê³¼ë¥¼ í’ˆì§ˆ ì ìˆ˜ë¡œ í‰ê°€í•˜ì—¬ ìµœì  ê²°ê³¼ ì„ íƒ:

```python
# íŒŒì¼: /backend/services/parser/pdf_parser.py:139-191
def _evaluate_text_quality(text: str) -> float:
    """í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€ (0.0 ~ 1.0)"""
    # 1. ì •ìƒ ë¬¸ì ë¹„ìœ¨ (í•œê¸€, ì˜ì–´, ìˆ«ì) - 40%
    normal_ratio = count_normal_chars / total_chars
    
    # 2. ë¹„ì •ìƒ ë¬¸ì ë¹„ìœ¨ (ê¹¨ì§„ ë¬¸ì) - 30%
    suspicious_ratio = count_suspicious_chars / total_chars
    
    # 3. ê³µë°± ë¹„ìœ¨ (ì ë‹¹í•œ ê³µë°±) - 20%
    whitespace_ratio = count_whitespace / total_chars
    
    # 4. ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ë¹„ìœ¨ - 10%
    word_ratio = meaningful_words / total_words
    
    return (normal_ratio * 0.4 + 
            (1 - suspicious_ratio) * 0.3 + 
            min(0.3, 1 - whitespace_ratio) * 0.2 + 
            word_ratio * 0.1)
```

#### ì‹¤ì œ PDF ì²˜ë¦¬ ì˜ˆì‹œ
```
ğŸ“– PDF íŒŒì‹± ì‹œì‘: research_paper.pdf
ğŸ”„ pymupdf4llm ì—”ì§„ìœ¼ë¡œ ì‹œë„ ì¤‘...
ğŸ“Š pymupdf4llm í’ˆì§ˆ ì ìˆ˜: 0.92 (ê¸¸ì´: 15324)
âœ… pymupdf4llm ì—”ì§„ìœ¼ë¡œ ê³ í’ˆì§ˆ ì¶”ì¶œ ì„±ê³µ
ğŸ¯ ìµœì¢… ì„ íƒ: pymupdf4llm ì—”ì§„ (í’ˆì§ˆ: 0.92)
```

### 3. DOCX íŒŒì¼ ì²˜ë¦¬ (`DocxParser`)

Microsoft Word ë¬¸ì„œì˜ êµ¬ì¡°ì  í…ìŠ¤íŠ¸ ì¶”ì¶œ:

#### ì¶”ì¶œ ë‹¨ê³„
```python
# íŒŒì¼: /backend/services/parser/docx_parser.py:45-64
# 1. ë³¸ë¬¸ ë‹¨ë½ ì¶”ì¶œ
for paragraph in doc.paragraphs:
    cleaned_text = TextCleaner.clean_text(paragraph.text)
    if cleaned_text.strip():
        text_parts.append(cleaned_text)

# 2. í‘œ ë°ì´í„° ì¶”ì¶œ
for table in doc.tables:
    for row in table.rows:
        row_text = []
        for cell in row.cells:
            cleaned_cell_text = TextCleaner.clean_text(cell.text.strip())
            if cleaned_cell_text.strip():
                row_text.append(cleaned_cell_text)
        if row_text:
            text_parts.append('\t'.join(row_text))  # íƒ­ìœ¼ë¡œ êµ¬ë¶„
```

#### ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
```python
# íŒŒì¼: /backend/services/parser/docx_parser.py:71-82
core_props = doc.core_properties
metadata = {
    'title': core_props.title,
    'author': core_props.author,
    'created_date': str(core_props.created),
    'modified_date': str(core_props.modified)
}
```

### 4. ê¸°íƒ€ íŒŒì¼ í˜•ì‹

| í˜•ì‹ | íŒŒì„œ | íŠ¹ì§• | ì£¼ìš” ì²˜ë¦¬ |
|------|------|------|----------|
| **HTML** | `HtmlParser` | ì›¹ ë¬¸ì„œ | BeautifulSoupë¡œ íƒœê·¸ ì œê±°, í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ |
| **Markdown** | `MarkdownParser` | ë§ˆí¬ë‹¤ìš´ | ë¬¸ë²• êµ¬ì¡° ë³´ì¡´í•˜ë©´ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ |
| **TXT** | `TxtParser` | í”Œë ˆì¸ í…ìŠ¤íŠ¸ | ì¸ì½”ë”© ìë™ ê°ì§€ ë° ë³€í™˜ |
| **ZIP** | `ZipParser` | ì••ì¶• íŒŒì¼ | ë‚´ë¶€ íŒŒì¼ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬ |

---

## í…ìŠ¤íŠ¸ ì •ì œ ê³¼ì •

### 1. í…ìŠ¤íŠ¸ ì •ì œ (`TextCleaner.clean_text()`)

#### ì •ì œ ë‹¨ê³„ë³„ ì²˜ë¦¬

```python
# íŒŒì¼: /backend/utils/text_cleaner.py:56-123
def clean_text(text: str) -> str:
    """12ë‹¨ê³„ í…ìŠ¤íŠ¸ ì •ì œ ê³¼ì •"""
    
    # 1ë‹¨ê³„: ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (NFC)
    text = unicodedata.normalize('NFC', text)
    
    # 2ë‹¨ê³„: ì œì–´ ë¬¸ì ì œê±°
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]', '', text)
    
    # 3ë‹¨ê³„: ì‚¬ì„¤ ì˜ì—­ ë¬¸ì ì œê±° (Private Use Area)
    text = re.sub(r'[\uE000-\uF8FF]', '', text)
    text = re.sub(r'[\U000F0000-\U000FFFFF]', '', text)
    
    # 4ë‹¨ê³„: ê¹¨ì§„ í•œê¸€ ìëª¨ ì¡°í•© ì œê±°
    text = re.sub(r'[\u1100-\u11FF\u3130-\u318F]', '', text)
    
    # 5ë‹¨ê³„: ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìŠ¤í¬ë¦½íŠ¸ ì œê±°
    suspicious_scripts = [
        r'[\u0600-\u06FF]',  # ì•„ëì–´
        r'[\u0900-\u097F]',  # ë°ë°”ë‚˜ê°€ë¦¬
        r'[\u0980-\u09FF]',  # ë²µê³¨ì–´
        # ... ê¸°íƒ€ ë¹„ì •ìƒ ìŠ¤í¬ë¦½íŠ¸ë“¤
    ]
    for pattern in suspicious_scripts:
        text = re.sub(pattern, '', text)
    
    # 6ë‹¨ê³„: ë³´ì´ì§€ ì•ŠëŠ” ë¬¸ì ì œê±°
    text = re.sub(r'[\u200B-\u200F\u202A-\u202E]', '', text)  # ì œë¡œí­ ë¬¸ì
    text = re.sub(r'[\uFEFF]', '', text)  # BOM ë¬¸ì
    
    # 7-8ë‹¨ê³„: íŠ¹ì • ê¹¨ì§„ ë¬¸ì íŒ¨í„´ ì œê±°
    broken_patterns = [
        r'[à¢–à¢¿à©¹à©—à³à©‰×¥à¤à¬°à©‰Õ¡à³’Û¨à©‰àª™İ¾à¥§Û…]',
        r'[à«Ó‚Ü»à¢²à®–à£ƒà° Ö™à³ßˆÓàª±à¤§à¤¦à©¢à©¹İà¢šßˆÓ]'
    ]
    for pattern in broken_patterns:
        text = re.sub(pattern, '', text)
    
    # 9ë‹¨ê³„: ê²°í•© ë°œìŒ êµ¬ë³„ ê¸°í˜¸ ì œê±°
    text = re.sub(r'[\u0300-\u036F]', '', text)
    
    # 10ë‹¨ê³„: íƒ­ê³¼ ê°œí–‰ì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜
    text = re.sub(r'[\t\n\r\f\v]', ' ', text)
    
    # 11ë‹¨ê³„: ì—°ì† ê³µë°± í†µí•©
    text = re.sub(r'\s+', ' ', text)
    
    # 12ë‹¨ê³„: ì•ë’¤ ê³µë°± ì œê±°
    return text.strip()
```

### 2. ì‹¤ì œ ì •ì œ ì˜ˆì‹œ

#### ì…ë ¥ (ê¹¨ì§„ ë¬¸ìê°€ í¬í•¨ëœ PDF í…ìŠ¤íŠ¸)
```text
ì‚¼ì„±ì „ìëŠ”à¢–à¢¿à©¹à©— 2023ë…„ì— ì„œìš¸ì‹œà³’Û¨à©‰ ê°•ë‚¨êµ¬ì—ì„œ ìƒˆë¡œìš´	ì—°êµ¬ì†Œë¥¼


	ì„¤ë¦½í–ˆë‹¤.
```

#### ì •ì œ í›„
```text
ì‚¼ì„±ì „ìëŠ” 2023ë…„ì— ì„œìš¸ì‹œ ê°•ë‚¨êµ¬ì—ì„œ ìƒˆë¡œìš´ ì—°êµ¬ì†Œë¥¼ ì„¤ë¦½í–ˆë‹¤.
```

#### ë¡œê·¸ ì¶œë ¥
```
ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ë‹¨ê³„:
  ì›ë³¸ ê¸¸ì´: 89 ë¬¸ì
  ì œì–´ë¬¸ì ì œê±°: 87 ë¬¸ì (-2)
  ê¹¨ì§„ë¬¸ì ì œê±°: 74 ë¬¸ì (-13)
  ê³µë°± ì •ê·œí™”: 72 ë¬¸ì (-2)
  ìµœì¢… ê¸¸ì´: 72 ë¬¸ì (ì´ 17ë¬¸ì ì œê±°)
```

---

## í‚¤ì›Œë“œ í•„í„°ë§ ë° ì •ê·œí™”

### 1. ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ íŒë‹¨ (`is_meaningful_keyword()`)

#### í•„í„°ë§ ê¸°ì¤€

```python
# íŒŒì¼: /backend/utils/text_cleaner.py:126-213
def is_meaningful_keyword(keyword: str) -> bool:
    """ë‹¤ì¤‘ ê²€ì¦ ê¸°ì¤€ìœ¼ë¡œ í‚¤ì›Œë“œ ìœ íš¨ì„± íŒë‹¨"""
    
    # ê¸°ë³¸ ê¸¸ì´ ê²€ì¦
    if len(keyword.strip()) < 2 or len(keyword) > 50:
        return False
    
    # 1. ìœ ë‹ˆì½”ë“œ ìœ íš¨ì„± ê²€ì‚¬
    if not is_valid_unicode(keyword):
        return False
    
    # 2. ìˆ«ìë§Œ êµ¬ì„± ì œì™¸
    if keyword.isdigit():
        return False
    
    # 3. íŠ¹ìˆ˜ë¬¸ìë§Œ êµ¬ì„± ì œì™¸
    if re.match(r'^[^\wê°€-í£]+$', keyword):
        return False
    
    # 4. ë°˜ë³µ ë¬¸ì ì œì™¸ ("aaaaa", "ã…‹ã…‹ã…‹ã…‹")
    if len(set(keyword)) < 2 and len(keyword) > 2:
        return False
    
    # 5. ê¹¨ì§„ ë¬¸ì ë¹„ìœ¨ ê²€ì‚¬ (25% ì´í•˜ë§Œ í—ˆìš©)
    suspicious_chars = count_suspicious_unicode(keyword)
    if suspicious_chars / len(keyword) > 0.25:
        return False
    
    # 6. ì •ìƒ ë¬¸ì ë¹„ìœ¨ ê²€ì‚¬ (50% ì´ìƒ í•„ìš”)
    normal_chars = count_normal_unicode(keyword)
    if normal_chars / len(keyword) < 0.5:
        return False
    
    # 7. ë¶ˆìš©ì–´ ê²€ì‚¬
    if keyword.lower() in KOREAN_STOPWORDS or keyword.lower() in ENGLISH_STOPWORDS:
        return False
    
    # 8. HTML/ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì œì™¸
    if re.match(r'^<[^>]+>$', keyword) or re.match(r'^[#*_`\[\]()]+$', keyword):
        return False
    
    return True
```

### 2. í‚¤ì›Œë“œ ì •ê·œí™” (`normalize_keyword()`)

#### ì •ê·œí™” ë‹¨ê³„

```python
# íŒŒì¼: /backend/utils/text_cleaner.py:272-289
def normalize_keyword(keyword: str) -> str:
    """í‚¤ì›Œë“œ ì •ê·œí™” - í•œêµ­ì–´ ì¡°ì‚¬ ì œê±° í¬í•¨"""
    
    # 1. ìœ ë‹ˆì½”ë“œ NFC ì •ê·œí™”
    normalized = unicodedata.normalize('NFC', keyword)
    
    # 2. ê³µë°± ì •ê·œí™”
    normalized = normalized.strip()
    normalized = re.sub(r'\s+', ' ', normalized)
    
    # 3. í•œêµ­ì–´ ì¡°ì‚¬ ì œê±°
    normalized = remove_korean_particles(normalized)
    
    return normalized
```

#### í•œêµ­ì–´ ì¡°ì‚¬ ì œê±° ì‹œìŠ¤í…œ

**ì§€ì›í•˜ëŠ” ì¡°ì‚¬ íŒ¨í„´** (ìš°ì„ ìˆœìœ„ìˆœ):

| ì¹´í…Œê³ ë¦¬ | ì¡°ì‚¬ ì˜ˆì‹œ | ì •ê·œì‹ íŒ¨í„´ |
|----------|-----------|------------|
| **ê´€í˜•ê²©ì¡°ì‚¬** | ~ì˜ | `r'^(.{2,})ì˜$'` |
| **ë³µí•©ì¡°ì‚¬** | ~ì—ì„œì˜, ~ìœ¼ë¡œëŠ” | `r'^(.{2,})ì—ì„œì˜$'` |
| **ì£¼ê²©ì¡°ì‚¬** | ~ì´, ~ê°€ | `r'^(.{2,})ì´$'` |
| **ëª©ì ê²©ì¡°ì‚¬** | ~ì„, ~ë¥¼ | `r'^(.{2,})ì„$'` |
| **ë¶€ì‚¬ê²©ì¡°ì‚¬** | ~ì—ì„œ, ~ìœ¼ë¡œ | `r'^(.{2,})ì—ì„œ$'` |

**ì¡°ì‚¬ ì œê±° ì˜ˆì‹œ:**
```python
# ì…ë ¥ â†’ ì¶œë ¥
"ì‚¼ì„±ì „ìì˜" â†’ "ì‚¼ì„±ì „ì"
"ì—°êµ¬ì†Œì—ì„œ" â†’ "ì—°êµ¬ì†Œ" 
"ê¸°ìˆ ì„" â†’ "ê¸°ìˆ "
"ê°œë°œìê°€" â†’ "ê°œë°œì"
"í”„ë¡œì íŠ¸ëŠ”" â†’ "í”„ë¡œì íŠ¸"
```

### 3. ë¶ˆìš©ì–´ ì‹œìŠ¤í…œ

#### í•œêµ­ì–´ ë¶ˆìš©ì–´ (26ê°œ)
```python
# íŒŒì¼: /backend/utils/text_cleaner.py:15-26
KOREAN_STOPWORDS = {
    'ìˆìŠµë‹ˆë‹¤', 'ê°™ìŠµë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'ë•Œë¬¸ì—', 
    'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ë˜í•œ', 'ì´ê²ƒì€', 'ê·¸ê²ƒì€', 'ì•„ë‹ˆë‹¤',
    'ë¬´ì—‡', 'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ëˆ„ê°€',
    'ê·¸ëŸ°ë°', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ê·¸ë ‡ì§€ë§Œ', 
    'ì´ëŸ°', 'ì €ëŸ°', 'ê·¸ëŸ°', 'ìš°ë¦¬', 'ì €í¬', 'ê·¸ë“¤'
}
```

#### ì˜ì–´ ë¶ˆìš©ì–´ (60ê°œ)
```python
# íŒŒì¼: /backend/utils/text_cleaner.py:29-40
ENGLISH_STOPWORDS = {
    'and', 'the', 'for', 'are', 'but', 'not', 'you', 'all',
    'this', 'that', 'these', 'those', 'very', 'just', 'only',
    'about', 'after', 'again', 'against', 'before', 'being',
    'would', 'could', 'should', 'might', 'must', 'shall'
    # ... ì´ 60ê°œ
}
```

---

## ì‹¤ì œ ì²˜ë¦¬ ì˜ˆì œ

### ì˜ˆì œ 1: PDF ì—°êµ¬ë…¼ë¬¸ ì²˜ë¦¬

#### ì…ë ¥ íŒŒì¼
```
íŒŒì¼ëª…: "AI_Research_2023.pdf"
í¬ê¸°: 2.5MB, 15í˜ì´ì§€
ë‚´ìš©: ê¹¨ì§„ ë¬¸ìê°€ í¬í•¨ëœ AI ì—°êµ¬ë…¼ë¬¸
```

#### ì²˜ë¦¬ ê³¼ì •

**1ë‹¨ê³„: íŒŒì¼ íŒŒì‹±**
```
ğŸ“– PDF íŒŒì‹± ì‹œì‘: AI_Research_2023.pdf
ğŸ”„ pymupdf4llm ì—”ì§„ìœ¼ë¡œ ì‹œë„ ì¤‘...
ğŸ“Š pymupdf4llm í’ˆì§ˆ ì ìˆ˜: 0.89 (ê¸¸ì´: 45623)
âœ… pymupdf4llm ì—”ì§„ìœ¼ë¡œ ê³ í’ˆì§ˆ ì¶”ì¶œ ì„±ê³µ
```

**2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ì œ**
```
ì›ë³¸ í…ìŠ¤íŠ¸ (ì¼ë¶€):
"ë”¥ëŸ¬ë‹ ê¸°ìˆ ì€à¢–à¢¿à©¹à©— ìµœê·¼ ëª‡ ë…„ê°„à³’Û¨à©‰	ê¸‰ì†ë„ë¡œ ë°œì „í•˜ê³  ìˆë‹¤.
ì¸ê³µì§€ëŠ¥ ì—°êµ¬ìë“¤ì€    ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ì—¬..."

ì •ì œ í›„:
"ë”¥ëŸ¬ë‹ ê¸°ìˆ ì€ ìµœê·¼ ëª‡ ë…„ê°„ ê¸‰ì†ë„ë¡œ ë°œì „í•˜ê³  ìˆë‹¤.
ì¸ê³µì§€ëŠ¥ ì—°êµ¬ìë“¤ì€ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ì—¬..."
```

**3ë‹¨ê³„: í‚¤ì›Œë“œ í›„ë³´ ìƒì„±**
```
ì¶”ì¶œëœ í‚¤ì›Œë“œ í›„ë³´ (spaCy NER):
- "ë”¥ëŸ¬ë‹" (ê¸°ìˆ ë¶„ì•¼)
- "ì¸ê³µì§€ëŠ¥" (ê¸°ìˆ ë¶„ì•¼)  
- "ì—°êµ¬ìë“¤" â†’ "ì—°êµ¬ì" (ì¡°ì‚¬ ì œê±°)
- "ì•Œê³ ë¦¬ì¦˜ì„" â†’ "ì•Œê³ ë¦¬ì¦˜" (ì¡°ì‚¬ ì œê±°)
- "2023ë…„" (ë‚ ì§œ)
- "ì‚¼ì„±ì „ì" (ê¸°ê´€ëª…)
```

### ì˜ˆì œ 2: DOCX ë³´ê³ ì„œ ì²˜ë¦¬

#### ì…ë ¥ íŒŒì¼
```
íŒŒì¼ëª…: "quarterly_report.docx"
ë‚´ìš©: í‘œì™€ ë³¸ë¬¸ì´ í˜¼ì¬ëœ ë¶„ê¸° ë³´ê³ ì„œ
```

#### ì²˜ë¦¬ ê²°ê³¼
```json
{
  "text": "2023ë…„ 3ë¶„ê¸° ì‹¤ì  ë³´ê³ ì„œ\nì‚¼ì„±ì „ì ë§¤ì¶œ 75ì¡°ì› ë‹¬ì„±\nì „ë…„ ëŒ€ë¹„ 15% ì¦ê°€\nì£¼ìš” ì œí’ˆ\tìŠ¤ë§ˆíŠ¸í°\t40%\në°˜ë„ì²´\t35%\nê°€ì „ì œí’ˆ\t25%",
  "metadata": {
    "title": "2023ë…„ 3ë¶„ê¸° ì‹¤ì  ë³´ê³ ì„œ",
    "author": "ì¬ë¬´íŒ€",
    "word_count": 156,
    "created_date": "2023-10-15T09:30:00"
  }
}
```

### ì˜ˆì œ 3: HTML ì›¹í˜ì´ì§€ ì²˜ë¦¬

#### ì…ë ¥ HTML
```html
<html>
<head><title>íšŒì‚¬ ì†Œê°œ</title></head>
<body>
  <h1>ì‚¼ì„±ì „ì ì†Œê°œ</h1>
  <p>ì‚¼ì„±ì „ìëŠ” <strong>ë°˜ë„ì²´</strong>ì™€ 
     <em>ìŠ¤ë§ˆíŠ¸í°</em>ì„ ì œì¡°í•˜ëŠ” íšŒì‚¬ì…ë‹ˆë‹¤.</p>
  <!-- ì£¼ì„ì€ ì œê±°ë¨ -->
  <script>alert('script removed');</script>
</body>
</html>
```

#### ì •ì œ ê²°ê³¼
```text
íšŒì‚¬ ì†Œê°œ
ì‚¼ì„±ì „ì ì†Œê°œ
ì‚¼ì„±ì „ìëŠ” ë°˜ë„ì²´ì™€ ìŠ¤ë§ˆíŠ¸í°ì„ ì œì¡°í•˜ëŠ” íšŒì‚¬ì…ë‹ˆë‹¤.
```

---

## ì„±ëŠ¥ ë° ìµœì í™”

### 1. ì²˜ë¦¬ ì†ë„ ë²¤ì¹˜ë§ˆí¬

| íŒŒì¼ í˜•ì‹ | í‰ê·  ì²˜ë¦¬ ì‹œê°„ (1MBë‹¹) | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ |
|-----------|----------------------|-------------|
| **TXT** | 0.1ì´ˆ | 5MB |
| **HTML** | 0.3ì´ˆ | 10MB |
| **DOCX** | 0.8ì´ˆ | 15MB |
| **PDF** | 2.5ì´ˆ | 25MB |
| **ZIP** | ê°€ë³€ì  | ê°€ë³€ì  |

### 2. í’ˆì§ˆ í‰ê°€ ê¸°ì¤€

#### PDF í’ˆì§ˆ ì ìˆ˜ ë¶„í¬
```
0.9 ~ 1.0: ì™„ë²½ (5%)
0.8 ~ 0.9: ìš°ìˆ˜ (25%) 
0.7 ~ 0.8: ì–‘í˜¸ (40%)
0.6 ~ 0.7: ë³´í†µ (20%)
0.5 ~ 0.6: ë¶ˆëŸ‰ (10%)
```

#### í…ìŠ¤íŠ¸ ì •ì œ íš¨ê³¼
```
í‰ê·  ë¬¸ì ì œê±°ìœ¨: 12%
  - ì œì–´ë¬¸ì: 3%
  - ê¹¨ì§„ë¬¸ì: 7%
  - ê³µë°±ì •ê·œí™”: 2%

í‚¤ì›Œë“œ í•„í„°ë§ìœ¨: 35%
  - ë¶ˆìš©ì–´: 15%
  - ë¬´ì˜ë¯¸ í† í°: 12%
  - ê¹¨ì§„ë¬¸ì: 8%
```

### 3. ë©”ëª¨ë¦¬ ìµœì í™” ë°©ë²•

```python
# 1. ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ëŒ€ìš©ëŸ‰ íŒŒì¼)
def process_large_file(file_path: Path):
    with open(file_path, 'r', encoding='utf-8') as f:
        for chunk in read_chunks(f, chunk_size=1024*1024):  # 1MBì”©
            cleaned_chunk = TextCleaner.clean_text(chunk)
            yield cleaned_chunk

# 2. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
import gc
def cleanup_after_parsing():
    gc.collect()  # ë©”ëª¨ë¦¬ í•´ì œ

# 3. ì„ì‹œ íŒŒì¼ ì‚¬ìš© (ì´ˆëŒ€ìš©ëŸ‰)
def use_temp_files_for_huge_documents():
    with tempfile.NamedTemporaryFile(mode='w+', delete=False) as temp:
        # ì„ì‹œ íŒŒì¼ì— ì¤‘ê°„ ê²°ê³¼ ì €ì¥
        pass
```

### 4. ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µì›ë ¥

```python
# íŒŒì„œ ì²´ì¸ ì‹œìŠ¤í…œ
def robust_parsing(file_path: Path):
    parsers = [PrimaryParser(), FallbackParser(), LastResortParser()]
    
    for parser in parsers:
        try:
            result = parser.parse(file_path)
            if result.success and result.quality > 0.5:
                return result
        except Exception as e:
            logger.warning(f"Parser {parser.name} failed: {e}")
            continue
    
    return create_error_result("ëª¨ë“  íŒŒì„œê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
```

---

## ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§

### 1. ìƒì„¸ ë¡œê¹… ì‹œìŠ¤í…œ

```python
# ì²˜ë¦¬ ë‹¨ê³„ë³„ ë¡œê¹…
logger.info(f"ğŸ“– íŒŒì¼ íŒŒì‹± ì‹œì‘: {file_name}")
logger.info(f"ğŸ”„ {engine_name} ì—”ì§„ìœ¼ë¡œ ì‹œë„ ì¤‘...")
logger.info(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f}")
logger.info(f"ğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ: {original_length} â†’ {cleaned_length}")
logger.info(f"âœ… í‚¤ì›Œë“œ í•„í„°ë§: {total_candidates} â†’ {valid_keywords}")
```

### 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
@timer_decorator
def parse_with_metrics(file_path: Path):
    start_time = time.time()
    memory_before = get_memory_usage()
    
    result = parse_file(file_path)
    
    processing_time = time.time() - start_time
    memory_after = get_memory_usage()
    memory_used = memory_after - memory_before
    
    metrics = {
        "file_size": file_path.stat().st_size,
        "processing_time": processing_time,
        "memory_used": memory_used,
        "characters_processed": len(result.text),
        "quality_score": result.quality_score
    }
    
    log_performance_metrics(metrics)
    return result
```

### 3. ë¬¸ì œ ì§„ë‹¨ ë„êµ¬

```python
def diagnose_parsing_issues(file_path: Path):
    """íŒŒì‹± ë¬¸ì œ ì§„ë‹¨ ë° í•´ê²°ì±… ì œì‹œ"""
    diagnosis = {
        "file_info": get_file_info(file_path),
        "encoding_issues": detect_encoding_problems(file_path),
        "corruption_signs": detect_file_corruption(file_path),
        "recommended_parser": suggest_best_parser(file_path),
        "expected_quality": estimate_parsing_quality(file_path)
    }
    
    return diagnosis
```

---

## ì°¸ê³  ìë£Œ ë° ì„¤ì •

### ì£¼ìš” êµ¬í˜„ íŒŒì¼
- **ì „ì²´ íŒŒì‹±**: `/backend/services/parser/auto_parser.py` (ë¼ì¸ 12-236)
- **PDF ì²˜ë¦¬**: `/backend/services/parser/pdf_parser.py` (ë¼ì¸ 7-372)
- **í…ìŠ¤íŠ¸ ì •ì œ**: `/backend/utils/text_cleaner.py` (ë¼ì¸ 11-370)
- **DOCX ì²˜ë¦¬**: `/backend/services/parser/docx_parser.py` (ë¼ì¸ 6-100)
- **HTML ì²˜ë¦¬**: `/backend/services/parser/html_parser.py`
- **Markdown ì²˜ë¦¬**: `/backend/services/parser/md_parser.py`

### ì„¤ì • ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜
```python
TEXT_CLEANING_CONFIG = {
    "remove_suspicious_unicode": True,
    "normalize_whitespace": True,
    "min_keyword_length": 2,
    "max_keyword_length": 50,
    "suspicious_char_threshold": 0.25,
    "normal_char_threshold": 0.5
}

PDF_PARSING_CONFIG = {
    "try_all_engines": True,
    "quality_threshold": 0.5,
    "timeout_seconds": 300,
    "fallback_to_ocr": False  # í–¥í›„ OCR ì§€ì› ì˜ˆì •
}
```

### ì§€ì›ë˜ëŠ” ë¬¸ì ì¸ì½”ë”©
- **UTF-8** (ê¸°ë³¸)
- **UTF-16** (ìë™ ê°ì§€)
- **EUC-KR** (í•œêµ­ì–´ ë ˆê±°ì‹œ)
- **CP949** (í™•ì¥ í•œêµ­ì–´)
- **ISO-8859-1** (ë¼í‹´ ë¬¸ì)

ì´ ë¬¸ì„œëŠ” DocExtract ì‹œìŠ¤í…œì˜ íŒŒì¼ ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê³¼ì •ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤. ê° ë‹¨ê³„ë³„ ë¡œê¹…ê³¼ ì˜ˆì œë¥¼ í†µí•´ ì‹¤ì œ ë™ì‘ ë°©ì‹ì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.