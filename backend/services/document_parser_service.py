"""
ë¬¸ì„œ íŒŒì‹± ì „ìš© ì„œë¹„ìŠ¤
ëª¨ë“  íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ì™„ì „í•œ íŒŒì‹±ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
"""
import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime

from services.parser.pdf_parser import PdfParser
from services.parser.docling_parser import DoclingParser
from services.parser.docx_parser import DocxParser
from services.parser.txt_parser import TxtParser
from services.parser.html_parser import HtmlParser
from services.parser.md_parser import MarkdownParser
from services.parser.zip_parser import ZipParser
from services.parser.base import ParseResult, DocumentMetadata

logger = logging.getLogger(__name__)


class DocumentParserService:
    """ë¬¸ì„œ íŒŒì‹± ì „ìš© ì„œë¹„ìŠ¤ - ëª¨ë“  íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ì™„ì „í•œ íŒŒì‹± ìˆ˜í–‰"""
    
    def __init__(self):
        self.parsers = {
            'pdf': [
                ('docling', DoclingParser()),
                ('pdf_parser', PdfParser())
            ],
            'docx': [('docx_parser', DocxParser())],
            'txt': [('txt_parser', TxtParser())],
            'html': [('html_parser', HtmlParser())],
            'htm': [('html_parser', HtmlParser())], 
            'md': [('md_parser', MarkdownParser())],
            'zip': [('zip_parser', ZipParser())]
        }
        
    def get_output_directory(self, file_path: Path, directory: Optional[Path] = None) -> Path:
        """íŒŒì¼ë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë°˜í™˜"""
        if directory:
            return directory / file_path.stem
        return file_path.parent / file_path.stem
        
    def get_parsing_result_path(self, file_path: Path, directory: Optional[Path] = None) -> Path:
        """íŒŒì‹± ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ"""
        output_dir = self.get_output_directory(file_path, directory)
        return output_dir / "parsing_results.json"
        
    def has_parsing_results(self, file_path: Path, directory: Optional[Path] = None) -> bool:
        """íŒŒì‹± ê²°ê³¼ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        result_path = self.get_parsing_result_path(file_path, directory)
        return result_path.exists()
        
    def load_existing_parsing_results(self, file_path: Path, directory: Optional[Path] = None) -> Optional[Dict[str, Any]]:
        """ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ ë¡œë“œ"""
        if not self.has_parsing_results(file_path, directory):
            return None
            
        try:
            result_path = self.get_parsing_result_path(file_path, directory)
            with open(result_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"íŒŒì‹± ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
            
    def parse_document_comprehensive(
        self, 
        file_path: Path, 
        force_reparse: bool = False,
        directory: Optional[Path] = None
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œë¥¼ ëª¨ë“  ì ìš© ê°€ëŠ¥í•œ íŒŒì„œë¡œ ì™„ì „ íŒŒì‹±
        
        Args:
            file_path: íŒŒì‹±í•  ë¬¸ì„œ ê²½ë¡œ
            force_reparse: ê¸°ì¡´ ê²°ê³¼ ë¬´ì‹œí•˜ê³  ì¬íŒŒì‹± ì—¬ë¶€
            
        Returns:
            íŒŒì‹± ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ğŸ“„ ë¬¸ì„œ ì™„ì „ íŒŒì‹± ì‹œì‘: {file_path.name}")
        
        # ê¸°ì¡´ ê²°ê³¼ í™•ì¸
        if not force_reparse and self.has_parsing_results(file_path, directory):
            logger.info("ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ ì¬ì‚¬ìš©")
            return self.load_existing_parsing_results(file_path, directory)
            
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = self.get_output_directory(file_path, directory)
        output_dir.mkdir(exist_ok=True)
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        extension = file_path.suffix.lower().lstrip('.')
        if extension not in self.parsers:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {extension}")
            
        parsing_results = {
            "file_info": {
                "name": file_path.name,
                "path": str(file_path),
                "size": file_path.stat().st_size,
                "extension": extension,
                "modified": file_path.stat().st_mtime
            },
            "parsing_timestamp": datetime.now().isoformat(),
            "parsers_used": [],
            "parsing_results": {},
            "summary": {
                "total_parsers": 0,
                "successful_parsers": 0,
                "failed_parsers": 0,
                "best_parser": None,
                "best_quality_score": 0
            }
        }
        
        # ëª¨ë“  ì ìš© ê°€ëŠ¥í•œ íŒŒì„œë¡œ íŒŒì‹± ì‹œë„
        applicable_parsers = self.parsers[extension]
        parsing_results["summary"]["total_parsers"] = len(applicable_parsers)
        
        for parser_name, parser in applicable_parsers:
            try:
                logger.info(f"ğŸ”„ {parser_name} íŒŒì„œë¡œ íŒŒì‹± ì‹œë„")
                result = parser.parse(file_path)
                
                if result.success:
                    parsing_results["summary"]["successful_parsers"] += 1
                    parsing_results["parsers_used"].append(parser_name)
                    
                    # ê°œë³„ íŒŒì„œ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (Markdown íŒŒì¼ ì´ë™ í¬í•¨)
                    self._save_individual_parser_result(output_dir, parser_name, result)
                    
                    # íŒŒì‹± ê²°ê³¼ ì €ì¥ (íŒŒì¼ ì €ì¥ í›„ ì—…ë°ì´íŠ¸ëœ ê²½ë¡œ ì‚¬ìš©)
                    parser_result = {
                        "success": True,
                        "parser_name": result.parser_name,
                        "text_length": len(result.text),
                        "word_count": len(result.text.split()) if result.text else 0,
                        "metadata": self._serialize_metadata(result.metadata) if result.metadata else None,
                        "md_file_path": result.md_file_path,  # ì´ë™ í›„ ì—…ë°ì´íŠ¸ëœ ê²½ë¡œ ì‚¬ìš©
                        "parsing_time": getattr(result, 'parsing_time', None)
                    }
                    
                    # í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                    quality_score = self._calculate_text_quality(result.text)
                    parser_result["quality_score"] = quality_score
                    
                    # ìµœê³  í’ˆì§ˆ íŒŒì„œ ì¶”ì 
                    if quality_score > parsing_results["summary"]["best_quality_score"]:
                        parsing_results["summary"]["best_quality_score"] = quality_score
                        parsing_results["summary"]["best_parser"] = parser_name
                    
                    # êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ (í•´ë‹¹ íŒŒì„œì¸ ê²½ìš°)
                    structured_info = self._extract_structured_info(result, parser_name)
                    if structured_info:
                        parser_result["structured_info"] = structured_info
                    
                    parsing_results["parsing_results"][parser_name] = parser_result
                    
                    logger.info(f"âœ… {parser_name} íŒŒì‹± ì„±ê³µ (í’ˆì§ˆ: {quality_score:.2f})")
                    
                else:
                    parsing_results["summary"]["failed_parsers"] += 1
                    parsing_results["parsing_results"][parser_name] = {
                        "success": False,
                        "error_message": result.error_message,
                        "parser_name": result.parser_name
                    }
                    logger.warning(f"âŒ {parser_name} íŒŒì‹± ì‹¤íŒ¨: {result.error_message}")
                    
            except Exception as e:
                parsing_results["summary"]["failed_parsers"] += 1
                parsing_results["parsing_results"][parser_name] = {
                    "success": False,
                    "error_message": str(e),
                    "parser_name": parser_name
                }
                logger.error(f"ğŸ’¥ {parser_name} íŒŒì„œ ì˜¤ë¥˜: {e}")
        
        # ì „ì²´ ê²°ê³¼ ì €ì¥
        self._save_comprehensive_results(file_path, parsing_results, directory)
        
        logger.info(f"ğŸ“‹ ì™„ì „ íŒŒì‹± ì™„ë£Œ: ì„±ê³µ {parsing_results['summary']['successful_parsers']}/{parsing_results['summary']['total_parsers']}")
        return parsing_results
        
    def _serialize_metadata(self, metadata: DocumentMetadata) -> Dict[str, Any]:
        """DocumentMetadataë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        if not metadata:
            return None
            
        result = {}
        for field in metadata.__dataclass_fields__:
            value = getattr(metadata, field)
            if value is not None:
                # ë³µì¡í•œ ê°ì²´ë“¤ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
                if isinstance(value, (list, dict)):
                    result[field] = value
                else:
                    result[field] = str(value) if not isinstance(value, (str, int, float, bool)) else value
        
        return result
        
    def _calculate_text_quality(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if not text:
            return 0.0
            
        # ê¸°ë³¸ í’ˆì§ˆ ì§€í‘œë“¤
        length_score = min(len(text) / 1000, 1.0) * 0.3  # ê¸¸ì´ ì ìˆ˜ (ìµœëŒ€ 30%)
        
        # ë‹¨ì–´ ë‹¤ì–‘ì„± ì ìˆ˜
        words = text.split()
        if words:
            unique_words = set(words)
            diversity_score = min(len(unique_words) / len(words), 1.0) * 0.3  # ë‹¤ì–‘ì„± ì ìˆ˜ (ìµœëŒ€ 30%)
        else:
            diversity_score = 0
        
        # ë¬¸ì¥ êµ¬ì¡° ì ìˆ˜ (ë§ˆì¹¨í‘œ, ì‰¼í‘œ ë“±ì˜ ì¡´ì¬)
        punctuation_count = sum(1 for c in text if c in '.!?,:;')
        structure_score = min(punctuation_count / len(text) * 100, 1.0) * 0.2  # êµ¬ì¡° ì ìˆ˜ (ìµœëŒ€ 20%)
        
        # í•œê¸€/ì˜ì–´ ë¬¸ì ë¹„ìœ¨ (ì˜ë¯¸ ìˆëŠ” í…ìŠ¤íŠ¸ ì—¬ë¶€)
        meaningful_chars = sum(1 for c in text if c.isalnum() or c.isspace())
        if len(text) > 0:
            meaning_score = (meaningful_chars / len(text)) * 0.2  # ì˜ë¯¸ ì ìˆ˜ (ìµœëŒ€ 20%)
        else:
            meaning_score = 0
            
        total_score = length_score + diversity_score + structure_score + meaning_score
        return min(total_score, 1.0)
        
    def _extract_structured_info(self, result: ParseResult, parser_name: str) -> Optional[Dict[str, Any]]:
        """íŒŒì„œë³„ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ"""
        structured_info = {}
        
        if parser_name == 'docling' and result.metadata:
            # Docling íŒŒì„œì˜ ê²½ìš° í…Œì´ë¸”, ì´ë¯¸ì§€ ë“± êµ¬ì¡° ì •ë³´ ì¶”ì¶œ
            if hasattr(result.metadata, 'document_structure'):
                structured_info['document_structure'] = result.metadata.document_structure
            if hasattr(result.metadata, 'tables_count'):
                structured_info['tables_count'] = result.metadata.tables_count
            if hasattr(result.metadata, 'images_count'):
                structured_info['images_count'] = result.metadata.images_count
                
        elif parser_name == 'pdf_parser':
            # PDF íŒŒì„œì˜ ê²½ìš° í˜ì´ì§€ ì •ë³´ ë“±
            if result.metadata and result.metadata.page_count:
                structured_info['pages'] = result.metadata.page_count
                
        # ê³µí†µ êµ¬ì¡° ì •ë³´
        if result.text:
            lines = result.text.split('\n')
            structured_info.update({
                'total_lines': len(lines),
                'non_empty_lines': len([l for l in lines if l.strip()]),
                'paragraphs': len([l for l in lines if l.strip() and not l.startswith('#')]),
                'headers': len([l for l in lines if l.strip().startswith('#')])
            })
        
        return structured_info if structured_info else None
        
    def _save_individual_parser_result(self, output_dir: Path, parser_name: str, result: ParseResult):
        """ê°œë³„ íŒŒì„œ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # íŒŒì„œë³„ ë””ë ‰í† ë¦¬ ìƒì„±
            parser_dir = output_dir / parser_name
            parser_dir.mkdir(exist_ok=True)
            
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
            text_file = parser_dir / f"{parser_name}_text.txt"
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(result.text or "")
            
            # ë©”íƒ€ë°ì´í„° JSON ì €ì¥
            if result.metadata:
                metadata_file = parser_dir / f"{parser_name}_metadata.json"
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(self._serialize_metadata(result.metadata), f, ensure_ascii=False, indent=2)
            
            # êµ¬ì¡°í™”ëœ ì •ë³´ ì €ì¥ (í•´ë‹¹í•˜ëŠ” ê²½ìš°)
            structured_info = self._extract_structured_info(result, parser_name)
            if structured_info:
                structure_file = parser_dir / f"{parser_name}_structure.json"
                with open(structure_file, 'w', encoding='utf-8') as f:
                    json.dump(structured_info, f, ensure_ascii=False, indent=2)
            
            # Markdown íŒŒì¼ì„ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ë¡œ ì´ë™ (docling ë° pymupdf4llm íŒŒì„œì˜ ê²½ìš°)
            if result.md_file_path and Path(result.md_file_path).exists():
                import shutil
                source_md_file = Path(result.md_file_path)
                
                # ì›ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
                target_md_file = output_dir / source_md_file.name
                try:
                    shutil.move(str(source_md_file), str(target_md_file))
                    logger.info(f"ğŸ“ Markdown íŒŒì¼ ì´ë™: {source_md_file} â†’ {target_md_file}")
                    
                    # ParseResultì˜ md_file_path ì—…ë°ì´íŠ¸
                    result.md_file_path = str(target_md_file)
                except Exception as move_error:
                    logger.warning(f"âš ï¸ Markdown íŒŒì¼ ì´ë™ ì‹¤íŒ¨: {move_error}")
            
            logger.info(f"ğŸ“ {parser_name} ê°œë³„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {parser_dir}")
            
        except Exception as e:
            logger.error(f"âŒ {parser_name} ê°œë³„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            
    def _save_comprehensive_results(self, file_path: Path, results: Dict[str, Any], directory: Optional[Path] = None):
        """ì „ì²´ íŒŒì‹± ê²°ê³¼ë¥¼ ì¢…í•© íŒŒì¼ë¡œ ì €ì¥"""
        try:
            result_path = self.get_parsing_result_path(file_path, directory)
            with open(result_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ì¢…í•© íŒŒì‹± ê²°ê³¼ ì €ì¥: {result_path}")
            
        except Exception as e:
            logger.error(f"âŒ ì¢…í•© ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            
    def get_supported_extensions(self) -> List[str]:
        """ì§€ì›ë˜ëŠ” íŒŒì¼ í™•ì¥ì ëª©ë¡ ë°˜í™˜"""
        return list(self.parsers.keys())
        
    def is_supported_file(self, file_path: Path) -> bool:
        """íŒŒì¼ì´ ì§€ì›ë˜ëŠ” í˜•ì‹ì¸ì§€ í™•ì¸"""
        extension = file_path.suffix.lower().lstrip('.')
        return extension in self.parsers