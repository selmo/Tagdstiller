"""
ì²­í¬ë³„ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ë° ë¡œê¹… ì‹œìŠ¤í…œ

ê° ì²­í¬ì— ëŒ€í•´ ê°œë³„ì ì¸ í”„ë¡¬í”„íŠ¸ ìƒì„±, ì‹¤í–‰ ë¡œê·¸, ê²°ê³¼ íŒŒì¼ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import uuid

from prompts.templates import get_prompt_template, list_available_templates


@dataclass
class ChunkPromptRequest:
    """ì²­í¬ í”„ë¡¬í”„íŠ¸ ìš”ì²­ ì •ë³´"""
    chunk_id: str
    chunk_level: str
    content: str
    content_length: int

    # í”„ë¡¬í”„íŠ¸ ì„¤ì •
    prompt_category: str  # "keyword_extraction", "document_summary", etc.
    prompt_template: str  # "basic_ko", "academic", etc.
    prompt_parameters: Dict[str, Any]

    # ë©”íƒ€ë°ì´í„°
    source_file: str
    chunk_context: Dict[str, Any]
    request_timestamp: str = None

    def __post_init__(self):
        if self.request_timestamp is None:
            self.request_timestamp = datetime.now().isoformat()


@dataclass
class ChunkPromptResult:
    """ì²­í¬ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ ê²°ê³¼"""
    request_id: str
    chunk_id: str

    # í”„ë¡¬í”„íŠ¸ ì •ë³´
    generated_prompt: str
    prompt_length: int

    # ì‹¤í–‰ ê²°ê³¼
    llm_response: str
    response_length: int
    processing_time_seconds: float

    # ìƒíƒœ ì •ë³´
    success: bool
    completion_timestamp: str

    # ì„ íƒì  í•„ë“œë“¤
    error_message: Optional[str] = None
    llm_provider: str = "unknown"
    llm_model: str = "unknown"

    def __post_init__(self):
        if not hasattr(self, 'completion_timestamp') or self.completion_timestamp is None:
            self.completion_timestamp = datetime.now().isoformat()


class ChunkPromptManager:
    """ì²­í¬ë³„ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ì‹œìŠ¤í…œ"""

    def __init__(self, base_output_directory: str):
        self.base_output_directory = Path(base_output_directory)
        self.logger = logging.getLogger(__name__)

        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
        self.prompts_dir = self.base_output_directory / "chunk_prompts"
        self.logs_dir = self.base_output_directory / "chunk_logs"
        self.results_dir = self.base_output_directory / "chunk_results"

        for directory in [self.prompts_dir, self.logs_dir, self.results_dir]:
            directory.mkdir(parents=True, exist_ok=True)

        self.logger.info(f"ğŸ“ ì²­í¬ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ì ì´ˆê¸°í™”: {self.base_output_directory}")

    def create_chunk_prompt_set(
        self,
        chunk_info: Dict[str, Any],
        source_file: str,
        prompt_categories: List[str] = None
    ) -> List[ChunkPromptRequest]:
        """
        ì²­í¬ì— ëŒ€í•œ ì „ì²´ í”„ë¡¬í”„íŠ¸ ì„¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            chunk_info: ì²­í¬ ì •ë³´
            source_file: ì›ë³¸ íŒŒì¼ ê²½ë¡œ
            prompt_categories: ìƒì„±í•  í”„ë¡¬í”„íŠ¸ ì¹´í…Œê³ ë¦¬ ëª©ë¡

        Returns:
            List[ChunkPromptRequest]: í”„ë¡¬í”„íŠ¸ ìš”ì²­ ëª©ë¡
        """
        chunk_id = chunk_info.get('chunk_id', 'unknown')
        chunk_level = chunk_info.get('level', 'unknown')

        # ìš°ì„ ìˆœìœ„ë¡œ ì „ì²´ ì½˜í…ì¸  ì¶”ì¶œ ì‹œë„
        content = ''

        # 1ìˆœìœ„: ChunkAnalyzerì—ì„œ ì œê³µí•œ full_content
        if 'full_content' in chunk_info:
            content = chunk_info['full_content']
            self.logger.info(f"âœ… full_content í‚¤ ì‚¬ìš©: {len(content)}ì")

        # 2ìˆœìœ„: chunks_text ë””ë ‰í† ë¦¬ì—ì„œ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°
        elif Path(self.output_directory).exists():
            chunk_text_file = Path(self.output_directory) / "chunks_text" / f"{chunk_id}.txt"
            if chunk_text_file.exists():
                content = chunk_text_file.read_text(encoding='utf-8')
                self.logger.info(f"âœ… ì²­í¬ í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì „ì²´ ì½˜í…ì¸  ë¡œë“œ: {len(content)}ì")

        # 3ìˆœìœ„: content_preview ì‚¬ìš©
        if not content or len(content.strip()) < 50:
            content = chunk_info.get('content_preview', '')
            self.logger.info(f"ğŸ”„ content_preview í´ë°± ì‚¬ìš©: {len(content)}ì")

        # ë””ë²„ê·¸: ìµœì¢… ì½˜í…ì¸  í™•ì¸
        self.logger.info(f"ğŸ” ì²­í¬ {chunk_id} ìµœì¢… ì½˜í…ì¸  ê¸¸ì´: {len(content)}ì")
        if not content:
            all_keys = list(chunk_info.keys())
            self.logger.warning(f"âš ï¸ ëª¨ë“  ë°©ë²•ìœ¼ë¡œë„ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ë“¤: {all_keys}")

            # ë§ˆì§€ë§‰ ëŒ€ì•ˆìœ¼ë¡œ ë‹¤ë¥¸ ì½˜í…ì¸  í‚¤ë“¤ì„ ì‹œë„
            for alt_key in ['content', 'text', 'full_content']:
                if alt_key in chunk_info and chunk_info[alt_key]:
                    content = chunk_info[alt_key]
                    self.logger.info(f"âœ… ìµœì¢… ëŒ€ì•ˆ í‚¤ '{alt_key}' ì‚¬ìš©: {len(content)}ì")
                    break

        # ìµœì¢… ê²€ì¦: ì½˜í…ì¸ ê°€ ì—¬ì „íˆ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬
        if not content or len(content.strip()) == 0:
            error_msg = f"ì²­í¬ {chunk_id}ì˜ ì½˜í…ì¸ ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤."
            self.logger.error(f"âŒ {error_msg}")
            return []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì¹´í…Œê³ ë¦¬ ì„¤ì •
        if prompt_categories is None:
            prompt_categories = ['keyword_extraction', 'document_summary', 'structure_analysis']

        requests = []

        self.logger.info(f"ğŸ¯ ì²­í¬ {chunk_id}ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ ì„¸íŠ¸ ìƒì„± ì‹œì‘ (ìµœì¢… ì½˜í…ì¸  ê¸¸ì´: {len(content)}ì)")

        for category in prompt_categories:
            category_requests = self._create_category_prompts(
                chunk_info=chunk_info,
                source_file=source_file,
                category=category,
                content=content,
                chunk_id=chunk_id,
                chunk_level=chunk_level
            )
            requests.extend(category_requests)

        # í”„ë¡¬í”„íŠ¸ ìš”ì²­ë“¤ì„ íŒŒì¼ë¡œ ì €ì¥
        self._save_prompt_requests(chunk_id, requests)

        self.logger.info(f"âœ… ì²­í¬ {chunk_id}: {len(requests)}ê°œ í”„ë¡¬í”„íŠ¸ ìš”ì²­ ìƒì„± ì™„ë£Œ")
        return requests

    def _create_category_prompts(
        self,
        chunk_info: Dict[str, Any],
        source_file: str,
        category: str,
        content: str,
        chunk_id: str,
        chunk_level: str
    ) -> List[ChunkPromptRequest]:
        """ì¹´í…Œê³ ë¦¬ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        requests = []
        available_templates = list_available_templates()

        if category not in available_templates:
            self.logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” í”„ë¡¬í”„íŠ¸ ì¹´í…Œê³ ë¦¬: {category}")
            return requests

        templates = available_templates[category]

        for template_name in templates:
            try:
                # í…œí”Œë¦¿ë³„ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ìƒì„±
                parameters = self._get_template_parameters(category, template_name, content)

                request = ChunkPromptRequest(
                    chunk_id=chunk_id,
                    chunk_level=chunk_level,
                    content=content,
                    content_length=len(content),
                    prompt_category=category,
                    prompt_template=template_name,
                    prompt_parameters=parameters,
                    source_file=source_file,
                    chunk_context=chunk_info
                )

                requests.append(request)

            except Exception as e:
                self.logger.error(f"âŒ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨ ({category}.{template_name}): {str(e)}")

        return requests

    def _get_template_parameters(self, category: str, template_name: str, content: str) -> Dict[str, Any]:
        """í…œí”Œë¦¿ë³„ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ìƒì„±"""

        base_params = {
            'text': content
        }

        if category == 'keyword_extraction':
            base_params.update({
                'max_keywords': 10,
            })
        elif category == 'knowledge_graph':
            base_params.update({
                'domain': 'general',
                'structure_info': '{"type": "chunk_analysis"}'
            })

        return base_params

    def generate_prompt(self, request: ChunkPromptRequest) -> str:
        """í”„ë¡¬í”„íŠ¸ ìš”ì²­ì—ì„œ ì‹¤ì œ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±"""

        try:
            prompt_text = get_prompt_template(
                category=request.prompt_category,
                template_name=request.prompt_template,
                **request.prompt_parameters
            )

            # ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            self._save_generated_prompt(request, prompt_text)

            return prompt_text

        except Exception as e:
            self.logger.error(f"âŒ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise

    def execute_prompt(self, request: ChunkPromptRequest, llm_executor) -> ChunkPromptResult:
        """
        í”„ë¡¬í”„íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.

        Args:
            request: í”„ë¡¬í”„íŠ¸ ìš”ì²­
            llm_executor: LLM ì‹¤í–‰ í•¨ìˆ˜ (prompt_text -> response_text)

        Returns:
            ChunkPromptResult: ì‹¤í–‰ ê²°ê³¼
        """
        request_id = str(uuid.uuid4())
        start_time = datetime.now()

        self.logger.info(f"ğŸš€ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ ì‹œì‘: {request.chunk_id} ({request.prompt_category}.{request.prompt_template})")

        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            generated_prompt = self.generate_prompt(request)

            # LLM ì‹¤í–‰
            llm_response = llm_executor(generated_prompt)

            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()

            result = ChunkPromptResult(
                request_id=request_id,
                chunk_id=request.chunk_id,
                generated_prompt=generated_prompt,
                prompt_length=len(generated_prompt),
                llm_response=llm_response,
                response_length=len(llm_response),
                processing_time_seconds=processing_time,
                success=True,
                completion_timestamp=end_time.isoformat()
            )

            # ê²°ê³¼ ì €ì¥
            self._save_prompt_result(request, result)

            self.logger.info(f"âœ… í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ ì™„ë£Œ: {request.chunk_id} (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ)")

            return result

        except Exception as e:
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()

            result = ChunkPromptResult(
                request_id=request_id,
                chunk_id=request.chunk_id,
                generated_prompt=generated_prompt if 'generated_prompt' in locals() else "",
                prompt_length=len(generated_prompt) if 'generated_prompt' in locals() else 0,
                llm_response="",
                response_length=0,
                processing_time_seconds=processing_time,
                success=False,
                error_message=str(e),
                completion_timestamp=end_time.isoformat()
            )

            # ì˜¤ë¥˜ ê²°ê³¼ë„ ì €ì¥
            self._save_prompt_result(request, result)

            self.logger.error(f"âŒ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {request.chunk_id} - {str(e)}")

            return result

    def _save_prompt_requests(self, chunk_id: str, requests: List[ChunkPromptRequest]):
        """í”„ë¡¬í”„íŠ¸ ìš”ì²­ë“¤ì„ íŒŒì¼ë¡œ ì €ì¥"""

        requests_file = self.prompts_dir / f"{chunk_id}_prompt_requests.json"

        requests_data = {
            'chunk_id': chunk_id,
            'total_requests': len(requests),
            'created_at': datetime.now().isoformat(),
            'requests': [asdict(req) for req in requests]
        }

        with open(requests_file, 'w', encoding='utf-8') as f:
            json.dump(requests_data, f, ensure_ascii=False, indent=2)

        self.logger.debug(f"ğŸ’¾ í”„ë¡¬í”„íŠ¸ ìš”ì²­ ì €ì¥: {requests_file}")

    def _save_generated_prompt(self, request: ChunkPromptRequest, prompt_text: str):
        """ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥"""

        prompt_filename = f"{request.chunk_id}_{request.prompt_category}_{request.prompt_template}.txt"
        prompt_file = self.prompts_dir / prompt_filename

        with open(prompt_file, 'w', encoding='utf-8') as f:
            f.write(f"# ì²­í¬ í”„ë¡¬í”„íŠ¸: {request.chunk_id}\n")
            f.write(f"ì¹´í…Œê³ ë¦¬: {request.prompt_category}\n")
            f.write(f"í…œí”Œë¦¿: {request.prompt_template}\n")
            f.write(f"ìƒì„±ì¼ì‹œ: {datetime.now().isoformat()}\n")
            f.write(f"ì²­í¬ ë ˆë²¨: {request.chunk_level}\n")
            f.write(f"ì½˜í…ì¸  ê¸¸ì´: {request.content_length}\n")
            f.write("=" * 80 + "\n\n")
            f.write(prompt_text)

        self.logger.debug(f"ğŸ’¾ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì €ì¥: {prompt_file}")

    def _save_prompt_result(self, request: ChunkPromptRequest, result: ChunkPromptResult):
        """í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì €ì¥"""

        # JSON ê²°ê³¼ íŒŒì¼
        result_filename = f"{request.chunk_id}_{request.prompt_category}_{request.prompt_template}_result.json"
        result_file = self.results_dir / result_filename

        result_data = {
            'request_info': asdict(request),
            'execution_result': asdict(result),
            'metadata': {
                'saved_at': datetime.now().isoformat(),
                'result_file': str(result_file)
            }
        }

        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)

        # í…ìŠ¤íŠ¸ ì‘ë‹µ íŒŒì¼ (ê°€ë…ì„±ì„ ìœ„í•´)
        response_filename = f"{request.chunk_id}_{request.prompt_category}_{request.prompt_template}_response.txt"
        response_file = self.results_dir / response_filename

        with open(response_file, 'w', encoding='utf-8') as f:
            f.write(f"# LLM ì‘ë‹µ: {request.chunk_id}\n")
            f.write(f"ì¹´í…Œê³ ë¦¬: {request.prompt_category}\n")
            f.write(f"í…œí”Œë¦¿: {request.prompt_template}\n")
            f.write(f"ì‹¤í–‰ì¼ì‹œ: {result.completion_timestamp}\n")
            f.write(f"ì„±ê³µì—¬ë¶€: {'ì„±ê³µ' if result.success else 'ì‹¤íŒ¨'}\n")
            if result.error_message:
                f.write(f"ì˜¤ë¥˜ë©”ì‹œì§€: {result.error_message}\n")
            f.write(f"ì²˜ë¦¬ì‹œê°„: {result.processing_time_seconds:.2f}ì´ˆ\n")
            f.write(f"ì‘ë‹µê¸¸ì´: {result.response_length}ì\n")
            f.write("=" * 80 + "\n\n")
            f.write(result.llm_response)

        # ë¡œê·¸ íŒŒì¼ ì—…ë°ì´íŠ¸
        self._update_chunk_log(request, result)

        self.logger.debug(f"ğŸ’¾ ì‹¤í–‰ ê²°ê³¼ ì €ì¥: {result_file}, {response_file}")

    def _update_chunk_log(self, request: ChunkPromptRequest, result: ChunkPromptResult):
        """ì²­í¬ë³„ ì‹¤í–‰ ë¡œê·¸ ì—…ë°ì´íŠ¸"""

        log_file = self.logs_dir / f"{request.chunk_id}_execution_log.jsonl"

        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'chunk_id': request.chunk_id,
            'prompt_category': request.prompt_category,
            'prompt_template': request.prompt_template,
            'success': result.success,
            'processing_time': result.processing_time_seconds,
            'prompt_length': result.prompt_length,
            'response_length': result.response_length,
            'error_message': result.error_message
        }

        # JSONL í˜•ì‹ìœ¼ë¡œ ì¶”ê°€ (í•œ ì¤„ì”© JSON ê°ì²´)
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')

    def get_chunk_prompt_summary(self, chunk_id: str) -> Dict[str, Any]:
        """ì²­í¬ì˜ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ ìš”ì•½ ì •ë³´ ë°˜í™˜"""

        log_file = self.logs_dir / f"{chunk_id}_execution_log.jsonl"

        if not log_file.exists():
            return {
                'chunk_id': chunk_id,
                'total_executions': 0,
                'successful_executions': 0,
                'failed_executions': 0,
                'total_processing_time': 0,
                'average_processing_time': 0,
                'executions': []
            }

        executions = []
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                executions.append(json.loads(line.strip()))

        successful = len([ex for ex in executions if ex.get('success', False)])
        failed = len(executions) - successful
        total_time = sum(ex.get('processing_time', 0) for ex in executions)

        return {
            'chunk_id': chunk_id,
            'total_executions': len(executions),
            'successful_executions': successful,
            'failed_executions': failed,
            'total_processing_time': total_time,
            'average_processing_time': total_time / len(executions) if executions else 0,
            'executions': executions
        }

    def generate_chunk_report(self, chunk_id: str) -> str:
        """ì²­í¬ì˜ ì „ì²´ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""

        summary = self.get_chunk_prompt_summary(chunk_id)

        report_lines = [
            f"# ì²­í¬ ë¶„ì„ ë³´ê³ ì„œ: {chunk_id}",
            f"ìƒì„±ì¼ì‹œ: {datetime.now().isoformat()}",
            "",
            "## ì‹¤í–‰ ìš”ì•½",
            f"- ì´ ì‹¤í–‰ íšŸìˆ˜: {summary['total_executions']}",
            f"- ì„±ê³µ: {summary['successful_executions']}",
            f"- ì‹¤íŒ¨: {summary['failed_executions']}",
            f"- ì´ ì²˜ë¦¬ ì‹œê°„: {summary['total_processing_time']:.2f}ì´ˆ",
            f"- í‰ê·  ì²˜ë¦¬ ì‹œê°„: {summary['average_processing_time']:.2f}ì´ˆ",
            "",
            "## ì‹¤í–‰ ì„¸ë¶€ ë‚´ì—­"
        ]

        for i, execution in enumerate(summary['executions'], 1):
            status = "âœ…" if execution.get('success', False) else "âŒ"
            report_lines.extend([
                f"### {i}. {execution.get('prompt_category', 'unknown')}.{execution.get('prompt_template', 'unknown')} {status}",
                f"- ì‹¤í–‰ì‹œê°: {execution.get('timestamp', 'unknown')}",
                f"- ì²˜ë¦¬ì‹œê°„: {execution.get('processing_time', 0):.2f}ì´ˆ",
                f"- í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {execution.get('prompt_length', 0):,}ì",
                f"- ì‘ë‹µ ê¸¸ì´: {execution.get('response_length', 0):,}ì",
            ])

            if execution.get('error_message'):
                report_lines.append(f"- ì˜¤ë¥˜: {execution['error_message']}")

            report_lines.append("")

        return "\n".join(report_lines)


__all__ = ["ChunkPromptManager", "ChunkPromptRequest", "ChunkPromptResult"]